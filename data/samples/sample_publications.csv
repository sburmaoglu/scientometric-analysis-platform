Lens ID,Title,Date Published,Publication Year,Publication Type,Source Title,ISSNs,Publisher,Source Country,Author/s,Abstract,Volume,Issue Number,Start Page,End Page,Fields of Study,Keywords,MeSH Terms,Chemicals,Funding,Source URLs,External URL,PMID,DOI,Microsoft Academic ID,PMCID,Citing Patents Count,References,Citing Works Count,Is Open Access,Open Access License,Open Access Colour
000-169-944-123-061,A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading,2024-06-27,2024,preprint,arXiv (Cornell University),,,,Yuan Li; Bingqiao Luo; Qian Wang; Nuo Chen; Xu Liu; Bingsheng He,"The utilization of Large Language Models (LLMs) in financial trading has primarily been concentrated within the stock market, aiding in economic and financial decisions. Yet, the unique opportunities presented by the cryptocurrency market, noted for its on-chain data's transparency and the critical influence of off-chain signals like news, remain largely untapped by LLMs. This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data. This approach leverages the transparency and immutability of on-chain data, as well as the timeliness and influence of off-chain signals, providing a comprehensive overview of the cryptocurrency market. CryptoTrade incorporates a reflective mechanism specifically engineered to refine its daily trading decisions by analyzing the outcomes of prior trading decisions. This research makes two significant contributions. Firstly, it broadens the applicability of LLMs to the domain of cryptocurrency trading. Secondly, it establishes a benchmark for cryptocurrency trading strategies. Through extensive experiments, CryptoTrade has demonstrated superior performance in maximizing returns compared to traditional trading strategies and time-series baselines across various cryptocurrencies and market conditions. Our code and data are available at \url{https://anonymous.4open.science/r/CryptoTrade-Public-92FC/}.",,,,,Cryptocurrency; Shot (pellet); Zero (linguistics); Computer science; Business; Computer security; Materials science; Philosophy; Linguistics; Metallurgy,,,,,https://arxiv.org/abs/2407.09546,http://dx.doi.org/10.48550/arxiv.2407.09546,,10.48550/arxiv.2407.09546,,,0,,0,true,,green
000-343-097-007-123,"Governing LLM-assisted retail equity-options decision pipeline: a single-case audit of ""vibe bloating"" in trade selection and structuring",,2025,preprint,,,Elsevier BV,,Dmitrii Gimmelberg; Alexey Belinskiy; IVETA LUDVIGA,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5751342,,10.2139/ssrn.5751342,,,0,,0,false,,
000-792-945-080-491,Intelligent Financial Assistant: Chatbot-Based Mutual Fund Advisory System,2025-07-09,2025,conference proceedings article,2025 6th International Conference on Data Intelligence and Cognitive Informatics (ICDICI),,IEEE,,Babymol Kurian; Kuppu Shanmuganathan K; Sabarish S; Krithikaa Venkat; Adline Freeda,,,,1719,1726,Chatbot; Computer science; Finance; Mutual fund; Business; World Wide Web,,,,,,http://dx.doi.org/10.1109/icdici66477.2025.11135013,,10.1109/icdici66477.2025.11135013,,,0,024-004-970-407-266; 028-642-359-076-337; 047-220-396-571-758; 049-490-676-370-435; 054-047-051-176-792; 073-303-412-668-303; 079-647-720-976-487; 083-989-154-270-555; 087-546-778-785-20X; 091-497-941-194-95X; 127-412-296-825-788; 168-854-505-294-916; 184-596-400-235-910,0,false,,
000-946-866-199-75X,Can large language models effectively process and execute financial trading instructions?,2025-11-17,2025,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Yu Kang; Xin Yang; Ge Wang; Yuda Wang; Zhanyu Wang; Mingwen Liu,,26,10,1832,1846,,,,,,,http://dx.doi.org/10.1631/fitee.2500285,,10.1631/fitee.2500285,,,0,020-528-617-016-901; 020-563-084-760-554; 021-137-096-165-589; 030-780-820-536-934; 031-257-663-390-395; 035-729-626-394-678; 040-303-531-390-765; 044-148-065-491-996; 044-937-010-718-593; 053-109-945-766-155; 057-040-097-643-496; 057-982-454-558-150; 061-960-297-209-594; 089-289-729-128-113; 092-770-866-733-36X; 096-082-868-445-261; 096-704-258-326-787; 101-968-220-076-477; 103-708-848-416-467; 105-772-178-512-773; 112-705-069-209-103; 119-900-757-468-115; 139-239-027-110-484; 156-652-154-081-693; 160-070-831-619-900; 163-979-407-953-145; 167-873-473-283-733; 186-721-289-190-680,0,false,,
001-071-106-063-494,Automating translation checks of financial documents using large language models,2025-07-18,2025,journal article,Language Resources and Evaluation,1574020x; 15740218,Springer Science and Business Media LLC,Germany,Maren Pielka; Max Hahnbück; Tobias Deußer; Daniel Uedelhoven; Moinam Chatterjee; Vijul Shah; Osama Soliman; Jannis von der Bank; Writwick Das; Maria Chiara Talarico; Cong Zhao; Carolina Held Celis; Christian Temath; Rafet Sifa,"<jats:title>Abstract</jats:title>;                   <jats:p>We introduce a tool for automated translation checking of financial reports in German-English. It uses a heuristic matching algorithm followed by a transformer encoder based error detection model on sentence pair level. For generating the training data, we leverage state-of-the-art large language models such as GPT-4o, thereby alleviating the need for expert annotations. The results suggest that smaller models fine-tuned specifically for this task significantly outperform large multi-purpose generative models like GPT-4 for this particular problem, and that a combination of informed and deep learning approaches works best in this case. The tool is being made publicly available as a demonstrator.</jats:p>",59,4,3873,3887,Computer science; Transformer; Leverage (statistics); Machine translation; Natural language processing; Artificial intelligence; Language model; Sentence; Encoder; Task (project management); German; Machine learning; Heuristic; Generative grammar; Linguistics; Philosophy; Physics; Management; Quantum mechanics; Voltage; Economics; Operating system,,,,Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme IAIS,,http://dx.doi.org/10.1007/s10579-025-09862-z,,10.1007/s10579-025-09862-z,,,0,003-536-411-571-444; 032-509-322-900-566; 073-027-536-086-323; 088-157-603-923-110; 106-045-308-014-744; 125-007-830-851-395; 189-973-804-567-981,0,true,cc-by,hybrid
001-309-005-961-997,"Green AI: exploring carbon footprints, mitigation strategies, and trade offs in large language model training",2024-07-08,2024,journal article,Discover Artificial Intelligence,27310809,Springer Science and Business Media LLC,,Vivian Liu; Yiqiao Yin,"<jats:title>Abstract</jats:title><jats:p>Prominent works in the space of Natural Language Processing (NLP) have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.</jats:p>",4,1,,,Greenhouse gas; Robustness (evolution); Carbon footprint; Computer science; Training (meteorology); Argument (complex analysis); Architecture; Artificial intelligence; Ecology; Meteorology; Art; Biochemistry; Chemistry; Physics; Biology; Visual arts; Gene,,,,,https://link.springer.com/content/pdf/10.1007/s44163-024-00149-w.pdf https://doi.org/10.1007/s44163-024-00149-w,http://dx.doi.org/10.1007/s44163-024-00149-w,,10.1007/s44163-024-00149-w,,,0,005-407-048-718-561; 011-395-861-247-207; 011-482-715-327-060; 028-818-999-209-819; 053-459-476-771-890; 053-591-547-250-936; 068-327-442-691-542; 094-864-860-262-632; 149-532-354-978-892; 156-811-783-385-919; 164-330-557-454-333,33,true,"CC BY, CC BY-NC-ND",gold
001-406-606-474-785,"Can We Make Code Green? Understanding Trade-Offs in LLMs vs. Human Code
  Optimizations",2025-03-25,2025,preprint,arXiv (Cornell University),,,,Pooja Rani; Jan-Andrea Bard; June Sallou; Alexander Boll; Timo Kehrer; Alberto Bacchelli,"The rapid technological evolution has accelerated software development for various domains and use cases, contributing to a growing share of global carbon emissions. While recent large language models (LLMs) claim to assist developers in optimizing code for performance and energy efficiency, their efficacy in real-world scenarios remains under exploration. In this work, we explore the effectiveness of LLMs in reducing the environmental footprint of real-world projects, focusing on software written in Matlab-widely used in both academia and industry for scientific and engineering applications. We analyze energy-focused optimization on 400 scripts across 100 top GitHub repositories. We examine potential 2,176 optimizations recommended by leading LLMs, such as GPT-3, GPT-4, Llama, and Mixtral, and a senior Matlab developer, on energy consumption, memory usage, execution time consumption, and code correctness. The developer serves as a real-world baseline for comparing typical human and LLM-generated optimizations. Mapping these optimizations to 13 high-level themes, we found that LLMs propose a broad spectrum of improvements--beyond energy efficiency--including improving code readability and maintainability, memory management, error handling while the developer overlooked some parallel processing, error handling etc. However, our statistical tests reveal that the energy-focused optimizations unexpectedly negatively impacted memory usage, with no clear benefits regarding execution time or energy consumption. Our qualitative analysis of energy-time trade-offs revealed that some themes, such as vectorization preallocation, were among the common themes shaping these trade-offs. With LLMs becoming ubiquitous in modern software development, our study serves as a call to action: prioritizing the evaluation of common coding practices to identify the green ones.",,,,,Code (set theory); Computer science; Programming language; Set (abstract data type),,,,,https://arxiv.org/abs/2503.20126,http://dx.doi.org/10.48550/arxiv.2503.20126,,10.48550/arxiv.2503.20126,,,0,,0,true,,green
001-536-892-769-300,Investigating managers' understanding of chatbots in the Korean financial industry,,2021,journal article,Computers in Human Behavior,07475632,Elsevier BV,United Kingdom,Moonkyoung Jang; Yoonhyuk Jung; Seongcheol Kim,,120,,106747,,Digital transformation; Financial services; Business; Perspective (graphical); Perception; Chatbot; Service (business); Knowledge management,,,,"Institute for Information and Communications Technology Promotion; Ministry of Education; Ministry of Science, ICT and Future Planning; National Research Foundation of Korea",https://dblp.uni-trier.de/db/journals/chb/chb120.html#JangJK21 https://www.sciencedirect.com/science/article/abs/pii/S0747563221000698 https://doi.org/10.1016/j.chb.2021.106747,http://dx.doi.org/10.1016/j.chb.2021.106747,,10.1016/j.chb.2021.106747,3132577611,,0,000-190-266-444-734; 000-625-062-722-309; 003-118-998-063-278; 017-766-559-664-702; 019-226-581-859-069; 020-251-449-267-381; 022-034-368-353-431; 024-920-582-562-188; 030-280-854-277-466; 030-438-610-495-210; 033-451-056-986-404; 033-525-740-901-402; 033-826-575-023-317; 036-615-479-671-743; 038-925-283-945-155; 040-481-565-458-221; 045-208-766-867-456; 046-637-337-319-349; 049-463-493-827-566; 050-668-863-259-637; 055-794-758-962-071; 056-849-757-952-252; 057-359-044-030-404; 058-124-584-289-580; 061-575-106-006-858; 065-154-344-392-221; 069-521-759-236-551; 072-389-597-520-500; 073-303-412-668-303; 074-407-245-786-654; 077-935-884-597-890; 080-497-361-174-582; 080-504-676-685-232; 080-948-966-790-927; 089-766-937-624-866; 090-759-094-047-289; 091-867-419-347-461; 093-759-709-146-945; 096-222-746-318-151; 099-462-015-682-437; 101-264-338-559-786; 101-885-252-920-130; 104-778-358-482-883; 105-753-084-464-981; 110-417-871-298-706; 110-446-479-859-460; 119-745-669-118-660; 124-171-824-969-31X; 126-339-213-560-362; 132-314-137-802-214; 137-211-774-200-005; 148-984-790-760-866; 163-864-824-279-286; 166-957-778-219-873,90,false,,
001-764-116-966-082,Automation of Customer Support System (Chatbot) to Solve Web Based Financial and Payment Application Service,2023-08-25,2023,journal article,Asian Journal of Computer Science and Technology,25837907; 22490701,Centre for Research and Innovation,,Roqib Akintunde Akinyemi; Wumi Ajayi; Ayuba Atuman,"<jats:p>One of the most important features of any online service is the quality of its customer care. However, with the development of NLP tools, businesses are considering automated chatbot solutions to keep up with the increasing demand for their products and services. In view of this, the chatbot was developed using AIML java interpreter library Program AB which helps match input and output predefined in the AIML file. AIML (Artificial Intelligence Markup Language) was used to preprocess and train the bot using ready-made AIML file for FAQ questions. Also, vaadin was used to build a web UI to interact with the trained AIML bot. Finally, a google script was written to translate from any language to English for the bot to understand and send the response in the preferred language of the user. Findings showed that the response time of the bot is dependent of the network, as the design gave a score of 70%, 80%, 90% and 90% for load testing, stability, reliability testing and usability testing, respectively. Also, the bot is compatible with different operating systems, both for forward compatibility and backward compatibility having a score of 95%. The bot was able to answer customer questions, enquiries and complaints and the response time of the bot depends on the strength of the network since it is web based. Hence, the system provided a simple, cheaper, and durable customer financial and payment application service. Since chatbots cannot answer all questions, businesses that decide to implement them should ensure that they have enough protections in place against attacks and that routine requests are standardised to ensure optimal performance.</jats:p>",12,2,1,17,Chatbot; Computer science; Usability; World Wide Web; Payment; Scripting language; Software engineering; Operating system,,,,,https://ojs.trp.org.in/index.php/ajcst/article/download/3697/4282 https://doi.org/10.51983/ajcst-2023.12.2.3697,http://dx.doi.org/10.51983/ajcst-2023.12.2.3697,,10.51983/ajcst-2023.12.2.3697,,,0,,3,true,cc-by-nc-nd,hybrid
001-973-295-597-841,Predicting Numeric Financial KPIs From Unstructured Text: a Comparative Study of LLM-Based Embeddings and Traditional NLP Techniques,2025-06-25,2025,conference proceedings article,"2025 IEEE/ACIS 29th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",,IEEE,,Lord Coffie; Melvin Ajuluchukwu; Michael Nsor,,,,1017,1022,,,,,,,http://dx.doi.org/10.1109/snpd65828.2025.11313543,,10.1109/snpd65828.2025.11313543,,,0,019-227-972-560-501; 098-247-797-943-444; 189-825-916-426-10X; 193-136-826-957-223,0,false,,
002-623-228-470-02X,Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Xinli Yu; Zheng Chen; Yuan Ling; Shujing Dong; Zongyi Liu; Yanbin Lu,"This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",,,,,Computer science; Inference; Financial market; Boosting (machine learning); Metadata; Machine learning; Artificial intelligence; Finance; Economics; World Wide Web,,,,,https://arxiv.org/abs/2306.11025,http://dx.doi.org/10.48550/arxiv.2306.11025,,10.48550/arxiv.2306.11025,,,0,,0,true,cc-by-nc-sa,green
003-048-752-741-023,LLM-Powered Multi-Agent System for Automated Crypto Portfolio Management,2025-01-01,2025,preprint,arXiv (Cornell University),,,,Yichen Luo; Yebo Feng; Jiahua Xu; Paolo Tasca; Yang Liu,"Cryptocurrency investment is inherently difficult due to its shorter history compared to traditional assets, the need to integrate vast amounts of data from various modalities, and the requirement for complex reasoning. While deep learning approaches have been applied to address these challenges, their black-box nature raises concerns about trust and explainability. Recently, large language models (LLMs) have shown promise in financial applications due to their ability to understand multi-modal data and generate explainable decisions. However, single LLM faces limitations in complex, comprehensive tasks such as asset investment. These limitations are even more pronounced in cryptocurrency investment, where LLMs have less domain-specific knowledge in their training corpora. To overcome these challenges, we propose an explainable, multi-modal, multi-agent framework for cryptocurrency investment. Our framework uses specialized agents that collaborate within and across teams to handle subtasks such as data analysis, literature integration, and investment decision-making for the top 30 cryptocurrencies by market capitalization. The expert training module fine-tunes agents using multi-modal historical data and professional investment literature, while the multi-agent investment module employs real-time data to make informed cryptocurrency investment decisions. Unique intrateam and interteam collaboration mechanisms enhance prediction accuracy by adjusting final predictions based on confidence levels within agent teams and facilitating information sharing between teams. Empirical evaluation using data from November 2023 to September 2024 demonstrates that our framework outperforms single-agent models and market benchmarks in classification, asset pricing, portfolio, and explainability performance.",,,,,Portfolio; Project portfolio management; Blockchain; Computer science; Business; Computer security; Systems engineering; Engineering; Finance; Project management,,,,,https://arxiv.org/abs/2501.00826,http://dx.doi.org/10.48550/arxiv.2501.00826,,10.48550/arxiv.2501.00826,,,0,,0,true,,green
003-294-968-035-404,Nexus between Chat GPT usage dimensions and investment decisions making in Pakistan: Moderating role of financial literacy,,2024,journal article,Technology in Society,0160791x; 18793274,Elsevier BV,Netherlands,Rafid Ullah; Hishamuddin Bin Ismail; Mohammad Tariqul Islam Khan; Ali Zeb,,76,,102454,102454,Snowball sampling; Financial literacy; Stock market; Investment decisions; Nexus (standard); Investment (military); Business; Financial market; Context (archaeology); Emerging markets; Stock (firearms); Finance; Marketing; Economics; Behavioral economics; Computer science; Medicine; Paleontology; Pathology; Politics; Political science; Law; Biology; Embedded system; Mechanical engineering; Engineering,,,,,,http://dx.doi.org/10.1016/j.techsoc.2024.102454,,10.1016/j.techsoc.2024.102454,,,0,000-993-303-190-884; 001-426-031-601-285; 003-864-759-858-908; 004-369-879-611-238; 008-526-374-105-091; 011-057-105-680-492; 011-760-499-769-901; 020-038-815-946-075; 020-069-551-115-56X; 022-754-418-693-625; 023-981-140-742-950; 027-892-634-351-23X; 028-948-405-834-39X; 033-171-354-077-456; 033-419-680-124-445; 034-703-498-558-110; 034-925-032-204-170; 035-730-317-696-224; 037-211-291-797-489; 043-397-944-479-29X; 044-552-323-229-435; 049-207-551-900-125; 049-260-958-544-487; 049-469-431-552-154; 050-689-449-173-769; 054-737-803-526-854; 067-856-138-094-847; 068-474-597-236-536; 068-632-954-158-398; 068-642-762-550-052; 069-687-646-515-546; 077-930-616-453-628; 079-208-988-824-148; 082-139-063-462-415; 086-994-956-658-127; 089-080-748-247-241; 093-424-310-783-34X; 099-204-393-813-279; 105-475-465-258-455; 106-010-415-509-742; 108-899-878-902-593; 109-092-577-645-027; 117-500-918-749-351; 123-945-600-073-434; 125-684-200-397-675; 128-649-722-592-860; 129-324-374-869-536; 133-942-463-254-19X; 135-189-641-687-525; 135-886-952-426-289; 137-469-751-849-213; 141-089-377-935-399; 145-271-718-538-268; 145-426-036-014-129; 148-925-144-527-058; 153-301-954-139-087; 157-522-139-120-552; 172-533-648-163-689; 188-320-554-004-179; 197-691-142-603-06X,28,false,,
003-350-145-748-925,Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Lars Hillebrand; Armin Berger; Tobias Deußer; Tim Dilmaghani; Mohamed Khaled; Bernd Kliem; Rüdiger Loitz; Maren Pielka; David Leonhard; Christian Bauckhage; Rafet Sifa,"Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.",,,,,Transformer; Computer science; Audit; Language model; Matching (statistics); Process (computing); Data mining; Information retrieval; Artificial intelligence; Natural language processing; Accounting; Engineering; Programming language; Mathematics; Statistics; Voltage; Electrical engineering; Business,,,,,https://arxiv.org/abs/2308.06111,http://dx.doi.org/10.48550/arxiv.2308.06111,,10.48550/arxiv.2308.06111,,,0,,1,true,other-oa,green
003-920-342-653-767,Graph LLM-Based Portfolio Management Algorithm,2025-05-05,2025,conference proceedings article,2025 IEEE Conference on Artificial Intelligence (CAI),,IEEE,,Dayu Qin; Yan Yi; Ercan E. Kuruoglu,,,,157,160,Computer science; Portfolio; Graph; Algorithm; Theoretical computer science; Finance; Business,,,,Shenzhen Science and Technology Innovation Commission,,http://dx.doi.org/10.1109/cai64502.2025.00032,,10.1109/cai64502.2025.00032,,,0,007-516-882-620-676; 019-089-850-263-397; 019-653-519-341-505; 021-541-154-871-540; 024-847-740-813-264; 036-665-842-111-878; 036-851-552-690-727; 051-778-085-924-525; 053-222-815-568-200; 058-833-105-315-369; 099-272-710-792-171; 112-373-291-929-955; 117-227-579-058-622; 134-983-976-135-871; 135-624-906-435-66X; 142-721-926-477-654; 160-804-681-367-030; 182-953-730-637-496; 186-827-515-649-986; 192-108-991-840-139; 199-956-873-084-500,0,false,,
004-464-277-212-550,"No Language is an Island: Unifying Chinese and English in Financial
  Large Language Models, Instruction Data, and Benchmarks",2024-03-10,2024,preprint,arXiv (Cornell University),,,,Gang Hu; Ke Qin; Chenhan Yuan; Min Peng; Alejandro Lopez-Lira; Benyou Wang; Sophia Ananiadou; Jimin Huang; Qianqian Xie,"While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in translation tasks and utilizing original English data, enhancing both linguistic flexibility and analytical acuity in financial contexts. Notably, ICE-INTENT distinguishes itself by showcasing significant enhancements over conventional LLMs and existing financial LLMs in bilingual milieus, underscoring the profound impact of robust bilingual data on the accuracy and efficacy of financial NLP.",,,,,Computer science; Linguistics; Philosophy,,,,,https://arxiv.org/abs/2403.06249,http://dx.doi.org/10.48550/arxiv.2403.06249,,10.48550/arxiv.2403.06249,,,0,,0,true,,green
004-592-223-825-526,"FMDLlama: Financial Misinformation Detection based on Large Language
  Models",2024-09-24,2024,preprint,arXiv (Cornell University),,,,Zhiwei Liu; Xin Zhang; Kailai Yang; Qianqian Xie; Jimin Huang; Sophia Ananiadou,"The emergence of social media has made the spread of misinformation easier. In the financial domain, the accuracy of information is crucial for various aspects of financial market, which has made financial misinformation detection (FMD) an urgent problem that needs to be addressed. Large language models (LLMs) have demonstrated outstanding performance in various fields. However, current studies mostly rely on traditional methods and have not explored the application of LLMs in the field of FMD. The main reason is the lack of FMD instruction tuning datasets and evaluation benchmarks. In this paper, we propose FMDLlama, the first open-sourced instruction-following LLMs for FMD task based on fine-tuning Llama3.1 with instruction data, the first multi-task FMD instruction dataset (FMDID) to support LLM instruction tuning, and a comprehensive FMD evaluation benchmark (FMD-B) with classification and explanation generation tasks to test the FMD ability of LLMs. We compare our models with a variety of LLMs on FMD-B, where our model outperforms all other open-sourced LLMs as well as ChatGPT.",,,,,Misinformation; Computer science; Finance; Natural language processing; Business; Computer security,,,,,https://arxiv.org/abs/2409.16452,http://dx.doi.org/10.48550/arxiv.2409.16452,,10.48550/arxiv.2409.16452,,,0,,1,true,,green
004-755-462-162-583,Fin-Grained: A Fine-Grained Benchmark Dataset and Evaluation Protocol for Llms' Numerical Reasoning Capabilities in Financial Domain,2025-01-01,2025,preprint,,,Elsevier BV,,Sangmin Lee; Jaewon Cheon; inyeol choi; Pilsung Kang,,,,,,Benchmark (surveying); Domain (mathematical analysis); Protocol (science); Computer science; Fin; Finance; Business; Geography; Cartography; Medicine; Mathematics; Engineering; Mechanical engineering; Mathematical analysis; Alternative medicine; Pathology,,,,,,http://dx.doi.org/10.2139/ssrn.5292853,,10.2139/ssrn.5292853,,,0,,0,false,,
005-033-612-248-430,Multi-Modal Fusion for Financial Crime Recognition Based on Large Language Model,2025-01-01,2025,preprint,,,Elsevier BV,,Ziyue Wang; Qinyan Shen; Yining Zhou; Tianyuan Deng,,,,,,Modal; Computer science; Finance; Business; Artificial intelligence; Speech recognition; Chemistry; Polymer chemistry,,,,,,http://dx.doi.org/10.2139/ssrn.5382501,,10.2139/ssrn.5382501,,,0,,0,false,,
005-465-348-823-49X,Financial Crimes on Social Media: Leveraging Large Language Models and Generative AI to Detect Ponzi Schemes in Indonesia,2025-08-06,2025,conference proceedings article,2025 International Conference on Information Technology and Computing (ICITCOM),,IEEE,,Rizaldy Anggriawan,,,,60,65,,,,,,,http://dx.doi.org/10.1109/icitcom66635.2025.11265598,,10.1109/icitcom66635.2025.11265598,,,0,006-560-083-162-164; 007-002-355-130-52X; 014-509-839-511-245; 016-220-347-545-794; 022-153-348-966-38X; 025-122-626-487-678; 031-912-799-476-348; 036-945-723-803-720; 045-484-225-394-116; 053-383-109-588-631; 058-981-188-686-644; 059-501-108-275-281; 065-481-637-027-082; 069-051-516-432-482; 078-474-636-862-816; 080-818-394-514-074; 081-811-856-619-669; 087-623-230-412-885; 091-411-051-258-480; 107-029-551-819-792; 109-946-085-711-792; 169-091-702-440-349; 173-829-462-052-680; 175-636-725-902-387,0,false,,
005-469-380-172-218,QF-LLM: Financial Sentiment Analysis with Quantized Large Language Model,2025-06-27,2025,conference proceedings article,Proceedings of the 2025 International Conference on Artificial Intelligence and Digital Finance,,ACM,,Qiqi Lei; Taozheng Zhu,,,,15,19,,,,,,,http://dx.doi.org/10.1145/3764727.3764731,,10.1145/3764727.3764731,,,0,000-633-812-188-185; 008-303-055-735-856; 115-976-226-936-894; 135-397-803-738-441; 155-498-300-221-705,0,false,,
005-536-477-024-892,Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Yoshia Abe; Shuhei Matsuo; Ryoma Kondo; Ryohei Hisano,,,,4799,4808,Persona; Computer science; Portfolio; Artificial intelligence; Natural language processing; Data science; Human–computer interaction; Business; Finance,,,,,http://arxiv.org/pdf/2411.19515 http://arxiv.org/abs/2411.19515,http://dx.doi.org/10.1109/bigdata62323.2024.10825362,,10.1109/bigdata62323.2024.10825362,,,0,011-223-654-787-20X; 017-037-956-347-495; 026-983-179-499-108; 028-050-640-100-440; 040-353-042-523-112; 049-539-988-689-490; 076-563-171-210-341; 078-336-527-997-564; 111-795-701-309-710; 153-933-761-161-582; 185-908-712-782-549; 198-573-119-668-956,0,true,,green
005-652-794-800-494,Adaptive and Explainable Margin Trading via Large Language Models on Portfolio Management,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Jingyi Gu; Junyi Ye; Guiling Wang; Wenpeng Yin,"Recent strategies for portfolio management often lack flexibility to adjust funds between long and short positions throughout trading periods. This prevents adapting portfolios to the market, which mitigates risks and seizes opportunities. To address these gaps, we propose an adaptive and explainable framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL) for dynamic long-short position adjustment in response to evolving market conditions. This approach leverages the recent advancements in LLMs for processing unstructured data and their capacity for explainable reasoning. The framework includes two stages: an Explainable Market Forecasting/Reasoning Pipeline, and a Position Reallocation stage. The Market Forecasting/Reasoning Pipeline allows various LLMs to learn market trends from diverse external data sources and determine optimal adjustment ratios with a clear reasoning path. The Portfolio Reallocation stage interacts with the sequential trading process from a pre-trained RL model to enhance decision-making and transparency. Our framework is flexible to accommodate various external data sources from microeconomics to macroeconomics data, diverse data types including time series and news text, along with multiple LLMs. Experiments demonstrate that our framework effectively achieves three times the return and doubles the Sharpe ratio compared to benchmarks. All the data and code are publicly available under NJIT FinTech Lab's GitHub1.",,,248,256,Margin (machine learning); Computer science; Project portfolio management; Portfolio; Business; Machine learning; Economics; Finance; Project management; Management,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698681 https://doi.org/10.1145/3677052.3698681,http://dx.doi.org/10.1145/3677052.3698681,,10.1145/3677052.3698681,,,0,034-111-475-208-944; 039-941-045-314-59X; 061-075-865-054-999; 085-431-304-743-160; 098-428-380-981-106; 107-158-681-287-174; 140-503-889-364-109; 146-335-085-869-417; 159-250-160-806-512; 159-890-427-391-05X; 173-116-597-012-168; 179-810-443-354-761; 190-728-798-850-989; 192-806-908-794-847; 193-150-161-446-090; 193-278-292-674-926,2,true,cc-by,hybrid
005-986-285-020-705,The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,2024-01-01,2024,preprint,,,Elsevier BV,,Minkyu Shin; Jin Kim; Jiwoong Shin,,,,,,Business; Marketing,,,,,,http://dx.doi.org/10.2139/ssrn.5004194,,10.2139/ssrn.5004194,,,0,013-712-407-073-875; 013-785-783-175-514; 014-661-079-593-124; 027-363-643-062-866; 054-690-633-283-486; 056-796-875-995-257; 078-520-863-981-326; 090-326-192-106-488; 090-526-107-954-797; 110-531-033-287-206; 117-203-880-254-001; 140-735-125-265-429; 152-750-000-592-451,1,false,,
006-057-345-832-255,A Convergence Study on Chatbot Persona and User Experience of Financial Service - Focused on Loan Service -,,2019,,,,,,null 이성경; Ray Jaeyung Yun,,37,4,257,267,Financial services; Business; User experience design; Loan; Chatbot; Service (business); Convergence (relationship); Persona; Knowledge management,,,,,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002509499,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002509499,,,2980732768,,0,,0,false,,
006-442-196-250-411,"SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability
  Report Generation",2024-12-14,2024,preprint,arXiv (Cornell University),,,,Qilong Wu; Xiaoneng Xiang; Hejia Huang; Xuan Wang; Yeo Wei Jie; Ranjan Satapathy; Ricardo Shirota Filho; Bharadwaj Veeravalli,"The rapid growth of the financial sector and the rising focus on Environmental, Social, and Governance (ESG) considerations highlight the need for advanced NLP tools. However, open-source LLMs proficient in both finance and ESG domains remain scarce. To address this gap, we introduce SusGen-30K, a category-balanced dataset comprising seven financial NLP tasks and ESG report generation, and propose TCFD-Bench, a benchmark for evaluating sustainability report generation. Leveraging this dataset, we developed SusGen-GPT, a suite of models achieving state-of-the-art performance across six adapted and two off-the-shelf tasks, trailing GPT-4 by only 2% despite using 7-8B parameters compared to GPT-4's 1,700B. Based on this, we propose the SusGen system, integrated with Retrieval-Augmented Generation (RAG), to assist in sustainability report generation. This work demonstrates the efficiency of our approach, advancing research in finance and ESG.",,,,,Sustainability; Artificial intelligence; Computer science; Natural language processing; Biology; Ecology,,,,,https://arxiv.org/abs/2412.10906,http://dx.doi.org/10.48550/arxiv.2412.10906,,10.48550/arxiv.2412.10906,,,0,,0,true,,green
006-753-131-237-281,A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs in Production,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Chandra Irugalbandara; Ashish Mahendra; Roland Daynauth; Tharuka Kasthuri Arachchige; Jayanaka Dantanarayana; Krisztian Flautner; Lingjia Tang; Yiping Kang; Jason Mars,"Many companies rely on APIs of managed AI models such as OpenAI's GPT-4 to create AI-enabled experiences in their products. Along with the benefits of ease of use and shortened time to production, this reliance on proprietary APIs has downsides in terms of model control, performance reliability, up-time predictability, and cost. At the same time, there has been a flurry of open source small language models (SLMs) that have been made available for commercial use. However, their readiness to replace existing capabilities remains unclear, and a systematic approach to test these models is not readily available. In this paper, we present a systematic evaluation methodology for, and characterization of, modern open source SLMs and their trade-offs when replacing a proprietary LLM APIs for a real-world product feature. We have designed SLaM, an automated analysis tool that enables the quantitative and qualitative testing of product features utilizing arbitrary SLMs. Using SLaM, we examine both the quality and the performance characteristics of modern SLMs relative to an existing customer-facing OpenAI-based implementation. We find that across 9 SLMs and 29 variants, we observe competitive quality-of-results for our use case, significant performance consistency improvement, and a cost reduction of 5x-29x when compared to OpenAI GPT-4.",,,,,Predictability; Consistency (knowledge bases); Quality (philosophy); Computer science; Product (mathematics); Production (economics); Reliability (semiconductor); Open source; Artificial intelligence; Economics; Operating system; Physics; Philosophy; Power (physics); Geometry; Mathematics; Epistemology; Software; Quantum mechanics; Macroeconomics,,,,,https://arxiv.org/abs/2312.14972,http://dx.doi.org/10.48550/arxiv.2312.14972,,10.48550/arxiv.2312.14972,,,0,,0,true,cc-by-sa,green
006-766-887-873-547,Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Mohammad Alipour-Vaezi; Kwok-Leung Tsui,"Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI). To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project. Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method. Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm. In this paper, firstly, the fame score of the celebrities is determined using a large language model. Then, to tackle the asymmetric character of MPI's data, projects are classified. Furthermore, the box office prediction takes place for each class of projects. Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed.",,,,,Computer science; Motion (physics); Portfolio; Artificial intelligence; Industrial engineering; Data science; Engineering; Economics; Financial economics,,,,,https://arxiv.org/pdf/2404.07434 https://arxiv.org/abs/2404.07434,http://dx.doi.org/10.2139/ssrn.4790637,,10.2139/ssrn.4790637,,,0,004-817-632-357-019; 007-017-884-126-763; 010-654-745-829-151; 010-784-229-475-292; 019-706-757-416-919; 020-062-092-347-983; 027-057-022-867-746; 027-762-137-031-731; 031-161-496-420-086; 032-141-140-603-437; 048-476-957-423-890; 048-670-318-877-019; 063-592-773-988-530; 071-057-625-894-028; 072-385-285-248-271; 074-411-338-021-978; 074-625-566-847-628; 094-175-215-344-922; 106-604-221-665-323; 120-445-008-776-800; 135-067-714-451-092; 138-830-554-151-968; 150-321-681-209-455; 162-824-519-935-07X; 168-316-269-779-912; 193-294-470-759-819,0,true,,green
007-086-490-505-989,Advancing Anomaly Detection: Non-Semantic Financial Data Encoding With Large Language Models,,2025,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Alexander Bakumenko; Kateřina Hlaváčková-Schindler; Claudia Plant; Nina C. Hubig,,13,,146757,146771,,,,,,,http://dx.doi.org/10.1109/access.2025.3600967,,10.1109/access.2025.3600967,,,0,002-787-738-187-321; 006-299-463-890-28X; 008-186-637-691-064; 009-155-329-829-636; 011-115-238-070-623; 014-316-626-027-467; 015-144-581-514-947; 016-647-542-440-122; 017-182-648-310-27X; 017-218-355-656-885; 019-223-242-496-810; 020-227-483-968-78X; 023-922-299-055-466; 024-320-807-236-553; 024-789-896-622-760; 029-880-620-224-525; 036-549-433-431-741; 041-475-595-858-192; 045-267-262-641-445; 049-230-960-420-937; 059-786-079-109-47X; 064-407-203-887-665; 068-864-486-586-772; 069-507-976-179-943; 080-835-791-573-515; 082-432-999-638-636; 083-114-835-446-120; 087-790-855-801-811; 105-433-037-398-851; 109-326-888-708-055; 123-307-486-446-893; 127-070-884-583-079; 139-239-027-110-484; 144-489-531-982-436; 162-490-045-503-859; 182-999-309-327-367; 185-774-871-143-508; 194-758-367-854-008,2,true,"CC BY, CC BY-NC-ND",gold
007-417-156-621-849,A Financial Brain Scan of the LLM,2025-01-01,2025,preprint,,,Elsevier BV,,Hui Chen; Antoine Didisheim; Luciano Somoza; Hanqing Tian,,,,,,Finance; Business; Computer science,,,,,,http://dx.doi.org/10.2139/ssrn.5412277,,10.2139/ssrn.5412277,,,0,,0,false,,
007-581-025-024-301,Toward a Chatbot for Financial Sustainability,2021-03-13,2021,journal article,Sustainability,20711050,MDPI AG,Switzerland,Sewoong Hwang; Jonghyuk Kim,"This study examines technology effectiveness for industry demand in which artificial intelligence (AI) is applied in the financial sector. It summarizes prior studies on chatbot and customer service and investigates theories on acceptance attitudes for innovative technologies. By setting variables, the study examines bank revenue methodologically and assesses the impact of customer service and chatbot on bank revenues through customer age classification. The results indicate that new product-oriented funds or housing subscription savings are more suitable for purchase through customer service than through chatbot. However, services for existing products through chatbot positively affect banks’ net income. When classified by age, purchases by the majority age group in the channel positively affect bank profits. Finally, there is a tendency to process small banking transactions through the chatbot system, which saves transaction and management costs, positively affecting profits. Through empirical analysis, we first examine the effect of an AI-based chatbot system implemented to strengthen financial soundness and suggest policy alternatives. Second, we use banking data to increase the study’s real-life applicability and prove that problems in customer service can be solved through a chatbot system. Finally, we investigate how resistance to technology can be reduced and efficiently accommodated.",13,6,3173,,Net income; Business; Marketing; Revenue; Chatbot; Resistance (psychoanalysis); Affect (psychology); Soundness; Process (engineering); Database transaction,chatbot; artificial intelligence; financial sustainability; telemarketing; cube model; voice recognition and conversion model,,,,https://www.mdpi.com/2071-1050/13/6/3173 https://www.mdpi.com/2071-1050/13/6/3173/pdf https://ideas.repec.org/a/gam/jsusta/v13y2021i6p3173-d516560.html,http://dx.doi.org/10.3390/su13063173,,10.3390/su13063173,3138009402,,6,001-210-600-856-85X; 001-682-548-991-995; 001-782-625-397-876; 002-042-228-504-714; 003-977-430-671-663; 007-647-790-001-261; 007-919-870-247-997; 008-706-894-595-790; 010-249-592-715-348; 012-417-930-194-200; 013-719-395-689-921; 016-273-769-082-143; 017-150-370-532-858; 017-880-350-580-750; 018-817-558-143-669; 018-886-164-695-056; 019-451-689-824-280; 023-451-429-221-648; 024-800-443-292-177; 025-662-390-844-364; 031-420-886-532-773; 046-461-020-984-336; 046-637-337-319-349; 050-016-604-873-284; 053-434-144-774-904; 055-884-893-437-463; 058-274-488-416-803; 064-621-697-234-468; 066-165-050-280-734; 068-764-662-097-102; 070-257-616-189-116; 075-023-264-465-008; 077-154-902-677-99X; 080-080-755-941-998; 080-707-677-624-554; 082-213-360-855-285; 084-421-736-966-469; 085-896-563-465-281; 088-253-355-257-250; 089-766-937-624-866; 090-079-158-017-179; 091-321-311-469-956; 099-070-826-342-273; 110-334-811-601-52X; 115-264-271-491-964; 119-897-190-401-141; 129-300-941-888-148; 136-379-861-657-372; 145-050-129-341-281; 149-313-894-223-810; 176-334-018-120-549,61,true,cc-by,gold
007-934-058-811-388,LLM-Based multi-agent system for individual investment in energy and natural resources,2025-04-07,2025,journal article,International Journal of Electronics and Telecommunications,23001933; 20818491,Polish Academy of Sciences Chancellery,Germany,Alexander Zaleski; Jaroslaw A. Chudziak,"<jats:p>Recent advancements in large language models and multiagent large language model based systems show that these technologies can be applied to a large number of problems. They can automate complex tasks and perform advanced analyses that would take an expert a significant amount of time. This article describes a multiagent large language model (LLM) based platform for investment advisory in the energy natural resources sector. The system integrates multiple types of investment analyses e.g. technical analysis, fundamental analysis, sentiment analysis and stock price prediction. The approach of integrating multiple types of analyses in one system allows the investor to save significant amount of time on analyzing potential investments.</jats:p>",,,11,11,Investment (military); Natural resource; Energy (signal processing); Business; Environmental economics; Natural (archaeology); Computer science; Knowledge management; Industrial organization; Commerce; Economics; Political science; Geography; Statistics; Mathematics; Archaeology; Politics; Law,,,,,,http://dx.doi.org/10.24425/ijet.2025.153538,,10.24425/ijet.2025.153538,,,0,,2,true,cc-by-nc-nd,gold
008-154-776-106-420,"A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial
  Text Classification",2024-11-04,2024,preprint,arXiv (Cornell University),,,,Sorouralsadat Fatemi; Yuheng Hu; Maryam Mousavi,"Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.",,,,,Computer science; Finance; Business,,,,,https://arxiv.org/abs/2411.02476,http://dx.doi.org/10.48550/arxiv.2411.02476,,10.48550/arxiv.2411.02476,,,0,,1,true,,green
008-475-517-564-854,"Revisiting Privacy, Utility, and Efficiency Trade-offs when Fine-Tuning
  Large Language Models",2025-02-18,2025,preprint,arXiv (Cornell University),,,,Soumi Das; Camila Kolling; Mohammad Aflah Khan; Mahsa Amani; Bishwamittra Ghosh; Qinyuan Wu; Till Speicher; Krishna P. Gummadi,"We study the inherent trade-offs in minimizing privacy risks and maximizing utility, while maintaining high computational efficiency, when fine-tuning large language models (LLMs). A number of recent works in privacy research have attempted to mitigate privacy risks posed by memorizing fine-tuning data by using differentially private training methods (e.g., DP), albeit at a significantly higher computational cost (inefficiency). In parallel, several works in systems research have focussed on developing (parameter) efficient fine-tuning methods (e.g., LoRA), but few works, if any, investigated whether such efficient methods enhance or diminish privacy risks. In this paper, we investigate this gap and arrive at a surprising conclusion: efficient fine-tuning methods like LoRA mitigate privacy risks similar to private fine-tuning methods like DP. Our empirical finding directly contradicts prevailing wisdom that privacy and efficiency objectives are at odds during fine-tuning. Our finding is established by (a) carefully defining measures of privacy and utility that distinguish between memorizing sensitive and non-sensitive tokens in training and test datasets used in fine-tuning and (b) extensive evaluations using multiple open-source language models from Pythia, Gemma, and Llama families and different domain-specific datasets.",,,,,Computer science,,,,,https://arxiv.org/abs/2502.13313,http://dx.doi.org/10.48550/arxiv.2502.13313,,10.48550/arxiv.2502.13313,,,0,,0,true,,green
008-477-454-011-505,"LARGE LANGUAGE MODELS IN FINANCIAL STATEMENT ANALYSIS: A SYSTEMATIC REVIEW OF RECENT ADVANCES, PRACTICAL IMPLICATIONS, AND FUTURE RESEARCH",2025-03-27,2025,review,Scientific papers OF DMYTRO MOTORNYI TAVRIA STATE AGROTECHNOLOGICAL UNIVERSITY (ECONOMIC SCIENCES),2519884x,,,Darya Trachova; Oksana Lysak,"This systematic literature review examines how Large Language Models (LLMs) have transformed financial statement analysis by integrating narrative (textual) and quantitative data. Focusing on publications from 2017 to the present, we identified peer-reviewed articles, working papers, and conference proceedings from leading databases (Scopus, Web of Science, SSRN, and Google Scholar). Our review highlights four principal areas where LLMs have shown particular promise: risk and fraud detection, narrative summarization and sentiment analysis, Environmental, Social, and Governance (ESG) and sustainability reporting, and the integration of textual disclosures with traditional accounting metrics. These models – ranging from general-purpose Transformers (e.g., GPT, BERT) to specialized financial variants (e.g., FinBERT) – often outperform earlier machine learning approaches in tasks requiring nuanced linguistic understanding, but face challenges such as domain adaptation, interpretability, and potential model biases. In synthesizing existing studies, we observe a growing trend toward using domain-specific LLMs that can handle both unstructured narrative text (e.g., annual reports, footnotes) and structured financial data, thereby offering richer insights for auditors, analysts, and investors. However, empirical findings reveal critical concerns regarding data availability, reproducibility, and regulatory compliance. We conclude by suggesting avenues for future research, including the development of standardized financial statement corpora for training robust LLMs, the refinement of explainability tools suitable for high-stakes decision-making, and the exploration of ethical and governance frameworks to mitigate the risks of algorithmic bias. Overall, this review underscores the transformative potential of LLMs for accounting and finance, while cautioning against uncritical deployment in sensitive settings. Large Language Models (LLMs) are transforming financial analysis by enhancing risk detection, fraud prevention, sentiment analysis, and ESG reporting. They integrate textual and quantitative data, improving auditing and financial statement analysis. Transformer-based NLP models like FinBERT enable deeper insights into financial documents, ensuring more accurate decision-making in the financial sector.",,1 (54),40,46,Statement (logic); Management science; Computer science; Economics; Linguistics; Philosophy,,,,,,,,,,,0,,0,false,,
008-680-120-887-180,Enhancing Graph Database Interaction through Generative AI-Driven Natural Language Interface for Financial Fraud Detection,2024-06-24,2024,conference proceedings article,2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),,IEEE,,T Simran; J Geetha,,,,1,8,Computer science; Generative grammar; Interface (matter); Natural language; Natural language processing; Artificial intelligence; Graph; Theoretical computer science; Operating system; Bubble; Maximum bubble pressure method,,,,,,http://dx.doi.org/10.1109/icccnt61001.2024.10725408,,10.1109/icccnt61001.2024.10725408,,,0,013-457-095-177-898; 016-177-373-839-920; 017-682-836-163-263; 028-780-299-212-83X; 065-698-390-715-082; 067-166-653-952-107; 069-628-333-662-799; 112-015-745-168-645; 124-754-507-664-090; 135-281-640-668-391; 141-369-955-826-562; 156-765-030-525-177; 162-050-261-912-64X; 179-370-297-621-422; 182-065-086-203-893; 192-806-908-794-847,2,false,,
009-303-540-615-467,Secure and Explainable GenAI Chatbot for Financial Document Analysis,2025-01-01,2025,preprint,,,Elsevier BV,,Anil Kumar Shukla,,,,,,Chatbot; Business; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.2139/ssrn.5265667,,10.2139/ssrn.5265667,,,0,,0,false,,
009-338-453-559-575,Evaluation of Large Language Models for Extracting Financial Data from Tables in Annual Reports,,2025,journal article,Advances in Artificial Intelligence and Machine Learning,25829793,Advances in Artificial Intelligence and Machine Learning,,Michael Andreas Buholzer; Frederico Fischer; Geremia Simonella; Thomas Hanne,"<jats:p>This study evaluates the performance of extracting data from tables using three large language models (LLMs), namely ChatGPT 4, Custom GPT based on ChatGPT 4, and ChatPDF, in extracting and interpreting quantitative data from tables in financial reports. The models were tested on six questions regarding financial data with varying levels of difficulty using three financial reports from different industries and provided in different formats. The results are compared in terms of accuracy, precision, error rates, and qualitative analysis of the output quality. The results indicate that LLMs have a very limited ability to correctly read and interpret data from tables using annual reports. The study also showed that the same reports including the text yielded better results than the tables alone. The results also indicated that a more specific query can lead to slightly better results. However, the study shows that the current LLM technology is still unsuitable for practical applications in similar use cases related to table extraction, in particular where a high reliability of results is required. Thus, the study suggests that future research should focus on improving the capabilities of LLMs in financial data analysis, including the development of more advanced techniques for data extraction and interpretation.</jats:p>",5,1,3314,3343,Computer science; Natural language processing; Data science,,,,,,http://dx.doi.org/10.54364/aaiml.2025.51190,,10.54364/aaiml.2025.51190,,,0,,0,false,,
009-429-532-265-827,Improving Zero-Shot Text Matching for Financial Auditing with Large Language Models,2023-08-22,2023,conference proceedings article,Proceedings of the ACM Symposium on Document Engineering 2023,,ACM,,Lars Hillebrand; Armin Berger; Tobias Deußer; Tim Dilmaghani; Mohamed Khaled; Bernd Kliem; Rüdiger Loitz; Maren Pielka; David Leonhard; Christian Bauckhage; Rafet Sifa,"Auditing financial documents is a very tedious and time-consuming process. As of today, it can already be simplified by employing AI-based solutions to recommend relevant text passages from a report for each legal requirement of rigorous accounting standards. However, these methods need to be fine-tuned regularly, and they require abundant annotated data, which is often lacking in industrial environments. Hence, we present ZeroShotALI, a novel recommender system that leverages a state-of-the-art large language model (LLM) in conjunction with a domain-specifically optimized transformer-based text-matching solution. We find that a two-step approach of first retrieving a number of best matching document sections per legal requirement with a custom BERT-based model and second filtering these selections using an LLM yields significant performance improvements over existing approaches.",,,1,4,Computer science; Transformer; Language model; Audit; Matching (statistics); Process (computing); Natural language processing; Information retrieval; Artificial intelligence; Data mining; Accounting; Programming language; Engineering; Statistics; Mathematics; Voltage; Electrical engineering; Business,,,,,https://dl.acm.org/doi/pdf/10.1145/3573128.3609344 https://doi.org/10.1145/3573128.3609344 https://arxiv.org/pdf/2308.06111 https://arxiv.org/abs/2308.06111,http://dx.doi.org/10.1145/3573128.3609344,,10.1145/3573128.3609344,,,0,028-177-414-077-965; 032-509-322-900-566; 068-864-486-586-772; 113-961-214-877-335; 125-007-830-851-395; 150-612-233-271-263; 189-973-804-567-981,13,true,cc-by,hybrid
010-035-339-670-154,Enhancing IT Project Portfolio Management Through Dynamic Dashboard Visualization and Integrated Chatbot and Mathematical ML Model for Strategic Decision Making,2024-09-25,2024,journal article,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,25823930,Edtech Publishers (OPC) Private Limited,,Pravesh Senthil; DR. J. K Verma,"<jats:p>This project paper presents an innovative approach to enhance IT Project Portfolio Management (IT PPM) using dynamic dashboard visualization and machine learning (ML) techniques. Effective IT PPM is essential for organizations to align IT projects with business objectives and optimize resource allocation. However, conventional methods often lack innovation and real-time visibility, leading to suboptimal decision-making and costly software investments. The objective of this project is to develop a dynamic dashboard visualization system integrated with a machine learning model derived from the Meta-Portfolio Method (MPM). The MPM leverages advanced analytics and historical project data to dynamically allocate resources and prioritize projects based on risk-return profiles. By incorporating the MPM into the dashboard, organizations gain real-time insights into project status, progress, and resource allocation, enabling informed decision-making and proactive portfolio optimization. The ML model derived from the MPM analyzes historical project data, including success rates, resource intensity, criticality, regulatory compliance, and portfolio diversification, to predict optimal project selection and resource allocation. By dynamically adapting to changing market conditions and project dynamics, the ML model facilitates strategic decision-making and enhances portfolio performance. Through the integration of dynamic dashboard visualization and ML-driven decision support, organizations can streamline IT PPM processes, improve project outcomes, and achieve strategic objectives effectively and efficiently. This innovative approach empowers organizations to navigate the complexities of IT project portfolios with agility, foresight, and confidence.</jats:p>",8,9,1,9,Visualization; Project portfolio management; Chatbot; Computer science; Portfolio; Dashboard; Dynamic decision-making; Process management; Knowledge management; Engineering management; Operations research; Management science; Software engineering; Project management; Engineering; Systems engineering; Business; World Wide Web; Artificial intelligence; Finance,,,,,,http://dx.doi.org/10.55041/ijsrem37619,,10.55041/ijsrem37619,,,0,,0,false,,
010-953-232-488-494,"Position paper: GPT conjecture: understanding the trade-offs between granularity, performance and timeliness in control-flow integrity",2021-09-16,2021,journal article,Cybersecurity,25233246,Springer Science and Business Media LLC,,Zhilong Wang; Peng Liu,"Performance/security trade-off is widely noticed in CFI research, however, we observe that not every CFI scheme is subject to the trade-off. Motivated by the key observation, we ask three questions: ➊ does trade-off really exist in different CFI schemes? ➋ if trade-off do exist, how do previous works comply with it? ➌ how can it inspire future research? Although the three questions probably cannot be directly answered, they are inspiring. We find that a deeper understanding of the nature of the trade-off will help answer the three questions. Accordingly, we proposed the GPT conjecture to pinpoint the trade-off in designing CFI schemes, which says that at most two out of three properties (fine granularity, acceptable performance, and preventive protection) could be achieved.",4,1,33,,Granularity; Key (cryptography); Scheme (programming language); Subject (documents); Control-flow integrity; Conjecture; Computer security; Computer science; Position paper; Ask price,Conjecture; Control-flow integrity; Trade-off,,,Army Research Office; National Science Foundation; National Science Foundation,https://doi.org/10.1186/s42400-021-00098-2 https://cybersecurity.springeropen.com/articles/10.1186/s42400-021-00098-2 https://dblp.uni-trier.de/db/journals/cybersec/cybersec4.html#WangL21,http://dx.doi.org/10.1186/s42400-021-00098-2,,10.1186/s42400-021-00098-2,3199541521,,0,000-823-550-652-52X; 001-052-309-165-403; 002-770-373-452-178; 007-393-118-092-583; 007-514-931-491-85X; 007-742-877-016-058; 008-035-764-196-711; 010-748-261-933-830; 011-997-543-652-467; 012-904-563-160-954; 022-009-050-597-167; 026-286-540-185-473; 027-722-844-855-581; 028-479-924-724-521; 033-227-885-328-533; 033-835-220-422-049; 034-916-320-484-379; 037-675-453-559-690; 038-975-805-630-480; 040-702-625-233-021; 045-580-952-295-667; 047-697-110-305-752; 047-791-031-284-114; 048-063-903-859-87X; 060-040-887-973-412; 060-707-544-279-528; 062-884-139-071-053; 063-790-834-939-319; 065-991-008-490-699; 077-273-733-985-052; 082-527-236-362-936; 085-749-930-707-255; 085-898-888-580-412; 089-623-507-149-374; 089-942-191-098-23X; 103-317-096-954-79X; 105-619-153-441-904; 113-482-709-668-924; 114-872-390-648-621; 116-136-129-754-823; 116-638-632-122-911; 127-293-438-443-410; 129-142-590-414-707; 129-734-876-455-650; 132-981-381-811-322; 134-429-333-061-559; 145-168-222-392-677; 149-795-268-415-967; 151-990-428-550-144; 168-253-216-307-860; 170-099-353-437-126; 172-582-172-572-524; 193-590-525-980-606,0,true,cc-by,gold
011-094-910-705-894,Biased echoes: Large language models reinforce investment biases and increase portfolio risks of private investors.,2025-06-27,2025,journal article,PloS one,19326203,Public Library of Science (PLoS),United States,Philipp Winder; Christian Hildebrand; Jochen Hartmann,"Large language models are increasingly used by private investors seeking financial advice. The current paper examines the potential of these models to perpetuate investment biases and affect the economic security of individuals at scale. We provide a systematic assessment of how large language models used for investment advice shape the portfolio risks of private investors. We offer a comprehensive model of large language model investment advice risk, examining five key dimensions of portfolio risks (geographical cluster risk, sector cluster risk, trend chasing risk, active investment allocation risk, and total expense risk). We demonstrate across four studies that large language models used for investment advice induce increased portfolio risks across all five risk dimensions, and that a range of debiasing interventions only partially mitigate these risks. Our findings show that large language models exhibit similar ""cognitive"" biases as human investors, reinforcing existing investment biases inherent in their training data. These findings have important implications for private investors, policymakers, artificial intelligence developers, financial institutions, and the responsible development of large language models in the financial sector.",20,6,e0325459,e0325459,Debiasing; Investment (military); Portfolio; Investment decisions; Financial risk management; Financial risk; Actuarial science; Investment strategy; Business; Risk management; Finance; Economics; Behavioral economics; Psychology; Market liquidity; Politics; Political science; Law; Cognitive science,,"Investments/economics; Humans; Language; Models, Economic; Large Language Models",,Swiss National Science Foundation,,http://dx.doi.org/10.1371/journal.pone.0325459,40577333,10.1371/journal.pone.0325459,,PMC12204588,0,001-600-650-191-247; 003-694-668-365-770; 004-239-413-234-842; 007-170-216-169-697; 010-845-736-556-579; 011-223-654-787-20X; 014-114-606-107-800; 016-556-654-810-446; 016-963-858-082-354; 018-818-258-567-501; 019-944-174-992-387; 023-511-283-577-749; 023-853-922-616-880; 027-476-689-288-538; 028-842-156-840-127; 029-132-587-935-857; 034-316-513-230-704; 039-255-997-298-072; 045-354-600-604-686; 045-474-061-435-241; 049-594-733-994-399; 051-201-561-722-62X; 055-672-688-192-976; 062-156-293-489-532; 063-104-480-070-392; 063-247-351-637-805; 063-375-574-125-23X; 063-836-720-391-245; 068-509-377-925-943; 068-776-976-565-835; 070-352-566-091-350; 071-602-982-256-295; 082-030-949-671-399; 084-296-717-156-065; 098-912-883-994-480; 102-847-854-916-71X; 121-323-404-465-87X; 124-716-395-176-545; 128-079-345-767-474; 130-185-490-312-282; 134-559-793-888-042; 143-218-672-931-252; 144-996-450-075-54X; 147-270-069-744-288; 154-513-179-142-971; 163-298-324-923-115; 167-120-412-853-011; 170-766-063-361-49X; 173-360-116-106-820; 189-464-632-085-955,0,true,cc-by,gold
011-430-951-131-989,Equity Research Chatbot Using LLM: A Responsive Agent for Investment Research,2024-12-13,2024,conference proceedings article,2024 IEEE Pune Section International Conference (PuneCon),,IEEE,,Nikhil Dongare; Amey Bhirange; Shreyas Kharche; Amar Buchade; Parikshit Mahalle,,,,1,5,Chatbot; Computer science; Investment (military); Equity (law); Knowledge management; World Wide Web; Political science; Politics; Law,,,,,,http://dx.doi.org/10.1109/punecon63413.2024.10895230,,10.1109/punecon63413.2024.10895230,,,0,002-126-877-983-127; 010-249-592-715-348; 041-758-231-782-182; 079-376-808-859-849; 103-814-985-328-817; 134-136-252-227-167; 159-794-184-450-739,1,false,,
011-694-248-388-802,Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning,2025-09-14,2025,preprint,arXiv (Cornell University),,,,Yijia Xiao; Edward Sun; Tong Chen; Fang Wu; Di Luo; Wei Wang,"Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.",,,,,Reinforcement learning; Algorithmic trading; Reinforcement; Computer science; Business; Finance; Artificial intelligence; Psychology; Social psychology,,,,,https://arxiv.org/abs/2509.11420,http://dx.doi.org/10.48550/arxiv.2509.11420,,10.48550/arxiv.2509.11420,,,0,,0,true,,green
011-880-790-133-294,Research on the Construction and Optimization Algorithm of Large Language Model in Financial Text Analysis,2025-03-17,2025,conference proceedings article,"2025 IEEE International Conference on Electronics, Energy Systems and Power Engineering (EESPE)",,IEEE,,Zhu Chaoyong; Ma Jian; Jiao Lingfeng,,,,536,540,Computer science; Natural language processing; Algorithm,,,,,,http://dx.doi.org/10.1109/eespe63401.2025.10987098,,10.1109/eespe63401.2025.10987098,,,0,,0,false,,
011-927-650-358-26X,FinGPT: Open-Source Financial Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Hongyang Yang; Xiao-Yang Liu; Christina Dan Wang,"Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \url{https://github.com/AI4Finance-Foundation/FinGPT} and \url{https://github.com/AI4Finance-Foundation/FinNLP}",,,,,Computer science; Source code; Pipeline (software); Data science; Open source; Adaptation (eye); Open data; World Wide Web; The Internet; Code (set theory); Data source; Finance; Financial modeling; Database; Business; Software; Programming language; Physics; Set (abstract data type); Optics,,,,,https://arxiv.org/abs/2306.06031,http://dx.doi.org/10.48550/arxiv.2306.06031,,10.48550/arxiv.2306.06031,,,0,,3,true,other-oa,green
011-997-397-128-172,Research on Personalized Financial Product Recommendation by Integrating Large Language Models and Graph Neural Networks,2025-06-13,2025,conference proceedings article,Proceedings of the 2025 International Conference on Software Engineering and Computer Applications,,ACM,,Yushang Zhao; Yike Peng; Dannier Li; Yuxin Yang; Chengrui Zhou; Jing Dong,,,,53,59,Computer science; Artificial neural network; Product (mathematics); Graph; Artificial intelligence; Theoretical computer science; Mathematics; Geometry,,,,,,http://dx.doi.org/10.1145/3747912.3747920,,10.1145/3747912.3747920,,,0,032-253-754-171-161; 041-973-379-481-986; 049-030-344-981-89X; 069-958-433-142-59X; 098-189-766-298-62X; 111-955-892-710-979; 113-733-939-965-098; 188-335-787-757-261,5,false,,
012-477-451-286-452,CryptoTrade: A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading,,2024,conference proceedings article,Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,,Association for Computational Linguistics,,Yuan Li; Bingqiao Luo; Qian Wang; Nuo Chen; Xu Liu; Bingsheng He,,,,1094,1106,Cryptocurrency; Shot (pellet); Zero (linguistics); Computer science; Zero-knowledge proof; Computer security; Cryptography; Materials science; Linguistics; Philosophy; Metallurgy,,,,,,http://dx.doi.org/10.18653/v1/2024.emnlp-main.63,,10.18653/v1/2024.emnlp-main.63,,,0,,3,false,,
012-726-411-192-536,Automate Strategy Finding with LLM in Quant investment,2024-09-10,2024,preprint,arXiv (Cornell University),,,,Zhizhuo Kou; Holam Yu; Junyu Luo; Jingshu Peng; Xujia Li; Chengzhong Liu; Juntao Dai; Lei Chen; Sirui Han; Yike Guo,"Despite significant progress in deep learning for financial trading, existing models often face instability and high uncertainty, hindering their practical application. Leveraging advancements in Large Language Models (LLMs) and multi-agent architectures, we propose a novel framework for quantitative stock investment in portfolio management and alpha mining. Our framework addresses these issues by integrating LLMs to generate diversified alphas and employing a multi-agent approach to dynamically evaluate market conditions. This paper proposes a framework where large language models (LLMs) mine alpha factors from multimodal financial data, ensuring a comprehensive understanding of market dynamics. The first module extracts predictive signals by integrating numerical data, research papers, and visual charts. The second module uses ensemble learning to construct a diverse pool of trading agents with varying risk preferences, enhancing strategy performance through a broader market analysis. In the third module, a dynamic weight-gating mechanism selects and assigns weights to the most relevant agents based on real-time market conditions, enabling the creation of an adaptive and context-aware composite alpha formula. Extensive experiments on the Chinese stock markets demonstrate that this framework significantly outperforms state-of-the-art baselines across multiple financial metrics. The results underscore the efficacy of combining LLM-generated alphas with a multi-agent architecture to achieve superior trading performance and stability. This work highlights the potential of AI-driven approaches in enhancing quantitative investment strategies and sets a new benchmark for integrating advanced machine learning techniques in financial trading can also be applied on diverse markets.",,,,,Investment (military); Computer science; Political science; Law; Politics,,,,,https://arxiv.org/abs/2409.06289,http://dx.doi.org/10.48550/arxiv.2409.06289,,10.48550/arxiv.2409.06289,,,0,,1,true,,green
012-862-153-339-668,"DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large
  Language Models",2025-04-22,2025,preprint,arXiv (Cornell University),,,,Jie Zhu; Qian Chen; Huaixia Dou; Junhui Li; Lifan Guo; Feng Chen; Chi Zhang,"Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.",,,,,Computer science; Business; Finance,,,,,https://arxiv.org/abs/2504.15716,http://dx.doi.org/10.48550/arxiv.2504.15716,,10.48550/arxiv.2504.15716,,,0,,1,true,,green
012-920-973-366-007,Enhancing Sentiment Analysis based Investment by Large Language Models in Japanese Stock Market,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Masafumi Nakano; Takuya Yamaoka,,,,,,Stock market; Financial economics; Sentiment analysis; Business; Investment (military); Stock (firearms); Economics; Econometrics; Linguistics; Computer science; Natural language processing; Political science; Engineering; Geography; Mechanical engineering; Philosophy; Context (archaeology); Archaeology; Politics; Law,,,,,,http://dx.doi.org/10.2139/ssrn.4511658,,10.2139/ssrn.4511658,,,0,000-454-346-263-577; 004-877-171-081-463; 018-193-483-515-989; 020-652-182-557-810; 022-486-017-682-206; 024-431-940-748-541; 044-702-284-074-338; 047-871-035-706-003; 050-233-396-111-576; 053-378-511-097-658; 061-559-088-082-928; 081-162-489-638-288; 082-945-313-450-254; 096-239-462-922-956; 103-307-604-166-441; 126-155-710-278-695; 134-643-111-977-50X; 159-295-887-189-841; 190-624-303-228-420,2,false,,
013-439-172-059-942,"Evolving trends, limitations, and ethical considerations in AI-driven conversational interfaces: assessing ChatGPT's impact on healthcare, financial services, and educational sectors",2024-11-04,2024,journal article,Technology Analysis & Strategic Management,09537325; 14653990,Informa UK Limited,United Kingdom,Mohd Afjal,,37,13,3712,3731,,,,,,,http://dx.doi.org/10.1080/09537325.2024.2420617,,10.1080/09537325.2024.2420617,,,0,000-125-187-700-508; 003-134-025-839-770; 005-077-699-617-861; 008-708-960-845-751; 011-252-598-016-947; 011-505-875-238-365; 012-113-843-197-240; 014-900-812-798-858; 023-478-802-370-116; 049-716-552-536-295; 049-919-019-294-013; 059-149-073-001-124; 064-704-230-329-74X; 067-573-393-305-736; 071-295-657-462-055; 079-436-597-029-678; 090-079-158-017-179; 132-781-341-881-928; 142-862-806-335-247; 154-910-565-038-477; 155-196-756-671-217; 178-551-929-566-887; 193-665-658-851-932,1,false,,
013-524-539-174-944,"Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA",2025-10-02,2025,preprint,arXiv (Cornell University),,,,Prudence Djagba; Abdelkader Y. Saley,"This research explores the strengths and weaknesses of domain-adapted Large Language Models (LLMs) in the context of financial natural language processing (NLP). The analysis centers on FinMA, a model created within the PIXIU framework, which is evaluated for its performance in specialized financial tasks. Recognizing the critical demands of accuracy, reliability, and domain adaptation in financial applications, this study examines FinMA's model architecture, its instruction tuning process utilizing the Financial Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark. Findings indicate that FinMA performs well in sentiment analysis and classification, but faces notable challenges in tasks involving numerical reasoning, entity recognition, and summarization. This work aims to advance the understanding of how financial LLMs can be effectively designed and evaluated to assist in finance-related decision-making processes.",,,,,Computer science; Finance; Business,,,,,https://arxiv.org/abs/2510.05151,http://dx.doi.org/10.48550/arxiv.2510.05151,,10.48550/arxiv.2510.05151,,,0,,0,true,,green
013-593-565-374-359,Digital Transformation in Financial Services Provision: A Nigerian Perspective to the Adoption of Chatbot,2020-10-27,2020,,Social Science Research Network,,,,Emmanuel Mogaji,"Purpose: Recognizing the high numbers of unbanked and financially excluded adults in Nigeria, this study positions chatbot as a digital transformation tool to radically change business model, improve customer experience, and enhance financial inclusion in emerging markets.; ; Methodology: The Search-Access-Test (S-A-T) model was adopted to understand how Nigerian banks are adopting chatbots.; ; Findings: Majority of Nigerian banks now have chatbots which enhance customer engagement and financial inclusion. WhatsApp was the most frequently used platform. Chatbots were often branded and presented with female gender identification. The chatbots were less responsive beyond their pre-defined path. While Nigeria is a multilingual country with English being the original language, none of the chatbots used any of the Nigerian’s local languages.; ; Originality: While many theoretically based model for investigating the adoption of digital technologies has often placed focus on users’ ability to engage, this study takes an alternative perspective; by using the Search-Access-Test (S-A-T) model, it lays the responsibilities on the banks and chatbot developer to ensure that their chatbots are secure, responsive and able to meet the needs of the customers.; ; Practical implications: Brands needs to reevaluate their chatbots with regards to responsiveness, pre-defined questions, verification and privacy. There are also possibilities of branding the chatbot and developing content creation strategies for proper engagement. Beyond English, the integration of African languages into chatbot is essential for digital transformation. Digital literacy and skills, particularly in the field of science, technology, engineering and mathematics (STEM) should be supported to equip future developers and create more jobs.",,,,,Digital transformation; Financial services; Content creation; Customer engagement; Digital literacy; Chatbot; Financial inclusion; Public relations; Business model,,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3720236,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3720236,,,3168949033,,0,,0,false,,
013-690-630-484-680,Enhancing Financial Analysis with Generative AI: Utilizing Large Language Models for Efficient Data Extraction,2025-04-17,2025,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Switzerland,Germany,Soham Agarwal; Durga Madhab Dash; Anto Fredric Henry Mohan Dass; Sai Bheeshma Ramaraju Pagilla; Chaitanya Varma Sanaboina; Matthew A. Lanham; Ashwin Mishra,,,,244,259,Computer science; Generative grammar; Extraction (chemistry); Natural language processing; Artificial intelligence; Information extraction; Chemistry; Chromatography,,,,,,http://dx.doi.org/10.1007/978-3-031-85856-7_19,,10.1007/978-3-031-85856-7_19,,,0,055-318-598-916-211,0,false,,
013-859-022-210-96X,Equity Aware Peer-to-Peer Energy Trading Market to Mitigate Energy Poverty: A n LLM - RL Agentic Workflow,,2025,journal article,"IEEE Transactions on Energy Markets, Policy and Regulation",27719626,Institute of Electrical and Electronics Engineers (IEEE),,Qianyi Ma; Zifa Liu; Xiao Liu; Ziqing Zhu; Siqi Bu,,,,1,15,,,,,,,http://dx.doi.org/10.1109/tempr.2025.3633992,,10.1109/tempr.2025.3633992,,,0,,0,false,,
013-922-542-090-50X,Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading,2025-08-08,2025,preprint,arXiv (Cornell University),,,,Lang Cao,"Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.",,,,,Alpha (finance); Power (physics); Computer science; Business; Physics; Marketing; Construct validity; Quantum mechanics; Patient satisfaction,,,,,https://arxiv.org/abs/2508.06312,http://dx.doi.org/10.48550/arxiv.2508.06312,,10.48550/arxiv.2508.06312,,,0,,1,true,,green
014-658-406-414-57X,Trade-Offs Between Traditional LLM and Chain-of-Thought Reasoning for Mathematical Tasks,2025-10-22,2025,conference proceedings article,"2025 IEEE 16th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",,IEEE,,Dharani Ravanam; Shivanjali Khare,,,,540,545,,,,,,,http://dx.doi.org/10.1109/uemcon67449.2025.11267619,,10.1109/uemcon67449.2025.11267619,,,0,004-563-801-130-046; 013-011-851-213-158; 111-927-924-258-238; 151-362-361-377-836,0,false,,
014-677-162-980-770,CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Yang Lei; Jiangtong Li; Dawei Cheng; Zhijun Ding; Changjun Jiang,"Large language models (LLMs) have demonstrated great potential in the financial domain. Thus, it becomes important to assess the performance of LLMs in the financial tasks. In this work, we introduce CFBenchmark, to evaluate the performance of LLMs for Chinese financial assistant. The basic version of CFBenchmark is designed to evaluate the basic ability in Chinese financial text processing from three aspects~(\emph{i.e.} recognition, classification, and generation) including eight tasks, and includes financial texts ranging in length from 50 to over 1,800 characters. We conduct experiments on several LLMs available in the literature with CFBenchmark-Basic, and the experimental results indicate that while some LLMs show outstanding performance in specific tasks, overall, there is still significant room for improvement in basic tasks of financial text processing with existing models. In the future, we plan to explore the advanced version of CFBenchmark, aiming to further explore the extensive capabilities of language models in more profound dimensions as a financial assistant in Chinese. Our codes are released at https://github.com/TongjiFinLab/CFBenchmark.",,,,,Benchmark (surveying); Finance; Computer science; Work (physics); Financial market; Plan (archaeology); Domain (mathematical analysis); Business; Engineering; Geography; Mechanical engineering; Geodesy; Archaeology; Mathematical analysis; Mathematics,,,,,https://arxiv.org/abs/2311.05812,http://dx.doi.org/10.48550/arxiv.2311.05812,,10.48550/arxiv.2311.05812,,,0,,1,true,other-oa,green
014-719-390-687-720,News sentiment and investment risk management: Innovative evidence from the large language models,,2025,journal article,Economics Letters,01651765; 18737374,Elsevier BV,Netherlands,Tong Liu; Yanlin Shi,,247,,112124,112124,Investment (military); Risk management; Economics; Business; Financial economics; Actuarial science; Finance; Political science; Politics; Law,,,,Macquarie University,,http://dx.doi.org/10.1016/j.econlet.2024.112124,,10.1016/j.econlet.2024.112124,,,0,004-047-916-650-710; 009-026-576-319-659; 014-605-767-028-018; 016-888-298-077-399; 038-229-585-139-07X; 040-556-924-679-266; 042-152-947-297-801; 043-183-819-506-881; 064-920-078-678-377; 091-697-140-316-685; 122-913-667-058-877; 124-058-858-151-378; 135-968-974-189-893; 149-614-865-755-788,1,true,cc-by,hybrid
014-853-701-193-048,Enhancing Trading Performance Through Sentiment Analysis with Large Language Models: Evidence from the S&P 500,2025-07-13,2025,preprint,arXiv (Cornell University),,,,Haojie Liu; Zihan Lin; Randall R. Rojas,"This study integrates real-time sentiment analysis from financial news, GPT-2 and FinBERT, with technical indicators and time-series models like ARIMA and ETS to optimize S&P 500 trading strategies. By merging sentiment data with momentum and trend-based metrics, including a benchmark buy-and-hold and sentiment-based approach, is evaluated through assets values and returns. Results show that combining sentiment-driven insights with traditional models improves trading performance, offering a more dynamic approach to stock trading that adapts to market changes in volatile environments.",,,,,Sentiment analysis; Computer science; Natural language processing,,,,,https://arxiv.org/abs/2507.09739,http://dx.doi.org/10.48550/arxiv.2507.09739,,10.48550/arxiv.2507.09739,,,0,,0,true,,green
014-888-742-797-139,Enhancing Financial Fraud Detection Using LLMs and Advanced Data Analytics,2024-10-23,2024,conference proceedings article,2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS),,IEEE,,Sukanth Korkanti,,,,1328,1334,Analytics; Financial fraud; Data science; Business; Computer science; Computer security; Finance; Accounting,,,,,,http://dx.doi.org/10.1109/icssas64001.2024.10760895,,10.1109/icssas64001.2024.10760895,,,0,,5,false,,
015-034-244-098-615,API Augmented Reinforcement Learning Framework Utilizing LLMs for Enhanced News-Based Stock Portfolio Strategies,2025-06-03,2025,journal article,International Journal on Advanced Computer Engineering and Communication Technology,22785140,Researcher Connect Innovation and Impact Private Limited and MRI,,R. A. Jamadar; Megha Pawar; Shraddha Pawaskar; Shruti Pitlellu; Avanti Shinde,"<jats:p>This paper introduces an API Augmented Reinforcement Learning (RL) framework that utilizes Large Language Models (LLMs) to enhance stock portfolio strategies by leveraging real-time news data. Traditional financial strategies primarily focus on historical and technical indicators; however, our approach integrates sentiment analysis and summarization of news articles to influence reinforcement learning agents’ decision- making. News-based features, combined with market indicators, form a comprehensive state representation for training RL algorithms such as Proximal Policy Optimization (PPO). This hybrid system optimizes stock allocation dynamically, offering enhanced portfolio performance when evaluated against benchmark strategies.</jats:p>",14,1,478,484,,,,,,,http://dx.doi.org/10.65521/ijacect.v14i1.562,,10.65521/ijacect.v14i1.562,,,0,,0,false,,
015-246-195-452-343,An Evaluation of Large Language Models in Financial Sentiment Analysis,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Alphaeus Dmonte; Eunmi Ko; Marcos Zampieri,,,,4869,4874,Computer science; Sentiment analysis; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/bigdata62323.2024.10825272,,10.1109/bigdata62323.2024.10825272,,,0,004-853-485-857-424; 006-034-530-389-409; 006-399-176-500-152; 007-635-920-160-781; 007-814-872-111-101; 008-102-153-925-863; 018-866-913-622-957; 044-456-506-571-452; 057-982-454-558-150; 062-518-441-478-014; 075-404-683-985-349; 083-449-940-348-930; 087-548-501-519-629; 088-544-427-532-849; 089-003-392-887-045; 090-748-124-007-779; 118-558-821-254-086; 137-408-709-935-186; 140-164-683-409-785; 154-042-138-562-576; 160-450-547-554-956; 163-979-407-953-145; 181-432-462-344-434,5,false,,
015-856-723-476-284,Leveraging LLMs for Systematic Reviews: Evaluating Venture Capital Investment Strategies Across Large Datasets,2025-11-18,2025,conference proceedings article,2025 IEEE 25th International Symposium on Computational Intelligence and Informatics (CINTI),,IEEE,,Csaba Árenás; Tamás P. Haidegger; Zsombor Zrubka,,,,761,766,,,,,Innovation Fund,,http://dx.doi.org/10.1109/cinti67731.2025.11311772,,10.1109/cinti67731.2025.11311772,,,0,001-808-464-827-574; 008-048-373-142-102; 061-977-298-391-413; 065-010-405-236-060; 086-497-113-910-913; 106-583-123-969-016; 166-067-256-568-705; 186-837-479-510-715; 198-407-208-116-797,0,false,,
016-183-452-285-613,Natural Language Querying of Invoice Data Using RAG and GraphRAG: Leveraging LLMs for Financial Document Insights,2025-06-14,2025,book chapter,Lecture Notes in Business Information Processing,18651348; 18651356,Springer Nature Switzerland,Germany,Teodora Cristiana Nemtoc; Ana-Maria Ghiran,,,,69,80,Invoice; Computer science; Natural (archaeology); Natural language; World Wide Web; Business; Natural language processing; Geography; Archaeology,,,,,,http://dx.doi.org/10.1007/978-3-031-94931-9_6,,10.1007/978-3-031-94931-9_6,,,0,058-899-820-944-449; 062-730-151-814-401; 114-421-350-805-749; 161-573-893-721-370; 167-224-544-774-901; 182-432-285-652-255,2,false,,
016-213-057-537-934,Leveraging Large Language Models and Deep Learning for Detecting Illegal Insider Trading,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Anoop Adusumilli; Sheikh Rabiul Islam; Iman Dehzangi; June Kim,,,,4809,4818,Computer science; Insider trading; Deep learning; Computer security; Insider threat; Artificial intelligence; Insider; Business; Finance; Political science; Law,,,,,,http://dx.doi.org/10.1109/bigdata62323.2024.10825461,,10.1109/bigdata62323.2024.10825461,,,0,011-332-406-581-256; 024-466-833-424-71X; 025-471-613-251-071; 042-590-409-353-239; 053-400-050-071-460; 065-861-490-550-091; 077-914-773-059-659; 081-077-801-039-997; 104-576-581-824-967; 108-208-427-337-016; 133-400-890-640-849; 181-824-951-331-629,1,false,,
016-659-319-252-530,Financial Advisory LLM Model for Modernizing Financial Services and Innovative Solutions for Financial Literacy in India,2024-05-03,2024,preprint,,,Research Square Platform LLC,,BHAVITH CHANDRA CHALLAGUNDLA; GURURANGA RAMANUJAM B,"<title>Abstract</title>;         <p>Dynamically evolving financial conditions in India place sophisticated models of financial advisory services relative to its own peculiar conditions more in demand than ever. The paper meets this demand by providing an introduction to the Financial Advisory LLM Model. The Financial Advisory LLM Model integrates legal, financial, and management expertise into a coherent platform that the financial advisors can apply to guide their clients successfully. The Financial Advisory LLM Model uses emerging technologies, including AI and machine learning, to improve the process of advising and bring a good deal of efficiency and precision to the services provided. Integrating these advanced technologies within this model equips financial advisors with culturally appropriate and powerful tools to deliver tailor-made solutions to their clients across an evolving market landscape. This project report elaborately discusses the solutions provided through the Financial Advisory LLM Model, running from different financial challenges within the Indian context. These solutions cater to mega-areas such as investment management, wealth preservation, and tax optimization. These elements within the model provide the financial advisors with the potency to serve their clients with a very effective and efficient manner. The Financial Advisory LLM Model bases itself on profound insight into the Indian financial ecosystem. It considers the intricacies and peculiarities of this ecosystem, such as the regulatory framework, the market dynamic, and the cultural factors that shape the conditions of financial decision-making. At the same time, through such awareness of the unique aspects, the model becomes more appropriate for advice and more practical in the provision of financial advisory services. The essence of the Financial Advisory LLM Model is multidisciplinary and the combination of legal, financial, and management expertise. This approach helps financial advisors in possession of the best toolkit to deal with the needs of multidimensional clients. AI and ML technologies have been integrated into the model in order to improve the advisory process to the effect of speed that helps advisors to process vast quantities of data in record time and derive insights guiding them in their recommendations. One of the critical areas of the Financial Advisory LLM Model is investment management. Understanding India as critical for prudent strategies of investment, the model delivers tailor-made solutions that optimize clients' investment portfolios. The solutions consider how clients can optimize their financial goals regarding risk tolerance, the volatility of the market, and the constraints that regulatory bodies exert. The model also deals with the vital issue of wealth preservation. The changes taking place in the economic landscape make the preservation of wealth a crucial consideration for clients in India. The Financial Advisory LLM Model provides strategies that safeguard wealth through diversification, asset allocation, and risk management techniques tailored to the Indian context. Furthermore, tax optimization is a critical consideration for clients in India. The Financial Advisory LLM Model provides sophisticated tax planning strategies that help to minimize tax liabilities while maximizing returns. The strategies take into consideration the convoluted tax laws and regulations in India, making sure that clients are compliant but optimizing their tax efficiency. The Financial Advisory LLM model is a step in the right direction for financial advisory services in India. This model combines legal, financial, and management expertise with advanced technologies such as AI and ML to empower financial advisors to navigate the complexities of the Indian financial ecosystem. Accordingly, the paper has demonstrated how the model addresses the various financial challenges that enable the advisors to empower them to meet diverse client needs hence leading to better quality financial advisory services in India.</p>",,,,,Financial literacy; Finance; Business; Financial services,,,,,https://www.researchsquare.com/article/rs-4354348/latest.pdf https://doi.org/10.21203/rs.3.rs-4354348/v1,http://dx.doi.org/10.21203/rs.3.rs-4354348/v1,,10.21203/rs.3.rs-4354348/v1,,,0,060-975-152-229-020; 154-042-138-562-576,0,true,cc-by,green
016-938-308-196-336,Bridging Finance and AI: A Comprehensive Survey of Large Language Models in Financial Applications,2025-05-14,2025,preprint,,,Springer Science and Business Media LLC,,Ameer Tamoor Khak; Shuai Li; Xinwei Cao,"<title>Abstract</title>;         <p>Large Language Models (LLMs) have rapidly transformed natural language processing across domains, and their adoption in finance promises to revolutionize tasks ranging from document understanding to quantitative analysis. This survey presents a comprehensive overview of LLM applications in financial contexts. We first trace the evolution of language modeling from RNNs to Transformers and summarize key architectures (encoder-only, decoder-only, encoder-decoder) and transfer-learning paradigms (pretraining, fine-tuning, prompt-based, instruction tuning). We then propose a taxonomy of finance-relevant tasks, including linguistic preprocessing (summarization, NER), sentiment analysis, financial reasoning and QA, forecasting/time-series modeling, and agent-based decision support. Next, we review the landscape of datasets and benchmarks highlighting English-dominant biases, gaps in crisis-period coverage, and emerging multilingual resources. We catalog financial domain--specific LLMs, contrasting fine-tuned models (FinBERT, FinGPT, FinMA) with from-scratch systems (BloombergGPT, FinTral, XuanYuan 2.0) and compare them to general-purpose models. We examine evaluation metrics, contrasting standard ML measures (accuracy, F1, MSE) with finance-centric criteria (Sharpe Ratio, Maximum Drawdown) and discuss challenges around data leakage, hallucinations, robustness, and explainability. Ethical and regulatory considerations data privacy, bias, fairness, and compliance are addressed, followed by a multi-level decision framework (zero-shot through pretraining from scratch) that balances accuracy, cost, and privacy. Finally, we outline open challenges and future directions, including cross-lingual models, symbolic reasoning integration, human--AI collaboration, and the development of open, temporally annotated finance benchmarks. This survey aims to guide researchers and practitioners in responsibly harnessing LLMs to advance financial AI.</p>",,,,,Bridging (networking); Finance; Business; Computer science; Computer network,,,,,,http://dx.doi.org/10.21203/rs.3.rs-6614220/v1,,10.21203/rs.3.rs-6614220/v1,,,0,,3,false,,
017-042-232-501-81X,Large Language Models as Financial Analysts,2024-01-01,2024,preprint,,,Elsevier BV,,Miquel Noguer i Alonso; Hanane Dupouy,,,,,,Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.4945481,,10.2139/ssrn.4945481,,,0,,0,false,,
017-163-325-534-291,Application and Modeling of LLM in Quantitative Trading Using Deep Learning Strategies,2024-02-14,2024,book chapter,Studies in Computational Intelligence,1860949x; 18609503,Springer Nature Switzerland,Germany,Tiejun Pan; Jinjie Yu; Leina Zheng; Yuejiao Li,,,,671,678,Artificial intelligence; Computer science; Deep learning; Machine learning,,,,,,http://dx.doi.org/10.1007/978-3-031-50381-8_72,,10.1007/978-3-031-50381-8_72,,,0,035-254-977-568-338; 073-738-617-359-179,0,false,,
017-181-442-874-083,Implementing Conversational AI in ERP Systems: A Technical Framework for Real-Time Financial Operations,2025-02-25,2025,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,null Bhagavathi Sathya Satish Kadiyala,"<jats:p>This article presents a comprehensive technical framework for implementing conversational AI interfaces within Enterprise Resource Planning (ERP) systems, focusing on real-time financial operations. The article examines how Natural Language Processing (NLP) technologies can transform traditional ERP interfaces into intuitive, conversation-driven systems that enhance user accessibility and operational efficiency. The article details the technical architecture, core capabilities, implementation methodologies, and business impact of integrating NLP-enhanced interfaces in enterprise environments. Through analysis of multiple enterprise implementations, the article demonstrates how conversational AI significantly improves user adoption, reduces training requirements, and enhances operational efficiency across financial processes. The article addresses critical aspects including security considerations, performance optimization, and scalability requirements while providing insights into the practical challenges and solutions for enterprise-scale deployments. The article reveals that NLP-enhanced ERP systems not only streamline financial operations but also democratize access to complex financial data, enabling more informed decision-making across organizational hierarchies while maintaining robust security and compliance standards.</jats:p>",11,1,3583,3593,Computer science; Finance; Software engineering; Business,,,,,,http://dx.doi.org/10.32628/cseit251112383,,10.32628/cseit251112383,,,0,141-779-272-905-677,0,true,,bronze
017-419-189-460-158,Human-Machine Collaboration in Healthcare Investment Integrating Large Language Models,2024-10-18,2024,conference proceedings article,2024 7th International Conference on Machine Learning and Natural Language Processing (MLNLP),,IEEE,,Hua Wang,,,,1,6,Health care; Computer science; Investment (military); Economics; Politics; Political science; Law; Economic growth,,,,,,http://dx.doi.org/10.1109/mlnlp63328.2024.10800279,,10.1109/mlnlp63328.2024.10800279,,,0,007-724-637-363-81X; 007-814-872-111-101; 044-438-658-121-670; 049-411-909-277-63X; 052-373-339-782-637; 079-205-045-518-125; 082-178-119-516-788; 089-897-532-399-365; 094-611-827-820-65X; 107-875-436-476-512; 157-612-711-865-854; 190-050-627-029-962,0,false,,
017-612-457-980-948,TradExpert: Revolutionizing Trading with Mixture of Expert LLMs,2024-10-16,2024,preprint,arXiv (Cornell University),,,,Qianggang Ding; Haochen Shi; Jiadong Guo; Bang Liu,"The integration of Artificial Intelligence (AI) in the financial domain has opened new avenues for quantitative trading, particularly through the use of Large Language Models (LLMs). However, the challenge of effectively synthesizing insights from diverse data sources and integrating both structured and unstructured data persists. This paper presents TradeExpert, a novel framework that employs a mix of experts (MoE) approach, using four specialized LLMs, each analyzing distinct sources of financial data, including news articles, market data, alpha factors, and fundamental data. The insights of these expert LLMs are further synthesized by a General Expert LLM to make a final prediction or decision. With specific prompts, TradeExpert can be switched between the prediction mode and the ranking mode for stock movement prediction and quantitative stock trading, respectively. In addition to existing benchmarks, we also release a large-scale financial dataset to comprehensively evaluate TradeExpert's effectiveness. Our experimental results demonstrate TradeExpert's superior performance across all trading scenarios.",,,,,Business; Commerce; Natural resource economics; Economics,,,,,https://arxiv.org/abs/2411.00782,http://dx.doi.org/10.48550/arxiv.2411.00782,,10.48550/arxiv.2411.00782,,,0,,0,true,,green
017-676-196-726-599,Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering,,2024,conference proceedings article,Findings of the Association for Computational Linguistics ACL 2024,,Association for Computational Linguistics,,Pragya Srivastava; Manuj Malik; Vivek Gupta; Tanuja Ganu; Dan Roth,,,,3853,3878,Question answering; Computer science; Information retrieval,,,,,,http://dx.doi.org/10.18653/v1/2024.findings-acl.231,,10.18653/v1/2024.findings-acl.231,,,0,,14,false,,
017-766-559-664-702,UX Evaluation of Financial Service Chatbot Interactions,2019-05-31,2019,journal article,Journal of the HCI Society of Korea,19760671; 26717611,The HCI Society of Korea,,Gukae Cho; Jae Young Yun,,14,2,61,69,World Wide Web; Interaction design; Financial services; Business; Chatbot,,,,,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002469072,http://dx.doi.org/10.17210/jhsk.2019.05.14.2.61,,10.17210/jhsk.2019.05.14.2.61,2949504473,,0,,4,false,,
017-998-123-804-340,The Impact of AI and Cross-Border Data Regulation on International Trade in Digital Services: A Large Language Model,,2023,report,,,National Bureau of Economic Research,,Ruiqi Sun; Daniel Trefler,"The rise of artificial intelligence (AI) and of cross-border restrictions on data flows has created a host of new questions and related policy dilemmas.This paper addresses two questions: How is digital service trade shaped by (1) AI algorithms and (2) by the interplay between AI algorithms and cross-border restrictions on data flows?Answers lie in the palm of your hand: From London to Lagos, mobile app users trigger international transactions when they open AI-powered foreign apps.We have 2015-2020 usage data for the most popular 35,575 mobile apps and, to quantify the AI deployed in each of these apps, we use a large language model (LLM) to link each app to each of the app developer's AI patents.(Thislinkage of specific products to specific patents is a methodological innovation.)Armed with data on app usage by country, with AI deployed in each app, and with an instrument for AI (a Heckscher-Ohlin cost-shifter), we answer our two questions.(1) On average, AI causally raises an app's number of foreign users by 2.67 log points or by more than 10-fold.(2) The impact of AI on foreign users is halved if the foreign users are in a country with strong restrictions on cross-border data flows.These countries are usually autocracies.We also provide a new way of measuring AI knowledge spillovers across firms and find large spillovers.Finally, our work suggests numerous ways in which LLMs such as ChatGPT can be used in other applications.",,,,,Computer science; International trade; Business,,,,,,http://dx.doi.org/10.3386/w31925,,10.3386/w31925,,,0,,6,true,,bronze
018-135-083-299-986,"Combining Financial Data and News Articles for Stock Price Movement
  Prediction Using Large Language Models",2024-11-02,2024,preprint,arXiv (Cornell University),,,,Ali Elahi; Fatemeh Taghvaei,"Predicting financial markets and stock price movements requires analyzing a company's performance, historic price movements, industry-specific events alongside the influence of human factors such as social media and press coverage. We assume that financial reports (such as income statements, balance sheets, and cash flow statements), historical price data, and recent news articles can collectively represent aforementioned factors. We combine financial data in tabular format with textual news articles and employ pre-trained Large Language Models (LLMs) to predict market movements. Recent research in LLMs has demonstrated that they are able to perform both tabular and text classification tasks, making them our primary model to classify the multi-modal data. We utilize retrieval augmentation techniques to retrieve and attach relevant chunks of news articles to financial metrics related to a company and prompt the LLMs in zero, two, and four-shot settings. Our dataset contains news articles collected from different sources, historic stock price, and financial report data for 20 companies with the highest trading volume across different industries in the stock market. We utilized recently released language models for our LLM-based classifier, including GPT- 3 and 4, and LLaMA- 2 and 3 models. We introduce an LLM-based classifier capable of performing classification tasks using combination of tabular (structured) and textual (unstructured) data. By using this model, we predicted the movement of a given stock's price in our dataset with a weighted F1-score of 58.5% and 59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and 6-month periods.",,,,,Stock (firearms); Stock price; Movement (music); Computer science; Financial economics; Finance; Economics; Series (stratigraphy); Engineering; Geology; Mechanical engineering; Paleontology; Philosophy; Aesthetics,,,,,https://arxiv.org/abs/2411.01368,http://dx.doi.org/10.48550/arxiv.2411.01368,,10.48550/arxiv.2411.01368,,,0,,0,true,,green
018-594-498-306-781,"UCFE: A User-Centric Financial Expertise Benchmark for Large Language
  Models",2024-10-17,2024,preprint,arXiv (Cornell University),,,,Yuzhe Yang; Yifei Zhang; Yan Hu; Yilin Guo; Ruoli Gan; Yueru He; Mingcong Lei; Xiao Zhang; Haining Wang; Qianqian Xie; Jimin Huang; Honghai Yu; Benyou Wang,"This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction. The benchmark dataset and evaluation code are available.",,,,,Benchmark (surveying); Computer science; Finance; Business; Geography; Cartography,,,,,https://arxiv.org/abs/2410.14059,http://dx.doi.org/10.48550/arxiv.2410.14059,,10.48550/arxiv.2410.14059,,,0,,0,true,,green
018-651-010-957-918,Voice UX in Banking: Designing Conversational AI for Financial Assistants,2025-07-25,2025,journal article,International Journal of Leading Research Publication,25828010,Sky Research Publication and Journals,,Sajindas Devidas -,"<jats:p>Voice user experience (Voice UX) is changing how people interact with digital banking. Instead of only typing or tapping, customers can now talk to their bank using smart assistants. This research explores how conversational AI helps banks offer better, more personal service. It explains how voice assistants work, how they help people with their money, and what challenges banks face, like keeping information safe and making sure the AI understands everyone. Ultimately, the study shows that banks using voice AI will give customers a smoother, more helpful experience.</jats:p>",6,7,,,Business; Computer science; Finance,,,,,,http://dx.doi.org/10.70528/ijlrp.v6.i7.1687,,10.70528/ijlrp.v6.i7.1687,,,0,,0,false,,
018-866-913-622-957,FinGPT: Open-Source Financial Large Language Models,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Hongyang Yang; Xiao-Yang Liu; Christina Dan Wang,"Large language models (LLMs) have shown the potential of revolutionizing natural language processing in diverse domains, sparking great interest in finance. However, the finance domain presents unique challenges, including high temporal sensitivity, constant dynamism, and a low signal-to-noise ratio (SNR). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize internet-scale financial data.In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to customize their financial LLMs (FinLLMs). We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we will showcase potential applications as stepping stones for users, such as robo-advising and sentiment analysis. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \url{https://github.com/AI4Finance-Foundation/FinGPT} and \url{https://github.com/AI4Finance-Foundation/FinNLP}",,,,,Financial modeling; Computer science; Business; Finance,,,,,https://arxiv.org/pdf/2306.06031 https://arxiv.org/abs/2306.06031,http://dx.doi.org/10.2139/ssrn.4489826,,10.2139/ssrn.4489826,,,7,,159,true,,green
019-113-946-690-50X,MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading,2025-09-05,2025,preprint,arXiv (Cornell University),,,,Yang Chen; Yueheng Jiang; Zhaozhao Ma; Yuchen Cao; Jacky Keung; Kun Kuang; Leilei Gan; Yiquan Wu; Fei Wu,"The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the router's market classification capability and experts' risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency.",,,,,Trading strategy; Business; Computer science; Finance,,,,,https://arxiv.org/abs/2509.05080,http://dx.doi.org/10.48550/arxiv.2509.05080,,10.48550/arxiv.2509.05080,,,0,,0,true,,green
019-508-132-538-02X,"FinRobot: An Open-Source AI Agent Platform for Financial Applications
  using Large Language Models",2024-05-23,2024,preprint,arXiv (Cornell University),,,,Hongyang Yang; Boyu Zhang; Neng Wang; Cheng Guo; Xiaoli Zhang; Likun Lin; Junlin Wang; Tianyu Zhou; Mao Guan; Runjia Zhang; Christina Dan Wang,"As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making. In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \url{https://github.com/AI4Finance-Foundation/FinRobot}.",,,,,Open source; Computer science; Programming language; Software engineering; Software,,,,,https://arxiv.org/abs/2405.14767,http://dx.doi.org/10.48550/arxiv.2405.14767,,10.48550/arxiv.2405.14767,,,0,,1,true,,green
019-523-708-971-72X,Demystifying Domain-adaptive Post-training for Financial LLMs,2025-01-08,2025,preprint,arXiv (Cornell University),,,,Zixuan Ke; Yifei Ming; Xuan-Phi Nguyen; Caiming Xiong; Shafiq Joty,"Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach begins by identifying the core capabilities required for the target domain and designing a comprehensive evaluation suite aligned with these needs. We then analyze the effectiveness of key post-training stages, including continual pretraining, instruction tuning, and preference alignment. Building on these insights, we propose an effective training recipe centered on a novel preference data distillation method, which leverages process signals from a generative reward model. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs. Project page: https://github.com/SalesforceAIResearch/FinDap",,,,,Training (meteorology); Domain (mathematical analysis); Business; Finance; Geography; Meteorology; Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2501.04961,http://dx.doi.org/10.48550/arxiv.2501.04961,,10.48550/arxiv.2501.04961,,,0,,0,true,,green
019-665-069-986-880,GPT-3 Models are Few-Shot Financial Reasoners,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Raul Salles de Padua; Imran Qureshi; Mustafa U. Karakaplan,"Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents. With this understanding, our refined prompt-engineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.",,,,,Variety (cybernetics); Task (project management); Computer science; Question answering; Open domain; Finance; Language model; Financial modeling; Artificial intelligence; Machine learning; Business; Economics; Management,,,,,https://arxiv.org/abs/2307.13617,http://dx.doi.org/10.48550/arxiv.2307.13617,,10.48550/arxiv.2307.13617,,,0,,0,true,other-oa,green
019-684-748-849-873,Enhancing Financial Risk Management with Retrieval-Augmented Large Language Models,2025-04-25,2025,conference proceedings article,"2025 4th International Conference on Artificial Intelligence, Internet and Digital Economy (ICAID)",,IEEE,,Jingzhi Xu,,,,138,141,Computer science; Risk management; Financial management; Language model; Finance; Business; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/icaid65275.2025.11034536,,10.1109/icaid65275.2025.11034536,,,0,020-026-057-381-539; 140-921-627-035-630; 162-050-261-912-64X,0,false,,
019-804-131-065-845,ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs,2025-10-06,2025,preprint,arXiv (Cornell University),,,,Wonjun Kang; Kevin Galim; Seunghyuk Oh; Minjae Lee; Yuchen Zeng; Shuibai Zhang; Coleman Hooper; Yuezhou Hu; Hyung Il Koo; Nam Ik Cho; Kangwook Lee,"While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.",,,,,Decoding methods; Diffusion; Economics; Computer science; Business; Telecommunications; Thermodynamics; Physics,,,,,https://arxiv.org/abs/2510.04767,http://dx.doi.org/10.48550/arxiv.2510.04767,,10.48550/arxiv.2510.04767,,,0,,0,true,,green
020-008-033-155-220,"KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government
  Financial Data and Regulations to Enhance Decision Making",2024-07-31,2024,preprint,arXiv (Cornell University),,,,Gilang Fajar Febrian; Grazziela Figueredo,"Data is crucial for evidence-based policymaking and enhancing public services, including those at the Ministry of Finance of the Republic of Indonesia. However, the complexity and dynamic nature of governmental financial data and regulations can hinder decision-making. This study investigates the potential of Large Language Models (LLMs) to address these challenges, focusing on Indonesia's financial data and regulations. While LLMs are effective in the financial sector, their use in the public sector in Indonesia is unexplored. This study undertakes an iterative process to develop KemenkeuGPT using the LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of Finance, Statistics Indonesia and the International Monetary Fund (IMF). Surveys and interviews with Ministry officials informed, enhanced and fine-tuned the model. We evaluated the model using human feedback, LLM-based evaluation and benchmarking. The model's accuracy improved from 35% to 61%, with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness with 73% faithfulness, 40% precision and 60% recall, outperforming several other base models. An interview with an expert from the Ministry of Finance indicated that KemenkeuGPT has the potential to become an essential tool for decision-making. These results are expected to improve with continuous human feedback.",,,,,Government (linguistics); Business; Finance; Linguistics; Philosophy,,,,,https://arxiv.org/abs/2407.21459,http://dx.doi.org/10.48550/arxiv.2407.21459,,10.48550/arxiv.2407.21459,,,0,,0,true,,green
020-013-939-578-47X,ECIS - The Effect of Anthropomorphism on Investment Decision-Making with Robo-Advisor Chatbots,2020-05-13,2020,conference proceedings,,,,,Stefan Morana; Ulrich Gnewuch; Dominik Jung; Carsten Granig,"Robo-advisors promise to offer professional financial advice to private households at low cost. However, the automation of financial advisory processes to fully-digitalized services often goes hand in hand with a loss of human contact and lack of trust. Especially in an investment context, customers demand a human involved to ensure the trustworthiness of digitalized financial services. In this paper, we report the findings of a lab experiment (N=183) investigating how different levels of anthropomorphic design of a robo-advisor chatbot might compensate for the lack of human involvement and positively affect users’ trusting beliefs and likeliness to follow its recommendations. We found that the anthropomorphic design influences users’ perceived social presence of the chatbot. While trusting beliefs in the chatbot enhance users’ likeliness to follow the recommendation by the chatbot, we do not find evidence for a direct effect of social presence on likeliness to follow its recommendation. However, social presence has a positive indirect effect on likeliness to follow the recommendation via trusting beliefs. Our research contributes to the body of knowledge on the design of robo-advisors and provides more insight into the factors that influence users’ investment decisions when interacting with a robo-advisor chatbot.",,,,,Internet privacy; Financial services; Trustworthiness; Psychology; Body of knowledge; Investment (macroeconomics); Investment decisions; Chatbot; Context (language use); Affect (psychology),,,,,https://aisel.aisnet.org/ecis2020_rp/63/ https://dblp.uni-trier.de/db/conf/ecis/ecis2020.html#MoranaGJG20 https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1062&context=ecis2020_rp https://publikationen.bibliothek.kit.edu/1000120359,https://aisel.aisnet.org/ecis2020_rp/63/,,,3025294766,,0,,4,false,,
020-574-263-757-653,Text Summarization and Sentiment Analysis Pipelines Using Large Language Models for Financial News,2025-04-13,2025,book chapter,"Smart Innovation, Systems and Technologies",21903018; 21903026,Springer Nature Singapore,Germany,Alin-Gabriel Văduva; Anca-Ioana Andreescu,,,,421,430,Automatic summarization; Sentiment analysis; Computer science; Natural language processing,,,,,,http://dx.doi.org/10.1007/978-981-96-0161-5_37,,10.1007/978-981-96-0161-5_37,,,0,002-639-766-645-882; 009-236-900-880-997; 013-169-430-329-320; 028-022-320-118-504; 032-166-042-486-119; 044-479-195-732-694; 048-784-659-702-976; 051-192-647-673-900; 053-045-748-271-460; 055-702-646-212-240; 061-939-849-084-376; 086-386-133-092-987; 088-416-941-649-795; 095-254-612-256-75X; 115-096-417-941-48X; 116-316-138-754-604,1,false,,
021-083-369-698-591,Leveraging interactive visualizations and LLM-driven explanations for transparent multi-objective portfolio management,2025-11-26,2025,journal article,Discover Data,27316955,Springer Science and Business Media LLC,,Yong Zheng; Jian Zhang; Kumar Neelotpal Shukla; Michael O'Leary; David Xuejun Wang; Jasmine Xu,,3,1,,,,,,,,,http://dx.doi.org/10.1007/s44248-025-00065-z,,10.1007/s44248-025-00065-z,,,0,000-541-797-292-217; 001-394-275-726-646; 004-879-322-945-177; 005-616-220-657-600; 006-849-347-636-464; 012-275-508-827-892; 014-504-966-099-190; 016-317-756-359-811; 021-478-957-578-39X; 025-823-046-712-228; 026-427-427-369-277; 027-970-982-371-991; 030-524-806-617-444; 034-420-620-131-04X; 035-833-358-195-065; 036-558-260-031-778; 037-171-718-232-716; 037-815-144-847-56X; 038-009-837-399-544; 039-498-601-598-754; 046-034-021-970-167; 046-673-150-521-342; 049-752-581-844-906; 050-523-809-280-040; 056-418-555-342-072; 061-112-587-284-443; 063-242-867-204-119; 063-685-861-183-145; 066-014-638-795-52X; 067-492-886-069-933; 068-685-598-293-137; 075-265-123-459-853; 075-441-926-434-06X; 080-540-940-758-217; 084-824-643-191-218; 087-520-546-276-136; 088-188-436-321-084; 089-521-138-786-249; 090-746-329-079-283; 096-637-753-659-266; 097-034-105-191-961; 099-768-191-610-274; 101-875-478-290-152; 104-881-321-126-952; 107-646-719-213-529; 108-783-952-301-936; 119-770-239-869-389; 120-333-330-672-288; 128-964-978-739-272; 131-246-804-018-055; 131-959-167-758-543; 155-554-164-069-983; 162-262-865-510-566; 163-647-933-979-491; 171-428-374-379-739; 177-419-062-832-083; 180-336-104-584-095; 186-836-985-946-04X; 198-421-781-400-601,1,true,"CC BY, CC BY-NC-ND",gold
021-131-186-495-672,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): ADR",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905713,https://guides.library.harvard.edu/c.php?g=1082940&p=7905713,,,3089094297,,0,,0,false,,
021-389-683-455-661,Optimizing LLM Based Retrieval Augmented Generation Pipelines in the Financial Domain,,2024,conference proceedings article,Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track),,Association for Computational Linguistics,,Yiyun Zhao; Prateek Singh; Hanoz Bhathena; Bernardo Ramos; Aviral Joshi; Swaroop Gadiyaram; Saket Sharma,,,,279,294,Computer science; Pipeline transport; Domain (mathematical analysis); Augmented reality; Artificial intelligence; Engineering; Mathematical analysis; Mathematics; Environmental engineering,,,,,,http://dx.doi.org/10.18653/v1/2024.naacl-industry.23,,10.18653/v1/2024.naacl-industry.23,,,0,,10,false,,
021-466-402-188-517,Financial Statement Fraud Detection via Large Language Models,2025-11-30,2025,journal article,"Intelligent Systems in Accounting, Finance and Management",15501949; 21600074,Wiley,United Kingdom,Zehra Erva Ergun; Emre Sefer,"<jats:title>ABSTRACT</jats:title>;                   <jats:p>;                     With the widespread adoption of Internet‐based AI technologies, addressing financial fraud has become increasingly critical, particularly within the realm of machine learning. In this case, deep learning and natural language processing (NLP) techniques offer powerful means of detecting fraudulent activity by analyzing financial documents, thereby enhancing both the efficiency and precision of such assessments and supporting financial security. In this study, we introduce deep representation learning‐based approaches relying mainly on large language models (LLMs) for identifying fraud in financial statements by examining temporal changes in the Management Discussion and Analysis (MD&amp;A) sections of corporate disclosures. Departing from conventional techniques that rely only on word frequency analysis, we propose D;                     <jats:sc>eep</jats:sc>;                     F;                     <jats:sc>raud</jats:sc>;                     that combines time‐evolving financial LLM embeddings, such as FinBERT, FinLlama, and FinGPT embeddings, of paragraphs and uses long short‐term memory (LSTM) to predict frauds via historical textual embeddings. In addition to LLM embeddings, we also integrate (1) time‐evolving word frequencies of words relevant to fraud detection, such as those expressing sentiment or uncertainty, and (2) time‐evolving financial ratios. Trajectories of paragraph‐level embeddings, frequencies, and ratios are used to construct a fraud detection model, which we evaluate against machine learning methods and deep time‐series models. Using 30 years of financial report data (from 1995 to 2024), our experiments demonstrate that D;                     <jats:sc>eep</jats:sc>;                     F;                     <jats:sc>raud</jats:sc>;                     on average enhances fraud detection performance across a number of scenarios and on average outperforms the competing approaches as well as conventional word frequency approaches. Our framework introduces a novel direction for deep feature engineering in the field of financial statement fraud detection.;                   </jats:p>",32,4,,,,,,,Türkiye Bilimsel ve Teknolojik Araştırma Kurumu,,http://dx.doi.org/10.1002/isaf.70021,,10.1002/isaf.70021,,,0,000-391-890-078-521; 000-524-338-757-738; 001-234-361-656-848; 004-364-724-327-82X; 005-370-300-067-996; 006-399-176-500-152; 008-186-637-691-064; 008-634-212-021-555; 009-275-434-848-173; 010-333-328-643-295; 010-556-443-588-699; 011-934-843-882-557; 012-449-248-979-466; 014-588-951-355-586; 015-854-849-276-190; 017-927-477-489-829; 025-053-687-596-118; 030-942-527-844-929; 040-985-643-459-086; 044-806-689-553-253; 047-472-686-353-143; 052-363-261-099-758; 053-094-537-530-97X; 057-528-374-003-091; 070-567-291-798-784; 070-972-916-093-637; 072-263-062-519-219; 072-344-788-098-622; 072-583-134-426-365; 072-658-198-050-962; 074-573-104-058-630; 078-141-527-597-27X; 080-828-384-981-970; 080-835-791-573-515; 082-178-119-516-788; 084-391-236-855-469; 089-805-264-780-029; 090-966-599-369-883; 098-358-225-770-631; 099-462-597-129-024; 100-026-855-446-798; 106-081-386-593-970; 108-867-611-312-707; 113-902-162-773-664; 115-116-192-516-372; 119-473-921-261-412; 123-498-859-171-509; 127-064-507-517-972; 129-957-509-540-413; 130-361-596-468-816; 139-562-543-297-775; 142-853-929-819-343; 147-413-187-777-81X; 148-599-778-724-925; 153-754-452-743-331; 154-293-426-700-837; 171-100-784-964-309; 171-667-650-005-943; 172-297-858-789-450; 177-085-128-759-193; 181-228-907-725-101; 186-096-516-811-862; 186-422-264-101-469; 193-770-620-946-367,0,false,,
021-626-901-974-598,A survey on the application and research progress of large language models in financial forecasting,2025-06-01,2025,journal article,AIP Advances,21583226,AIP Publishing,United States,Ruonan Wu; Hong Liu,"<jats:p>Large language models (LLMs) are reshaping the technical paradigms of financial forecasting through their robust representation learning and reasoning capabilities. This paper systematically reviews the application pathways of architectures such as transformers and graph neural networks in scenarios like stock prediction and risk management, highlighting key technologies for enhancing prediction accuracy through knowledge injection and temporal modeling improvements. The study reveals that LLMs demonstrate significant advantages in unstructured data processing and cross-market correlation analysis but face challenges related to economic logic interpretability and data non-stationarity. Future research should focus on advancing causal reasoning augmentation and federated learning collaboration to achieve secure and trustworthy evolution of financial forecasting systems.</jats:p>",15,6,,,Computer science; Data science,,,,,,http://dx.doi.org/10.1063/5.0274031,,10.1063/5.0274031,,,0,004-771-690-834-863; 014-579-058-588-876; 025-214-242-536-504; 028-082-464-771-603; 038-098-794-911-655; 046-204-603-588-768; 054-883-635-921-366; 060-975-152-229-020; 068-042-879-431-736; 074-792-909-152-421; 075-255-495-210-929; 085-158-489-855-964; 105-956-835-654-883; 123-906-633-448-288; 124-690-193-709-451; 131-030-459-473-790; 139-926-489-528-484; 150-397-545-090-569; 155-384-092-877-551; 171-525-285-825-244; 186-557-343-909-915; 187-998-021-933-832; 190-050-627-029-962,0,true,cc-by,gold
021-727-584-495-392,"Large Language Models as Financial Data Annotators: A Study on
  Effectiveness and Efficiency",2024-03-26,2024,preprint,arXiv (Cornell University),,,,Toyin Aguda; Suchetha Siddagangappa; Elena Kochkina; Simerjot Kaur; Dongsheng Wang; Charese Smiley; Sameena Shah,"Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify outputs that may require expert attention. Finally, we perform an extensive time, cost and error analysis and provide recommendations for the collection and usage of automated annotations in domain-specific settings.",,,,,Computer science; Finance; Business,,,,,https://arxiv.org/abs/2403.18152,http://dx.doi.org/10.48550/arxiv.2403.18152,,10.48550/arxiv.2403.18152,,,0,,0,true,,green
022-142-411-119-922,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Perma.cc - Preserving and Citing Materials on the Internet",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,The Internet; Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7906097,https://guides.library.harvard.edu/c.php?g=1082940&p=7906097,,,3088698480,,0,,0,false,,
022-387-792-867-785,GENERATIVE ARTIFICIAL INTELLIGENCE AND THE FUTURE OF FINANCIAL FORECASTING: EVIDENCE FROM LARGE LANGUAGE MODELS,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Mfon Akpan,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5884602,,10.2139/ssrn.5884602,,,0,,0,false,,
022-485-847-628-87X,Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Ethan Callanan; Amarachi Mbakwe; Antony Papadimitriou; Yulong Pei; Mathieu Sibue; Xiaodan Zhu; Zhiqiang Ma; Xiaomo Liu; Sameena Shah,"Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.",,,,,Leverage (statistics); Task (project management); Finance; Perspective (graphical); Matching (statistics); Computer science; Artificial intelligence; Business; Management; Economics; Medicine; Pathology,,,,,https://arxiv.org/abs/2310.08678,http://dx.doi.org/10.48550/arxiv.2310.08678,,10.48550/arxiv.2310.08678,,,0,,4,true,other-oa,green
022-523-716-845-06X,AuditBench: A Benchmark for Large Language Models in Financial Statement Auditing,2025-07-01,2025,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Singapore,Germany,Rushi Wang; Jiateng Liu; Weijie Zhao; Shenglan Li; Denghui Zhang,,,,59,81,Statement (logic); Audit; Benchmark (surveying); Financial statement; Computer science; Accounting; Business; Linguistics; Philosophy; Geography; Cartography,,,,,,http://dx.doi.org/10.1007/978-981-96-8912-5_3,,10.1007/978-981-96-8912-5_3,,,0,009-146-126-681-999; 009-429-532-265-827; 011-856-960-141-570; 019-736-560-026-447; 033-061-514-715-801; 041-306-244-948-965; 054-768-354-510-890; 069-470-928-878-196; 089-752-231-256-54X; 105-196-484-637-045; 198-374-519-662-543,0,false,,
022-790-624-647-716,Real-Time Regulatory Intelligence Framework: LLM-powered compliance automation for financial services,2025-05-30,2025,journal article,World Journal of Advanced Engineering Technology and Sciences,25828266,GSC Online Press,,null Naga Krishna Mahesh Pulikonda,"<jats:p>This article presents a novel real-time regulatory intelligence framework leveraging Large Language Models (LLMs) to transform compliance automation in financial services. The article addresses the growing complexity of regulatory requirements that financial institutions face globally by developing an integrated system that continuously monitors regulatory sources, interprets complex requirements, and implements compliant controls. The framework consists of a layered architecture with specialized components for data ingestion, regulatory interpretation, policy orchestration, and governance with human oversight. Extensive evaluations demonstrate the framework's ability to significantly improve interpretation accuracy, reduce time-to-compliance, and enhance risk management compared to traditional approaches. Case studies across multiple regulatory domains confirm the system's versatility and effectiveness, while implementation data from financial institutions validates substantial cost reductions and operational improvements. The article identifies promising directions for further enhancement and examines broader implications for the regulatory technology landscape, suggesting a paradigm shift in how compliance is managed and how regulators and regulated entities interact in an increasingly digital financial ecosystem.</jats:p>",15,2,3106,3115,Compliance (psychology); Automation; Financial services; Business; Finance; Engineering; Psychology; Mechanical engineering; Social psychology,,,,,,http://dx.doi.org/10.30574/wjaets.2025.15.2.0784,,10.30574/wjaets.2025.15.2.0784,,,0,,0,false,,
022-858-996-555-941,Development of a Personalized Gamified Mobile Learning App Using an AI-Based Virtual Assistant for Financial Literacy,2025-07-24,2025,preprint,,,Springer Science and Business Media LLC,,Angie Nayeli Ruiz-Carhuamaca; Juliana Alexandra Yauricasa-Seguil; Juan Carlos Morales-Arevalo,"<title>Abstract</title>;         <p>This study presents the implementation of a mobile application that integrates gamification and artificial intelligence to strengthen financial literacy in college students. The proposed solution combines a user-centered approach, educational and structured content, and interactive challenges, integrating a virtual assistant powered by Gemini AI. The application is composed of three stages: an initial diagnostic assessment, gamified challenges, and a final assessment. The system offers personalized feedback, financial topics with levels of difficulty, and a system of achievements and rewards, which seeks to enhance engagement and learning outcomes. The development stage was carried out under the Scrum framework over eight sprints, where Unity and FastAPI were used for frontend and backend development, respectively. On the other hand, usability and user experience were evaluated using two instruments: the System Usability Scale (SUS) and the User Experience Questionnaire (UEQ). The results obtained from the tests with 50 university students show a high usability (SUS = 91.9) and a positive experience in all the dimensions evaluated by the UEQ, which confirms the effectiveness of the application. This work contributes to the field of educational technologies by providing a scalable and validated architecture for financial education through mobile learning.</p>",,,,,Usability; Computer science; Scalability; Architecture; Personalized learning; Financial literacy; Virtual learning environment; Literacy; Scrum; Multimedia; Field (mathematics); System usability scale; Human–computer interaction; Usability engineering; Mathematics education; Software; Psychology; Teaching method; Software development; Pedagogy; Finance; Art; Mathematics; Database; Open learning; Visual arts; Programming language; Pure mathematics; Economics; Cooperative learning,,,,,https://www.researchsquare.com/article/rs-7170120/latest.pdf https://doi.org/10.21203/rs.3.rs-7170120/v1,http://dx.doi.org/10.21203/rs.3.rs-7170120/v1,,10.21203/rs.3.rs-7170120/v1,,,0,002-564-943-321-958; 003-742-020-510-270; 006-962-164-341-078; 009-367-665-400-42X; 016-606-401-900-323; 029-735-278-946-205; 032-237-881-394-835; 033-415-193-191-482; 043-844-396-611-502; 046-993-353-330-784; 056-804-817-980-863; 070-813-679-699-070; 072-750-965-244-728; 083-204-552-028-526; 084-881-284-514-214; 093-315-894-926-777; 096-092-955-246-008; 096-661-136-197-65X; 098-573-309-532-835; 099-077-547-289-619; 099-798-840-377-95X; 116-233-967-858-853; 118-185-239-687-055; 121-931-244-530-300; 125-952-309-625-035; 131-376-793-039-349; 134-204-989-094-815; 135-457-000-567-054; 144-069-365-102-585; 147-839-649-870-487; 152-172-472-465-205; 166-901-911-340-339; 169-901-764-989-961; 186-580-391-206-785; 191-963-218-277-36X; 199-309-898-673-909; 199-327-163-535-344,0,false,,
023-005-589-172-662,Temal: A Time Encoding Module Augmented LLM for Financial Forecasting,,2024,conference proceedings article,"Poster Volume Ⅰ  The 2024 Twentieth International  Conference on Intelligent Computing  August 5-8, 2024  Tianjin, China",,The Society of International Computing,,Mingjun Ma,"<jats:p>In the domain of time series analysis, financial forecasting presents itself as a pinnacle of intricacy. Despite the multitude of models, even those powered by cutting-edge transformer architectures, their practical efficacy on financial datasets has remained unexplored. This challenge stems from the unique nature of financial derivatives: various time scales, multifaceted attributes, and volatile patterns. Therefore, this study introduces an innovative multi-modal fine-tuning framework, which harnesses the semantic comprehension capabilities of Large Language Models (LLMs) and encodes both time-series data and its domain-specific knowledge. To mitigate the shortcomings of LLMs in capturing temporal dynamics, we propose two pivotal innovations: a Time-series Encoding Module (TEM) and a Multi-Patch Method. The TEM seamlessly embeds sophisticated temporal representation algorithms within the LLM architecture. Concurrently, the Multi-Patch Method transforms 1D time series into multiple sets of 2D tensors, each representing distinct temporal segments, thereby enriching the model's temporal analysis capabilities. Our empirical evaluations reveal that the Multi-Patch Method adeptly handles the complex temporal fluctuations across varied intervals. The proposed model outperforms other competing methods, marking a 20.2% enhancement in forecasting accuracy for Turnover Ratio and an 9.1% improvement in zero-shot forecasting performance. Crucially, The TEM and Multi-Patch offer modular improvements for LLM-based time-series forecasting, with potential applications across various domains.</jats:p>",,,455,468,,,,,,,http://dx.doi.org/10.65286/icic.v20i2.72905,,10.65286/icic.v20i2.72905,,,0,,0,false,,
023-231-794-454-664,Extracting Financial Data From Unstructured Sources: Leveraging Large Language Models,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Huaxia Li; Haoyun Gao; Chengzhang Wu; Miklos A. Vasarhelyi,,,,,,Unstructured data; Computer science; Finance; Data science; Business; Data mining; Big data,,,,,,http://dx.doi.org/10.2139/ssrn.4567607,,10.2139/ssrn.4567607,,,0,000-064-613-799-138; 009-261-751-244-528; 010-525-115-959-84X; 013-425-734-974-695; 016-893-782-437-444; 020-409-968-116-435; 025-972-661-029-980; 026-166-698-578-385; 028-151-692-716-91X; 038-287-375-579-320; 039-363-136-262-869; 044-111-846-729-62X; 048-708-058-231-027; 051-161-858-118-526; 053-749-787-528-586; 078-982-190-689-836; 087-196-244-670-101; 087-776-095-156-221; 087-986-828-061-613; 096-484-313-396-218; 107-002-716-250-830; 110-510-842-185-282; 110-739-741-507-751; 122-618-040-279-711; 133-793-781-089-971; 141-843-032-210-01X; 148-012-056-970-651; 154-042-138-562-576; 187-985-761-754-968,10,false,,
023-451-429-221-648,AVA: A Financial Service Chatbot Based on Deep Bidirectional Transformers,2021-08-26,2021,journal article,Frontiers in Applied Mathematics and Statistics,22974687,Frontiers Media SA,,Shi Yu; Yuxin Chen; Hussain Zaidi,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to handle client questions in financial investment customer service. The bot can recognize 381 intents, and decides when to say \textit{I don't know} and escalates irrelevant/uncertain questions to human operators. Our main novel contribution is the discussion about uncertainty measure for BERT, where three different approaches are systematically compared on real problems. We investigated two uncertainty metrics, information entropy and variance of dropout sampling in BERT, followed by mixed-integer programming to optimize decision thresholds. Another novel contribution is the usage of BERT as a language model in automatic spelling correction. Inputs with accidental spelling errors can significantly decrease intent classification performance. The proposed approach combines probabilities from masked language model and word edit distances to find the best corrections for misspelled words. The chatbot and the entire conversational AI system are developed using open-source tools, and deployed within our company's intranet. The proposed approach can be useful for industries seeking similar in-house solutions in their specific business domains. We share all our code and a sample chatbot built on a public dataset on Github.",7,,,,Machine learning; Language model; Artificial intelligence; Variance (accounting); Spelling; Intranet; Chatbot; Transformer (machine learning model); Computer science; Sample (statistics); Word (computer architecture),chabot; BERT; rasa; bayesian learning; intent classification,,,,http://dblp.uni-trier.de/db/journals/corr/corr2003.html#abs-2003-04987 https://arxiv.org/pdf/2003.04987 https://www.frontiersin.org/articles/10.3389/fams.2021.604842/full https://arxiv.org/abs/2003.04987 https://dblp.uni-trier.de/db/journals/corr/corr2003.html#abs-2003-04987,http://dx.doi.org/10.3389/fams.2021.604842,,10.3389/fams.2021.604842,3197210064,,0,007-814-872-111-101; 008-708-960-845-751; 015-211-825-353-092; 015-683-510-811-015; 028-733-625-338-066; 028-784-093-249-926; 032-221-948-483-976; 033-034-368-260-285; 034-304-301-930-110; 038-134-268-437-952; 038-443-748-925-073; 040-639-723-273-917; 044-498-949-210-002; 046-083-595-644-433; 046-791-994-958-226; 049-324-875-236-331; 051-073-344-349-685; 056-935-251-080-195; 059-431-949-623-26X; 062-449-192-706-779; 065-134-013-925-216; 071-685-214-817-308; 074-033-855-257-801; 082-178-119-516-788; 087-363-447-270-007; 087-864-289-366-200; 089-199-749-285-053; 089-802-595-581-370; 091-430-772-314-589; 103-506-297-548-029; 119-525-054-688-38X; 123-979-645-524-601; 130-460-460-908-695; 135-467-519-462-979; 138-275-133-139-362; 144-171-143-777-107; 149-094-088-552-09X; 154-910-565-038-477; 156-643-811-054-720; 158-902-227-944-755; 161-996-011-736-816; 162-880-630-852-463; 166-753-131-423-481; 171-885-864-003-332; 174-677-065-164-255; 175-819-593-427-749; 176-054-463-554-509; 176-334-018-120-549; 181-142-549-396-290; 187-234-190-686-086,31,true,cc-by,gold
023-717-083-969-194,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,,2025,conference proceedings article,Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,Association for Computational Linguistics,,Xin Guo; Haotian Xia; Zhaowei Liu; Hanyang Cao; Zhi Yang; Zhiqiang Liu; Sizhe Wang; Jinyi Niu; Chuqi Wang; Yanhui Wang; Xiaolong Liang; Xiaoming Huang; Bing Zhu; Zhongyu Wei; Yun Chen; Weining Shen; Liwen Zhang,,,,6258,6292,Computer science; Benchmark (surveying); Domain (mathematical analysis); Language model; Artificial intelligence; Natural language processing; Mathematics; Mathematical analysis; Geodesy; Geography,,,,,,http://dx.doi.org/10.18653/v1/2025.naacl-long.318,,10.18653/v1/2025.naacl-long.318,,,0,,4,false,,
023-852-103-528-639,"Artificial Intelligence Across Industries: Speech Recognition, Chatbots, and Financial Technologies in Focus",,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Linda F Harris,,,,,,Focus (optics); Speech recognition; Computer science; Business; Optics; Physics,,,,,,http://dx.doi.org/10.2139/ssrn.5401447,,10.2139/ssrn.5401447,,,0,,0,false,,
024-253-087-955-56X,LLM-enhanced multi-causal event causality mining in financial texts,2025-11-11,2025,journal article,Journal of King Saud University Computer and Information Sciences,13191578; 22131248,Springer Science and Business Media LLC,Saudi Arabia,Chunyu Yan,,37,9,,,,,,,,,http://dx.doi.org/10.1007/s44443-025-00330-w,,10.1007/s44443-025-00330-w,,,0,009-837-388-105-601; 010-438-295-988-721; 010-555-545-400-752; 017-155-967-455-122; 018-053-739-230-836; 022-052-317-501-94X; 025-745-018-274-733; 038-081-513-467-244; 039-476-275-737-722; 041-443-484-796-252; 081-001-055-113-348; 082-178-119-516-788; 083-259-820-392-446; 084-298-644-668-797; 087-830-718-047-764; 089-151-546-439-769; 090-168-650-512-90X; 097-853-633-170-805; 111-866-367-528-488; 141-115-102-429-872; 144-659-447-389-735; 145-429-207-822-141; 181-305-154-713-910,0,true,"CC BY, CC BY-NC-ND",gold
025-050-023-472-231,Comparative analysis of large language models adaptability to detect sentiments in financial domain,2024-08-30,2024,preprint,,,Wiley,,Steve Samson; Jitendra Rout K,"Large language models(LLMs) have become increasingly useful for predicting sentiments in financial text, such as social media posts, tweets, and news articles. However, most pre-trained LLMs are proprietary or too large for classification purposes. This study aims to help finance AI developers identify the suitability of different LLMs for financial sentiment analysis tasks. The study used three datasets (balanced and imbalanced) and three types of LLMs (textclassification, text-2-text generation, and text generation). Fine-tuning of chosen LLMs was performed within resource constraints, and techniques like instruction tuning were explored to improve performance. Small LLMs, designed for classification tasks, recorded the best accuracy and F1-score, but overfitting was evident. Large LLMs, particularly those of text-2-text generation type, showed more promising performance on large datasets. It is evident from the study that small LLMs train well on small datasets, but performance decreases with increasing diversity and size of dataset, especially on imbalanced datasets.",,,,,Overfitting; Computer science; Adaptability; Artificial intelligence; Artificial neural network; Management; Economics,,,,,,http://dx.doi.org/10.22541/au.172498784.43599501/v1,,10.22541/au.172498784.43599501/v1,,,0,,1,true,,green
025-227-861-501-418,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Tracing scholars and scholarship",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Tracing; Political science; Law; Scholarship; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905737,https://guides.library.harvard.edu/c.php?g=1082940&p=7905737,,,3092110289,,0,,0,false,,
025-366-455-527-671,FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Neng Wang; Hongyang Yang; Christina Dan Wang,"In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",,,,,Computer science; Benchmarking; Interoperability; Benchmark (surveying); Relevance (law); Task (project management); Debugging; Language model; Artificial intelligence; Adaptability; Machine learning; Software engineering; Data science; Programming language; World Wide Web; Engineering; Systems engineering; Geodesy; Marketing; Political science; Law; Business; Geography; Ecology; Biology,,,,,https://arxiv.org/abs/2310.04793,http://dx.doi.org/10.48550/arxiv.2310.04793,,10.48550/arxiv.2310.04793,,,0,,0,true,other-oa,green
026-294-977-463-130,GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Udit Gupta,"Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&P500 returns. This paper intends to provide a framework for future work in this direction. To facilitate this, the code has been released as open source.",,,,,Stock (firearms); Econometrics; Economics; Business; Financial economics; Geography; Archaeology,,,,,https://arxiv.org/pdf/2309.03079 https://arxiv.org/abs/2309.03079,http://dx.doi.org/10.2139/ssrn.4568964,,10.2139/ssrn.4568964,,,0,,18,true,,green
026-381-355-769-077,Pre-trained Large Language Models for Financial Sentiment Analysis,2024-01-01,2024,preprint,arXiv (Cornell University),,,,Wei Luo; Dihong Gong,"Financial sentiment analysis refers to classifying financial text contents into sentiment categories (e.g. positive, negative, and neutral). In this paper, we focus on the classification of financial news title, which is a challenging task due to a lack of large amount of training samples. To overcome this difficulty, we propose to adapt the pretrained large language models (LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge amount of text corpora,have an advantage in text understanding and can be effectively adapted to domain-specific task while requiring very few amount of training samples. In particular, we adapt the open-source Llama2-7B model (2023) with the supervised fine-tuning (SFT) technique [4]. Experimental evaluation shows that even with the 7B model (which is relatively small for LLMs), our approach significantly outperforms the previous state-of-the-art algorithms.",,,,,Task (project management); Computer science; Focus (optics); Sentiment analysis; Artificial intelligence; Domain (mathematical analysis); Language model; Finance; Natural language processing; Machine learning; Management; Business; Economics; Mathematics; Mathematical analysis; Physics; Optics,,,,,https://arxiv.org/abs/2401.05215,http://dx.doi.org/10.48550/arxiv.2401.05215,,10.48550/arxiv.2401.05215,,,0,,0,true,cc-by-nc-nd,green
026-984-186-973-562,LLMs Analyzing the Analysts: Do BERT and GPT Extract More Value from Financial Analyst Reports?,2023-11-25,2023,conference proceedings article,4th ACM International Conference on AI in Finance,,ACM,,Seonmi Kim; Seyoung Kim; Yejin Kim; Junpyo Park; Seongjin Kim; Moolkyeol Kim; Chang Hwan Sung; Joohwan Hong; Yongjae Lee,"This paper examines the use of Large Language Models (LLMs), specifically BERT-based models and GPT-3.5, in the sentiment analysis of Korean financial analyst reports. Due to the specialized language in these reports, traditional natural language processing techniques often prove insufficient, making LLMs a better alternative. These models are capable of understanding the complexity and subtlety of the language, allowing for a more nuanced interpretation of the data. We focus our study on the extraction of sentiment scores from these reports, using them to construct and test investment strategies. Given that Korean analyst reports present unique linguistic challenges and a significant 'buy' recommendation bias, we employ LLMs fine-tuned for the Korean language and Korean financial texts. The aim of this study is to investigate and compare the effectiveness of LLMs in enhancing the sentiment analysis of financial reports, and subsequently utilize the sentiment scores to construct and test investment strategies, thereby evaluating these models' potential in extracting valuable insights from the reports. The code is available at https://github.com/msraask3.",,,383,391,Construct (python library); Investment (military); Computer science; Test (biology); Sentiment analysis; Code (set theory); Natural language processing; Artificial intelligence; Finance; Political science; Business; Paleontology; Set (abstract data type); Politics; Law; Biology; Programming language,,,,Ulsan National Institute of Science and Technology,,http://dx.doi.org/10.1145/3604237.3627721,,10.1145/3604237.3627721,,,0,000-640-588-736-794; 002-126-877-983-127; 002-517-865-892-699; 006-399-176-500-152; 036-275-428-505-149; 042-269-147-288-746; 051-008-067-001-638; 051-652-729-402-973; 055-318-598-916-211; 073-854-781-052-96X; 082-250-084-377-004; 095-139-638-711-710; 101-520-573-807-639; 127-945-278-770-148; 135-837-503-980-320; 138-362-593-848-392; 148-916-890-644-411; 171-318-917-365-484; 178-427-077-523-183; 190-050-627-029-962; 191-424-188-274-666; 194-752-328-369-766,11,false,,
027-713-046-803-678,FinLBench: A Benchmark for Evaluating Large Language Models on Long-Text Financial Documents,2025-11-09,2025,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Singapore,Germany,Sihan Hu; Dingfu Yu; Yuhao Zhang; Zhongliang Yang; Linna Zhou; Xinguang Jiang; Yu Li; Binjie Fei; Jiaqi Liu,,,,244,258,,,,,,,http://dx.doi.org/10.1007/978-981-95-4091-4_17,,10.1007/978-981-95-4091-4_17,,,0,014-982-144-578-172; 050-704-382-213-879; 075-356-529-898-550; 089-323-388-950-331; 090-135-685-849-51X; 112-265-783-697-662; 127-795-289-353-963; 136-130-056-421-582; 137-014-686-937-063; 138-326-378-661-148; 179-355-149-733-611; 191-386-949-114-188,0,false,,
027-840-493-660-885,Firm-level trade policy effect uncertainty and cost stickiness: Evidence from a large language model approach ,2024-01-01,2024,preprint,,,Elsevier BV,,Shuyang Jia; Peng Liang; Nan Hu,,,,,,Economics; Industrial organization; Business,,,,,,http://dx.doi.org/10.2139/ssrn.4854505,,10.2139/ssrn.4854505,,,0,,0,false,,
028-082-464-771-603,Financial Analysis: Intelligent Financial Data Analysis System Based on LLM-RAG,2025-04-21,2025,journal article,Applied and Computational Engineering,27552721; 2755273x,EWA Publishing,,Jingru Wang; Wen Ding; Xiaotong Zhu,"<jats:p> In the modern financial sector, the exponential growth of data has made efficient and accurate financial data analysis increasingly crucial. Traditional methods, such as statistical analysis and rule-based systems, often struggle to process and derive meaningful insights from complex financial information effectively. These conventional approaches face inherent limitations in handling unstructured data, capturing intricate market patterns, and adapting to rapidly evolving financial contexts, resulting in reduced accuracy and delayed decision-making processes. To address these challenges, this paper presents an intelligent financial data analysis system that integrates Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology. Our system incorporates three key components: a specialized preprocessing module for financial data standardization, an efficient vector-based storage and retrieval system, and a RAG-enhanced query processing module. Using the NASDAQ financial fundamentals dataset from 2010 to 2023, we conducted comprehensive experiments to evaluate system performance. Results demonstrate significant improvements across multiple metrics: the fully optimized configuration (gpt-3.5-turbo-1106+RAG) achieved 78.6% accuracy and 89.2% recall, surpassing the baseline model by 23 percentage points in accuracy while reducing response time by 34.8%. The system also showed enhanced efficiency in handling complex financial queries, though with a moderate increase in memory utilization. Our findings validate the effectiveness of integrating RAG technology with LLMs for financial analysis tasks and provide valuable insights for future developments in intelligent financial data processing systems.</jats:p>",145,1,182,189,Computer science; Finance; Data pre-processing; Process (computing); Data mining; Financial analysis; Key (lock); Financial market; Computer security; Business; Operating system,,,,,,http://dx.doi.org/10.54254/2755-2721/2025.22221,,10.54254/2755-2721/2025.22221,,,0,,8,false,,
028-342-980-371-284,"Knowledge distillation for financial large language models: a systematic review of strategies, applications, and evaluation",2025-11-17,2025,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Jiaqi Shi; Xulong Zhang; Xiaoyang Qu; Junfei Xie; Jianzong Wang,,26,10,1793,1808,,,,,,,http://dx.doi.org/10.1631/fitee.2500282,,10.1631/fitee.2500282,,,0,006-399-176-500-152; 010-055-234-636-61X; 010-620-655-480-296; 016-887-102-988-613; 023-675-814-610-693; 025-375-743-897-030; 032-471-769-867-765; 035-557-488-599-254; 039-549-685-815-564; 043-621-536-812-827; 044-030-184-411-211; 046-817-935-032-274; 056-542-756-996-88X; 057-365-031-069-271; 064-989-848-170-832; 069-127-830-236-453; 070-049-608-205-466; 075-356-529-898-550; 078-141-527-597-27X; 079-598-607-597-954; 079-787-879-356-102; 080-878-370-554-949; 081-776-456-098-496; 090-477-211-138-281; 092-770-866-733-36X; 094-588-113-462-384; 096-704-258-326-787; 099-500-667-461-132; 099-687-718-657-569; 120-459-775-455-829; 124-407-553-402-722; 124-690-193-709-451; 131-337-012-300-857; 133-419-572-269-204; 137-120-754-193-596; 138-331-652-128-642; 139-239-027-110-484; 142-036-085-872-338; 145-487-224-433-848; 152-414-040-319-131; 152-537-263-659-299; 153-191-159-066-347; 154-042-138-562-576; 159-583-644-550-918; 168-271-333-231-69X; 173-355-611-706-640; 175-490-158-771-560; 193-136-826-957-223; 194-094-887-495-433; 197-453-159-841-757; 197-673-280-254-524,0,false,,
028-461-242-291-233,Empowering Financial Peace through Chatbot Guidance,2024-02-07,2024,journal article,"International Journal of Advanced Research in Science, Communication and Technology",25819429,Naksh Solutions,,null Prof. Vijaykumar S; null Shriya R; null Vibha M,"<jats:p>Real-time conversations with humans can be held by intelligent chatbots, utilized by businesses to enhance customer service and streamline operations. A gap is perceived in existing chatbots' achievement of human-like conversational abilities, despite their widespread acceptance and implementation across various sectors. This research paper discusses the development of a unique chatbot providing not only financial guidance but also emotional support, thus sparking a blend of technological innovation and human connection. Emotional states of users are deciphered by the chatbot, using natural language processing (NLP) and sentiment analysis. Tailored responses are generated, offering practical financial advice as well as emotional support during times of stress. Aiming to empower users in managing their financial challenges with serenity, the chatbot integrates emotional support, financial education, goal setting and planning, stress management, and continuous support and monitoring. Cutting-edge technologies, including the Python programming language and NLP libraries like spaCy and NLTK, are incorporated in the project. It interfaces with external financial APIs to provide real-time data enabling informed decision-making. Our chatbot's focus lies in providing a secure and positive environment for users to share financial worries, with the unique ability to comprehend and respond to financial inquiries and emotional nuances in users' messages. By documenting the creation of this chatbot, a contribution is made to the expanding field of emotional intelligence in artificial intelligence applications. The expected outcomes of this research project include an enhanced sense of financial well-being for users, bridging the gap between technological solutions and human-centered support</jats:p>",,,509,516,Chatbot; Psychology; Political science; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.48175/ijarsct-15368,,10.48175/ijarsct-15368,,,0,001-536-892-769-300; 007-581-025-024-301; 017-516-148-600-358; 024-884-677-827-470; 032-073-204-736-529; 040-504-787-044-496; 070-245-057-854-299; 076-988-878-924-325; 081-253-420-497-353; 089-799-058-378-620; 131-707-499-552-559; 147-180-869-106-752,0,true,,gold
028-486-532-030-008,AI-Powered Test Automation for Financial Cloud Applications Using LLMs and Serverless Computing,2024-11-13,2024,journal article,International Journal For Multidisciplinary Research,25822160,International Journal for Multidisciplinary Research (IJFMR),,Saili Krishna Maliye -,"<jats:p>This article proposes a conceptual framework for AI-powered testing of financial cloud applications by integrating Large Language Models (LLMs) with serverless computing architecture. The framework addresses critical challenges in financial software testing, including regulatory compliance, security validation, and performance scalability. Through theoretical analysis, the design incorporates; sophisticated prompt engineering techniques and automated compliance monitoring systems to achieve high compliance validation accuracy while potentially reducing monitoring costs. The architecture suggests superior performance capabilities, potentially processing concurrent test executions efficiently while reducing operational costs compared to traditional testing infrastructures. The framework design anticipates improvements in test generation efficiency, regulatory compliance coverage, and overall testing costs while maintaining robust security standards through enhanced vulnerability detection mechanisms. The theoretical integration of LLMs with serverless computing shows promise in addressing the complex challenges of financial software testing, offering a potential pathway to a scalable, cost-effective solution that could meet the stringent requirements of modern financial cloud applications while ensuring regulatory compliance and security.</jats:p>",6,6,,,Cloud computing; Automation; Test (biology); Computer science; Computer security; Business; Data science; Finance; Engineering; Operating system; Mechanical engineering; Biology; Ecology,,,,,https://www.ijfmr.com/papers/2024/6/30383.pdf https://doi.org/10.36948/ijfmr.2024.v06i06.30383,http://dx.doi.org/10.36948/ijfmr.2024.v06i06.30383,,10.36948/ijfmr.2024.v06i06.30383,,,0,,0,true,cc-by-sa,hybrid
028-686-950-067-053,Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Yinyu Lan; Yanru Wu; Wang Xu; Weiqiang Feng; Youhao Zhang,"Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which should be the focus of future research. The FinChina SA dataset is publicly available at https://github.com/YerayL/FinChina-SA",,,,,Sentiment analysis; Computer science; Resource (disambiguation); Matching (statistics); Data science; Domain (mathematical analysis); Scale (ratio); Finance; Natural language processing; Artificial intelligence; Business; Computer network; Mathematical analysis; Statistics; Physics; Mathematics; Quantum mechanics,,,,,https://arxiv.org/abs/2306.14096,http://dx.doi.org/10.48550/arxiv.2306.14096,,10.48550/arxiv.2306.14096,,,0,,1,true,other-oa,green
029-274-636-193-487,Utilizing LLM and Deep Learning Strategies to Amplify Algorithmic Proficiency in Detecting Complex Patterns of Insider Trading Fraud,2024-10-29,2024,book chapter,"Proceedings in Adaptation, Learning and Optimization",23636084; 23636092,Springer Nature Switzerland,,Dishant Zaveri; Hemant Singh; Sayali Chaskar; Rihansh Hingad; Shubham Mehta; Nilesh Patil,,,,205,215,Insider; Insider trading; Computer science; Computer security; Data science; Artificial intelligence; Business; Political science; Finance; Law,,,,,,http://dx.doi.org/10.1007/978-3-031-71388-0_17,,10.1007/978-3-031-71388-0_17,,,0,001-584-182-593-773; 025-101-290-382-211; 043-195-518-806-686; 047-667-157-382-125; 056-936-072-543-70X; 057-909-396-298-555; 058-870-233-668-727; 068-156-341-222-719; 068-245-218-050-987; 074-271-264-930-965; 081-077-801-039-997; 089-349-706-608-503; 090-471-811-650-628; 096-138-348-963-874; 105-491-077-848-066; 108-208-427-337-016; 108-641-188-322-016; 196-089-171-467-863,0,false,,
029-633-297-149-300,LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making,2023-11-25,2023,conference proceedings article,4th ACM International Conference on AI in Finance,,ACM,,Kausik Lakkaraju; Sara E Jones; Sai Krishna Revanth Vuruma; Vishal Pallagani; Bharath C Muppasani; Biplav Srivastava,"As Large Language Model (LLM) based chatbots are becoming more accessible, users are relying on these chatbots for reliable and personalized recommendations in diverse domains, ranging from code generation to financial advisement. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We test widely used LLM-based chatbots, ChatGPT and Bard, and compare their performance against SafeFinance, a rule-based chatbot built using the Rasa platform. The comparison is across two critical tasks: product discovery and multi-product interaction, where product refers to banking products like Credit Cards, Certificate of Deposits, and Checking Accounts. With this study, we provide interesting insights into the chatbots' efficacy in financial advisement and their ability to provide fair treatment across different user groups. We find that both Bard and ChatGPT can make errors in retrieving basic online information, the responses they generate are inconsistent across different user groups, and they cannot be relied on for reasoning involving banking products. On the other hand, despite their limited generalization capabilities, rule-based chatbots like SafeFinance provide safe and reliable answers to users that can be traced back to their original source. Overall, although the outputs of the LLM-based chatbots are fluent and plausible, there are still critical gaps in providing consistent and reliable financial information.",,,100,107,Chatbot; Computer science; Context (archaeology); Certificate; Product (mathematics); Financial services; Domain (mathematical analysis); Set (abstract data type); World Wide Web; Finance; Business; Programming language; Paleontology; Mathematical analysis; Geometry; Mathematics; Algorithm; Biology,,,,,https://dl.acm.org/doi/pdf/10.1145/3604237.3626867 https://doi.org/10.1145/3604237.3626867,http://dx.doi.org/10.1145/3604237.3626867,,10.1145/3604237.3626867,,,0,002-924-317-409-959; 038-604-541-386-791; 040-386-951-442-61X; 053-591-547-250-936; 056-475-132-101-307; 062-684-353-689-974; 079-156-817-367-550; 104-851-877-479-801; 115-532-492-923-538; 116-457-340-331-743; 160-579-052-604-103,27,true,cc-by-nc,hybrid
029-863-051-915-101,Conversational AI and LLMs for Real-Time Troubleshooting and Decision Support in Asset Management,2024-11-15,2024,journal article,Journal of Quantum Science and Technology,30486351,Resagate Global,,Rajesh Ojha; null Er. Siddharth,"<jats:p>In the evolving landscape of asset management, leveraging advanced technologies such as Conversational AI and Large Language Models (LLMs) has become crucial for enhancing operational efficiency and decision-making. This paper explores the integration of Conversational AI and LLMs to enable real-time troubleshooting and decision support in asset management systems. By utilizing natural language processing (NLP) and machine learning techniques, these models can understand, interpret, and respond to complex asset-related queries, allowing operators to efficiently diagnose issues and make informed decisions. We examine the potential benefits, such as improved response times, reduced human error, and enhanced decision-making accuracy, along with the challenges and limitations associated with deploying these technologies in real-world asset management environments. This research also highlights case studies and best practices for implementing Conversational AI and LLMs, offering insights into their scalability, adaptability, and potential for transforming asset management workflows in industries ranging from manufacturing to energy</jats:p>",1,4,,,Troubleshooting; Decision support system; Asset management; Asset (computer security); Business; Computer science; Process management; Finance; Computer security; Artificial intelligence; Operating system,,,,,,http://dx.doi.org/10.63345/jqst.v1i4.147,,10.63345/jqst.v1i4.147,,,0,,0,false,,
030-095-065-603-236,Chatbots in Marketing and Financial Sector,2025-08-08,2025,book chapter,AI and Fintech,,CRC Press,,R. Esther Hepziba; A. Millicent Serena; Alamelu Mangai Raman,"This chapter discusses the ubiquitous rapid growth of Artificial Intelligence technology and the disruptive invasion of AI across domains and firms selling their products online have become highly dependent on AI-based tools to stay on par with the technology transformations in order to strive towards sustainability and competitive edge. Chatbot is one such artificial intelligence software used by marketers to interact with customers online. Hence marketers use chatbots to keep customers engaged which reduces the waiting time of consumers and helps to respond to their queries promptly. Consumers appreciate such interactions as they remove boredom and find it convenient to live-chat with bots, state Petter Bae Brandtzaeg and Asbjorn Folstad in their study (2017). So, marketers have started capitalizing this technology to keep customers engaged and to give information about the products and financial products/services that the customers want, in a personalized manner. Continuous learning happens in chatbots which aids in customized responses and product suggestions. This technology is an emerging one, which finds its place in almost all domains. Since marketing is inevitable and financial planning is essential, which indirectly impacts the commerce and economy of the country, the use of these latest technologies has become imperative. This chapter helps to understand the significance that chatbots have on customers, businesses and resultantly on customer engagement. Existing chatbots implementation and success stories in the financial sector have been discussed to identify the potential of chatbots and use it for advancement and better customer relationships in future. A properly structured questionnaire was framed to carry out the study collecting data from respondents in Chennai to find out the impact on customer engagement valence. Appropriate statistical tools were applied to analyze the data. Though there are some adverse effects or negative engagement due to factors such as privacy and security concerns, it has been found that chatbots enhance positive engagement in customers.",,,183,201,Financial sector; Business; Marketing; Finance,,,,,,http://dx.doi.org/10.1201/9781003645849-11,,10.1201/9781003645849-11,,,0,,0,false,,
030-411-992-343-692,The Role of Large Language Models in Financial Planning: Replace or Complement Advisors?,,2025,preprint,,,Elsevier BV,,Daniel Greene; Douglas J. Fairhurst,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5985655,,10.2139/ssrn.5985655,,,0,,0,false,,
030-780-820-536-934,The FinBen: An Holistic Financial Benchmark for Large Language Models,2024-02-19,2024,preprint,arXiv (Cornell University),,,,Qianqian Xie; Weiguang Han; Zhengyu Chen; Ruoyu Xiang; Xiao Zhang; Yueru He; Mengxi Xiao; Dong Li; Yongfu Dai; Duanyu Feng; Yijing Xu; Haoqiang Kang; Ziyan Kuang; Chenhan Yuan; Kailai Yang; Zheheng Luo; Tianlin Zhang; Zhiwei Liu; Guojun Xiong; Zhiyang Deng; Yuechen Jiang; Zhiyuan Yao; Haohang Li; Yangyang Yu; Gang Hu; Jiajia Huang; Xiao-Yang Liu; Alejandro Lopez-Lira; Benyou Wang; Yanzhao Lai; Hao Wang; Min Peng; Sophia Ananiadou; Jimin Huang,"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain. The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities. FinBen seeks to continuously evaluate LLMs in finance, fostering AI development with regular updates of tasks and models.",,,,,Benchmark (surveying); Economics; Finance; Computer science; Business; Geography; Cartography,,,,,https://arxiv.org/abs/2402.12659,http://dx.doi.org/10.48550/arxiv.2402.12659,,10.48550/arxiv.2402.12659,,,0,,3,true,,green
030-987-504-867-190,Connectedness of Chinese financial institutions in news extracted by LLM,,2025,journal article,Procedia Computer Science,18770509,Elsevier BV,,Xueheng Wang; Jingrui Cai; Yvbo Yin; Pei Quan; Jingyu Li,,266,,540,546,Computer science; Social connectedness; Finance; Business; Psychology; Psychotherapist,,,,Humanities and Social Science Fund of Ministry of Education of China; Natural Science Foundation of Beijing Municipality; National Natural Science Foundation of China; National Natural Science Foundation of China,,http://dx.doi.org/10.1016/j.procs.2025.08.068,,10.1016/j.procs.2025.08.068,,,0,001-522-111-296-874; 001-746-575-466-424; 013-999-631-017-481; 027-593-869-573-689; 029-652-108-487-444; 063-329-652-421-660; 083-393-929-647-227; 119-031-570-848-300; 127-193-967-271-230; 161-556-153-901-830,0,true,,bronze
031-257-663-390-395,TradingAgents: Multi-Agents LLM Financial Trading Framework,2025-06-01,2025,preprint,RePEc: Research Papers in Economics,,,,Yijia Xiao; Edward Sun; Di Luo; Wei Wang,"Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs). In finance, efforts have largely focused on single-agent systems handling specific tasks or multi-agent frameworks independently gathering data. However, multi-agent systems' potential to replicate real-world trading firms' collaborative dynamics remains underexplored. TradingAgents proposes a novel stock trading framework inspired by trading firms, featuring LLM-powered agents in specialized roles such as fundamental analysts, sentiment analysts, technical analysts, and traders with varied risk profiles. The framework includes Bull and Bear researcher agents assessing market conditions, a risk management team monitoring exposure, and traders synthesizing insights from debates and historical data to make informed decisions. By simulating a dynamic, collaborative trading environment, this framework aims to improve trading performance. Detailed architecture and extensive experiments reveal its superiority over baseline models, with notable improvements in cumulative returns, Sharpe ratio, and maximum drawdown, highlighting the potential of multi-agent LLM frameworks in financial trading. More details on TradingAgents are available at https://TradingAgents-AI.github.io.",,,,,Business; Finance; Financial system,,,,,https://arxiv.org/abs/2412.20138,http://dx.doi.org/10.48550/arxiv.2412.20138,,10.48550/arxiv.2412.20138,,,0,,1,true,,green
031-378-437-521-562,Meeting companies' innovative requirements on online technology trading platforms: A novel large language model-based framework,2025-09-08,2025,journal article,Information Processing & Management,03064573; 18735371,Elsevier BV,United Kingdom,Qingyu Xu; Zhaobin Liu; Jian Ma,,63,2,104392,104392,Computer science; Business; Software engineering; Engineering management; Systems engineering; Engineering,,,,,,http://dx.doi.org/10.1016/j.ipm.2025.104392,,10.1016/j.ipm.2025.104392,,,0,007-262-919-466-194; 007-516-882-620-676; 009-661-190-177-945; 010-191-196-034-023; 014-966-765-543-108; 024-542-144-364-266; 026-018-449-868-416; 028-302-244-154-998; 029-142-410-873-194; 029-465-411-286-449; 039-397-017-996-312; 039-948-194-386-855; 040-984-108-150-082; 042-593-706-547-164; 052-761-710-566-424; 052-872-610-694-789; 079-927-095-270-775; 080-193-012-588-91X; 085-850-298-302-866; 091-418-697-746-434; 119-680-014-224-847; 121-307-168-455-521; 128-127-423-911-296; 138-480-883-136-200; 152-335-054-288-940; 158-729-730-798-226; 163-538-463-102-181; 177-397-562-197-560; 183-050-341-556-640; 192-891-568-264-389,0,false,,
031-876-505-696-121,Comparative Analysis of Financial Sentiment Analysis Models for the Thai Stock Market: Traditional NLP vs. GPT vs. Gemini,2024-01-01,2024,preprint,,,Elsevier BV,,Dhouchdhanin Leechewyuwasorn; Nuthdanai Wangpratham,,,,,,Sentiment analysis; Stock market; Artificial intelligence; Natural language processing; Computer science; Finance; Economics; Geography; Context (archaeology); Archaeology,,,,,,http://dx.doi.org/10.2139/ssrn.4921837,,10.2139/ssrn.4921837,,,0,,3,false,,
032-073-204-736-529,Implementation of Artificial Intelligence Chatbot in Optimizing Customer Service in Financial Technology Company PT. FinAccel Finance Indonesia,2022-12-26,2022,conference proceedings article,The 5th International Conference on Vocational Education Applied Science and Technology 2022,,MDPI,,Mohammad Ridha; Khansa Haura Maharani,"Chatbots are increasingly being used as a representative agent for big companies as their implementation opens up promising opportunities to improve customer service. The present paper examines chatbots in this context, elaborating on their impacts in service quality. The purpose of this research is to understand the form of implementation of an artificial intelligence chatbot in the field of customer service in PT. FinAccel Finance Indonesia. A descriptive qualitative method is used in this research by collecting data through interviews and literature studies. This study uses SWOT analysis (strength, weakness, opportunity, threat) as a strategy planning technique and problem solving that emphasizes the importance of the role of internal and external factors. The authors informed the solution obtained by PT. FinAccel Finance Indonesia after knowing the advantages and disadvantages of chatbots.",,,21,21,Chatbot; SWOT analysis; Context (archaeology); Knowledge management; Customer service; Service (business); Big data; Competitive intelligence; Computer science; Finance; Business; Marketing; Artificial intelligence; Paleontology; Biology; Operating system,chatbot; customer service; artificial intelligence; SWOT analysis,,,,https://www.mdpi.com/2504-3900/83/1/21/pdf?version=1672135499 https://doi.org/10.3390/proceedings2022083021,http://dx.doi.org/10.3390/proceedings2022083021,,10.3390/proceedings2022083021,,,0,034-055-127-676-916; 044-523-432-982-677; 136-903-168-416-157,6,true,cc-by,hybrid
032-274-787-083-494,Hybrid LLM-BERT Classification Pipeline with Synthetic Data Generation for Financial Document Processing,2025-10-09,2025,journal article,European Modern Studies Journal,25229400,Lomaka & Romina Publisher,,Niraj Katkamwar,"<jats:p>The hybrid classification architecture presented in this article addresses the critical cold-start challenge in financial document processing by strategically combining large language models with BERT-based classifiers. Financial institutions face significant barriers to implementing automated classification systems due to extensive labeled data requirements, resulting in extended timelines and substantial implementation costs. The described architecture begins with zero-shot LLM classification for immediate deployment capability, then progressively transitions to more efficient BERT models as classification decisions accumulate. A sophisticated synthetic data generation component accelerates this transition by creating realistic financial documents based on patterns identified in previously classified materials, significantly reducing dependence on authentic examples while maintaining comparable performance. The system incorporates confidence-based routing between models based on dynamically evolving thresholds and continuous feedback loops for model refinement. Validation using IRS tax form datasets demonstrates the architecture's effectiveness in rapidly achieving high classification accuracy while substantially reducing computational costs compared to traditional methods. The framework enables financial institutions to implement efficient document classification systems without the prohibitive initial data requirements that typically delay adoption of conventional approaches.</jats:p>",9,5,1024,1031,,,,,,,http://dx.doi.org/10.59573/emsj.9(5).2025.94,,10.59573/emsj.9(5).2025.94,,,0,024-031-210-885-357; 043-521-729-643-961; 048-949-263-591-926; 050-101-535-062-563; 055-909-321-243-397; 090-137-849-181-640; 120-671-169-740-185; 132-269-800-515-060,0,false,,
032-662-566-749-760,Improved Chatbot Tool for Stock Trading using Machine Learning,2025-09-17,2025,conference proceedings article,2025 3rd International Conference on Intelligent Cyber Physical Systems and Internet of Things (ICoICI),,IEEE,,K Malarvizhi; Akash A; Vasanth L; Rangarajan M,,,,731,736,,,,,,,http://dx.doi.org/10.1109/icoici65217.2025.11252976,,10.1109/icoici65217.2025.11252976,,,0,000-168-884-667-432; 002-863-836-038-866; 020-140-524-819-878; 023-799-688-698-763; 033-493-544-037-158; 061-143-694-186-306; 061-512-563-852-197; 076-486-787-420-973; 078-731-741-861-406; 084-819-478-201-075; 097-528-605-219-457; 156-278-238-002-078; 180-437-721-688-546; 181-728-813-780-215; 187-728-311-596-90X,0,false,,
032-983-647-482-962,Agent-Based Simulation of a Financial Market with Large Language Models,2025-12-15,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Ryuji Hashimoto; Takehiro Takayanagi; Masahiro Suzuki; Kiyoshi Izumi,,,,20,28,,,,,,,http://dx.doi.org/10.1007/978-3-032-13562-9_2,,10.1007/978-3-032-13562-9_2,,,0,001-627-712-016-947; 009-424-979-704-037; 022-609-226-545-432; 027-490-023-093-276; 055-741-317-620-149; 064-223-688-926-151; 085-128-627-835-316; 085-905-526-143-293; 131-573-978-530-030; 137-094-388-525-215; 145-890-687-666-437; 149-382-854-694-094; 175-502-649-836-520,0,false,,
033-240-641-756-052,"Generative Large Language Models in Clinical, Legal and Financial Domains",2025-06-17,2025,book chapter,Transformative Natural Language Processing,,Springer Nature Switzerland,,Geetanjali Garg; Shobha Bhatt,,,,205,221,Generative grammar; Linguistics; Computer science; Artificial intelligence; Natural language processing; Philosophy,,,,,,http://dx.doi.org/10.1007/978-3-031-88988-2_9,,10.1007/978-3-031-88988-2_9,,,0,035-567-339-868-958; 039-187-427-813-456; 051-893-437-437-119; 053-831-615-322-138; 065-954-058-898-702; 087-807-064-907-108; 105-956-835-654-883; 153-529-717-074-241; 155-286-409-767-328; 157-995-960-565-853; 178-440-806-032-007; 182-441-419-456-36X; 194-347-983-609-632,0,false,,
033-402-376-477-376,Enhancing portfolio optimization with multi-LLM sentiment aggregation: A Black-Litterman integration approach,2025-08-14,2025,journal article,Investment Management and Financial Innovations,18104967; 18129358; 18134998,LLC CPC Business Perspectives,Ukraine,Lamukanyani Alson Mantshimuli; John Weirstrass Muteba Mwamba,"<jats:p>Type of the article: Research Article; AbstractSentiment analysis of financial text data plays a crucial role in investment decision-making, yet existing approaches often rely on single-model sentiment scores that may suffer from biases or hallucinations. This study aims to enhance portfolio optimization by integrating sentiment signals from multiple Large Language Models (LLMs) into the Black-Litterman framework. The proposed method aggregates sentiment scores from three finance-domain fine-tuned LLMs using a Long Short-Term Memory network, which captures non-linear relationships and temporal dependencies to produce a robust Meta-LLM sentiment score. This score is then incorporated into the Black-Litterman model as investor views to derive optimal portfolio weights. The methodology is tested on a portfolio of S&amp;amp;amp;P 500 stocks. The results show that the proposed approach significantly improves portfolio performance, achieving an annualized return of 31.22%, compared to 24.57% for the market capital-weighted portfolio. Additionally, the model attains a Sharpe Ratio of 3.02, an Omega Ratio of 2.48, and a Jensen’s Alpha of 1.95%, outperforming both the benchmark portfolios and portfolios based on single-LLM sentiment. The findings demonstrate that aggregating sentiment from multiple LLMs enhances risk-adjusted returns while mitigating model-specific limitations. Future research could explore the integration of LLMs with different architectures to further refine sentiment-aware portfolio strategies.</jats:p>",22,3,213,226,Black–Litterman model; Portfolio optimization; Portfolio; Computer science; Econometrics; Economics; Financial economics; Replicating portfolio,,,,,https://www.businessperspectives.org/images/pdf/applications/publishing/templates/article/assets/22699/IMFI_2025_03_Mantshimuli.pdf https://doi.org/10.21511/imfi.22(3).2025.16,http://dx.doi.org/10.21511/imfi.22(3).2025.16,,10.21511/imfi.22(3).2025.16,,,0,005-935-564-222-967; 014-711-631-488-436; 015-246-195-452-343; 016-546-361-428-113; 044-456-506-571-452; 044-632-588-020-311; 045-214-117-227-804; 059-147-572-309-300; 060-952-442-274-661; 066-992-128-812-171; 087-837-274-931-541; 124-936-386-543-635; 126-226-045-638-771; 147-644-589-040-835; 149-160-037-003-51X; 168-747-195-050-276; 190-996-521-081-84X; 191-384-641-806-007,0,true,,gold
033-862-346-089-900,When CoMM Isn't Enough: Dynamic Congruence–Augmentation Trade-offs in LLM-based Multi-Agents Systems Across Input Complexity and Customer Willingness,2025-01-01,2025,preprint,,,Elsevier BV,,Ji Hyo Kang; Kyung  Joon Cha,,,,,,Congruence (geometry); Business; Computer science; Industrial organization; Psychology; Social psychology,,,,,,http://dx.doi.org/10.2139/ssrn.5584678,,10.2139/ssrn.5584678,,,0,,0,false,,
034-111-475-208-944,FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design,2024-05-20,2024,journal article,Proceedings of the AAAI Symposium Series,29944317,Association for the Advancement of Artificial Intelligence (AAAI),,Yangyang Yu; Haohang Li; Zhi Chen; Yuechen Jiang; Yang Li; Denghui Zhang; Rong Liu; Jordan W. Suchow; Khaldoun Khashanah,"<jats:p>Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce FinMem, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, FinMem's memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare FinMem with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, FinMem presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.</jats:p>",3,1,595,597,Computer science; Interpretability; Trading strategy; Financial market; Artificial intelligence; Finance; Business,,,,,https://ojs.aaai.org/index.php/AAAI-SS/article/download/31290/33450 https://doi.org/10.1609/aaaiss.v3i1.31290 http://arxiv.org/pdf/2311.13743 http://arxiv.org/abs/2311.13743,http://dx.doi.org/10.1609/aaaiss.v3i1.31290,,10.1609/aaaiss.v3i1.31290,,,0,,20,true,,bronze
034-442-958-153-593,"Designing multi-model conversational AI financial systems: understanding
  sensitive values of women entrepreneurs in Brazil",2024-06-27,2024,preprint,arXiv (Cornell University),,,,Heloisa Candello; Gabriel Meneguelli Soella; Leandro de Carvalho Nascimento,"Small business owners (SBOs), specially women, face several challenges in everyday life, especially when asking for microcredit loans from financial institutions. Usual difficulties include low credit scores, unbaked situations, outstanding debts, informal employment situations, inability to showcase their payable capacity, and lack of financial guarantor. Moreover, SBOs often need help applying for microcredit loans due to the lack of information on how to proceed. The task of asking for a loan is a complex practice, and asymmetric power relationships might emerge, but that benefits micro-entrepreneurs only sometimes. In this paper, we interviewed 20 women entrepreneurs living in a low-income community in Brazil. We wanted to unveil value tensions derived from this practice that might influence the design of AI technologies for the public. In doing so, we used a conversational system as a probe to understand the opportunities for empowering their practices with the support of AI multimedia conversational systems. We derived seven recommendations for designing AI systems for evaluating micro-business health in low-income communities.",,,,,Women entrepreneurs; Business; Computer science; Finance; Economics; Entrepreneurship,,,,,https://arxiv.org/abs/2406.19601,http://dx.doi.org/10.48550/arxiv.2406.19601,,10.48550/arxiv.2406.19601,,,0,,0,true,,green
034-927-575-494-361,FinLLM-B: When Large Language Models Meet Financial Breakout Trading,,2025,conference proceedings article,Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track),,Association for Computational Linguistics,,Kang Zhang; Osamu Yoshie; Lichao Sun; Weiran Huang,,,,349,357,Breakout; Computer science; Business; Programming language; Finance,,,,,,http://dx.doi.org/10.18653/v1/2025.naacl-industry.29,,10.18653/v1/2025.naacl-industry.29,,,0,,0,false,,
035-044-301-076-434,"The Economic Implications of Large Language Model Selection on Earnings
  and Return on Investment: A Decision Theoretic Model",2024-05-27,2024,preprint,arXiv (Cornell University),,,,Geraldo Xexéo; Filipe Braida; Marcus Parreiras; Paulo Xavier,"Selecting language models in business contexts requires a careful analysis of the final financial benefits of the investment. However, the emphasis of academia and industry analysis of LLM is solely on performance. This work introduces a framework to evaluate LLMs, focusing on the earnings and return on investment aspects that should be taken into account in business decision making. We use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use. The study reveals how the superior accuracy of more expensive models can, under certain conditions, justify a greater investment through more significant earnings but not necessarily a larger RoI. This article provides a framework for companies looking to optimize their technology choices, ensuring that investment in cutting-edge technology aligns with strategic financial objectives. In addition, we discuss how changes in operational variables influence the economics of using LLMs, offering practical insights for enterprise settings, finding that the predicted gain and loss and the different probabilities of success and failure are the variables that most impact the sensitivity of the models.",,,,,Earnings; Selection (genetic algorithm); Model selection; Economics; Investment (military); Return on investment; Decision model; Economic model; Financial economics; Econometrics; Computer science; Microeconomics; Mathematical economics; Finance; Artificial intelligence; Political science; Production (economics); Politics; Law,,,,,https://arxiv.org/abs/2405.17637,http://dx.doi.org/10.48550/arxiv.2405.17637,,10.48550/arxiv.2405.17637,,,0,,1,true,,green
035-724-691-318-820,"Assessing LLMs' Mathematical Reasoning in Financial Document Question
  Answering",2024-02-17,2024,preprint,arXiv (Cornell University),,,,Pragya Srivastava; Manuj Malik; Vivek Gupta; Tanuja Ganu; Dan Roth,"Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding of LLMs abilities for such a task.",,,,,Question answering; Finance; Computer science; Business; Artificial intelligence,,,,,https://arxiv.org/abs/2402.11194,http://dx.doi.org/10.48550/arxiv.2402.11194,,10.48550/arxiv.2402.11194,,,0,,0,true,,green
036-072-474-076-306,"Comparative Advances in Financial Sentiment Analysis:A Review of BERT,FinBert, and Large Language Models",2025-02-05,2025,conference proceedings article,2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT),,IEEE,,Manish Barath Mahendran; Aswin Kumar Gokul; Poornima Lakshmi; S. Pavithra,,,,39,45,Sentiment analysis; Computer science; Language model; Natural language processing; Artificial intelligence; Data science,,,,,,http://dx.doi.org/10.1109/idciot64235.2025.10914764,,10.1109/idciot64235.2025.10914764,,,0,012-477-451-286-452; 017-602-162-984-352; 028-374-984-419-069; 036-132-253-625-326; 037-294-941-464-552; 043-035-412-497-640; 047-666-875-310-084; 051-181-512-576-659; 056-925-469-475-633; 068-064-588-742-224; 086-397-082-495-682; 087-807-064-907-108; 090-503-174-316-902; 099-573-060-261-297; 109-451-804-243-848; 114-266-013-023-458; 126-728-024-659-67X; 128-006-378-271-355; 135-256-035-814-384; 142-365-418-228-530; 172-592-058-859-088; 177-243-432-437-122,1,false,,
036-315-591-397-76X,Large Language Models (LLMs) for Financial Sentiment Analysis and Market Forecasting,2025-11-02,2025,book chapter,"Advances in Economics, Business and Management Research",27317854; 23525428,Atlantis Press International BV,,Vishnu Ravi; Vineet Kumar Srivastava; Maninder Pal Singh; Srinivas Chippagiri; Nikhil Kassetty; Padma Naresh Vardhineedi; Ravi Kumar Burila; Nuzhat Noor Islam Prova,,,,681,694,,,,,,,http://dx.doi.org/10.2991/978-94-6463-872-1_43,,10.2991/978-94-6463-872-1_43,,,0,,0,false,,
036-851-552-690-727,Large Language Model for Dynamic Strategy Interchange in Financial Markets,2024-04-25,2024,conference proceedings article,2024 9th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),,IEEE,,Xingyu Zhong; Zongyi Zhao; Bingda Yan; Qingzhen Xu,,,,306,312,Computer science; Financial market; Finance; Business,,,,,,http://dx.doi.org/10.1109/icccbda61447.2024.10569928,,10.1109/icccbda61447.2024.10569928,,,0,012-194-321-529-671; 018-866-913-622-957; 019-118-328-510-826; 030-454-248-865-53X; 035-726-859-927-793; 044-456-506-571-452; 046-368-392-682-64X; 053-847-859-862-209; 057-487-165-995-479; 100-446-896-445-856; 119-730-045-002-457; 137-898-855-175-685; 154-201-884-155-679; 190-050-627-029-962; 192-806-908-794-847; 193-278-292-674-926,3,false,,
037-444-217-173-501,Dynamic Docker Resource Scaling Architecture: LLM-Driven Urgency Analysis for Multi-Agent Financial Systems,2025-09-01,2025,journal article,International Journal of Computational and Experimental Science and Engineering,21499144,Iskender AKKURT,,null Kiran Purushotham,"<jats:p>This article presents a groundbreaking framework for Dynamic Docker Resource Scaling Based on LLM-Inferred Urgency, specifically designed to address the critical challenges of resource management in financial multi-agent systems. The integration of large language model-driven multi-agent systems has revolutionized financial services, enabling sophisticated workflows for customer support, fraud detection, and regulatory compliance, yet traditional container orchestration frameworks fail to meet the dynamic demands of these applications. The proposed framework leverages the semantic understanding capabilities of LLMs to analyze incoming financial queries and infer their urgency, enabling proactive resource allocation that aligns computational resources with business priorities. Built on the LangGraph platform, the system comprises three core components: an LLM-based Urgency Analyzer that classifies queries based on semantic cues and contextual indicators, a Dynamic Resource Manager that interfaces with Docker Engine to adjust container resources in real-time, and a persistence layer that maintains state across distributed workflows. The framework implements a hierarchical priority model with multiple urgency levels, enabling granular control over resource allocation while maintaining sub-second response times. Through comprehensive evaluation across real-world financial applications, including fraud detection, customer support prioritization, and regulatory compliance reporting, the framework demonstrates significant improvements in latency reduction, resource utilization efficiency, and cost optimization compared to traditional metric-based scaling approaches. The research establishes a new paradigm for AI infrastructure in financial services, where semantic-aware scaling enables intent-driven resource management that fundamentally transforms how institutions deliver customer experiences in an increasingly AI-powered landscape.</jats:p>",11,3,,,Architecture; Resource (disambiguation); Computer science; Business; Computer network; Art; Visual arts,,,,,,http://dx.doi.org/10.22399/ijcesen.3840,,10.22399/ijcesen.3840,,,0,040-720-403-471-859; 197-226-049-956-73X,0,true,,bronze
037-444-316-448-048,Leveraging LLMs for Financial Fraud Detection and Secure Auditing,2025-10-24,2025,book chapter,Securing Large Language Models Against Emerging Threats,,IGI Global Scientific Publishing,,Syed Q. Raza; Rindah Febriana Suryawati; Wisam Hazım Gwad,"<jats:p>The rapid adoption of Large Language Models (LLMs) in the financial sector presents both transformative opportunities and unprecedented risks. This chapter explores the application of LLMs for financial fraud detection and secure auditing, highlighting how advanced natural language processing techniques can identify anomalous transactions, detect patterns of illicit activity, and enhance compliance mechanisms. By integrating machine learning with domain-specific financial knowledge, LLMs offer real-time insights into complex datasets, improving the accuracy and efficiency of fraud detection systems. The chapter also addresses challenges related to data privacy, adversarial attacks, and model interpretability, proposing strategies to mitigate risks while maximizing the benefits of LLM deployment in financial oversight. Ultimately, this work underscores the potential of LLMs as robust tools for safeguarding financial integrity in an increasingly digital economy.</jats:p>",,,81,112,,,,,,,http://dx.doi.org/10.4018/979-8-3373-7133-7.ch003,,10.4018/979-8-3373-7133-7.ch003,,,0,,0,false,,
038-241-735-129-443,FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Hongyang Yang,"As financial institutions and professionals increasingly incorporate Large Language Models (LLMs) into their workflows, substantial barriers, including proprietary data and specialized knowledge, persist between the finance sector and the AI community. These challenges impede the AI community's ability to enhance financial tasks effectively. Acknowledging financial analysis's critical role, we aim to devise financial-specialized LLM-based toolchains and democratize access to them through open-source initiatives, promoting wider AI adoption in financial decision-making.In this paper, we introduce FinRobot, a novel open-source AI agent platform supporting multiple financially specialized AI agents, each powered by LLM. Specifically, the platform consists of four major layers: 1) the Financial AI Agents layer that formulates Financial Chain-of-Thought (CoT) by breaking sophisticated financial problems down into logical sequences; 2) the Financial LLM Algorithms layer dynamically configures appropriate model application strategies for specific tasks; 3) the LLMOps and DataOps layer produces accurate models by applying training/fine-tuning techniques and using task-relevant data; 4) the Multi-source LLM Foundation Models layer that integrates various LLMs and enables the above layers to access them directly. Finally, FinRobot provides hands-on for both professional-grade analysts and laypersons to utilize powerful AI techniques for advanced financial analysis. We open-source FinRobot at \url{https://github.com/AI4Finance-Foundation/FinRobot}.",,,,,Open source; Computer science; Artificial intelligence; Natural language processing; Programming language; Software engineering; Software,,,,,https://arxiv.org/pdf/2405.14767 https://arxiv.org/abs/2405.14767,http://dx.doi.org/10.2139/ssrn.4841493,,10.2139/ssrn.4841493,,,1,069-507-976-179-943; 081-775-177-194-94X; 098-428-380-981-106,18,true,,green
038-409-574-648-694,"Aligning LLMs with Human Instructions and Stock Market Feedback in
  Financial Sentiment Analysis",2024-10-18,2024,preprint,arXiv (Cornell University),,,,Zijie Zhao; Roy E. Welsch,"Financial sentiment analysis is crucial for trading and investment decision-making. This study introduces an adaptive retrieval augmented framework for Large Language Models (LLMs) that aligns with human instructions through Instruction Tuning and incorporates market feedback to dynamically adjust weights across various knowledge sources within the Retrieval-Augmented Generation (RAG) module. Building upon foundational models like LLaMA 2, we fine-tune a series of LLMs ranging from 7B to 70B in size, enriched with Instruction Tuning and RAG, and further optimized through direct feedback and Reinforcement Learning (RL)-based refinement methods applied to the source weights of RAG.Through extensive evaluation, we demonstrate that the sentiment outputs from our LLMs more accurately mirror the intrinsic sentiment of textual data, showcasing a 1% to 6% boost in accuracy and F1 score over existing state-of-the-art models and leading conversational AI systems. Moreover, the sentiments extracted are more indicative of the directions in stock price movements. On top of that, we successfully construct portfolios that yield a 3.61% higher Sharpe ratio compared to the S&P 500 baseline in bullish markets. These portfolios also demonstrate resilience in bearish markets, with a 5x reduction in return losses compared to those typically experienced by the S&P 500.",,,,,Stock market; Financial market; Business; Stock (firearms); Finance; Financial economics; Economics; Engineering; Mechanical engineering; Paleontology; Horse; Biology,,,,,https://arxiv.org/abs/2410.14926,http://dx.doi.org/10.48550/arxiv.2410.14926,,10.48550/arxiv.2410.14926,,,0,,2,true,,green
038-443-109-905-391,"Large Language Model Agents for Investment Management: Foundations, Benchmarks, and Research Frontiers",2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Preetha Saha; Jingrao Lyu; Arnav Saxena; Tianjiao Zhao; Dhagash Mehta,,,,736,744,,,,,,,http://dx.doi.org/10.1145/3768292.3770387,,10.1145/3768292.3770387,,,0,034-002-562-823-256; 034-111-475-208-944; 056-454-472-053-759; 078-351-781-717-578; 093-903-483-464-134; 096-963-090-316-775; 116-773-224-960-301; 119-425-902-411-378; 141-096-698-263-330; 157-332-925-099-669; 158-910-643-170-648; 168-905-676-553-211; 177-575-126-909-348; 183-434-875-935-824; 186-827-515-649-986; 191-445-828-814-96X,0,false,,
038-506-901-009-633,Enhancing Cryptocurrency Trading Strategies: A Deep Reinforcement Learning Approach Integrating Multi-Source LLM Sentiment Analysis,2025-03-17,2025,conference proceedings article,2025 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CiFer),,IEEE,,Nanjiang Du; Yida Zhao; Jintao Wang; Yicheng Zhu; Siyu Xie; Luyao Yang; Yiru Tong; Shengzhe Xu; Wangying Zhang; Zecheng Tang; Kai Xu; Jianfeng Ren; Tianxiang Cui,,,,1,7,Cryptocurrency; Reinforcement learning; Computer science; Sentiment analysis; Artificial intelligence; Reinforcement; Data science; Machine learning; Computer security; Engineering; Structural engineering,,,,Ningbo Science and Technology Bureau,,http://dx.doi.org/10.1109/cifer64978.2025.10975733,,10.1109/cifer64978.2025.10975733,,,0,000-454-346-263-577; 002-386-175-636-156; 003-370-904-320-830; 003-395-819-830-810; 006-617-699-187-321; 007-474-591-110-446; 010-062-908-684-702; 013-085-492-674-48X; 017-085-332-707-831; 022-154-504-510-654; 030-097-726-947-327; 034-638-136-380-118; 040-625-215-807-84X; 044-503-041-756-113; 051-264-449-862-524; 059-592-450-070-325; 081-535-653-965-985; 094-910-748-277-61X; 097-340-261-203-309; 099-314-269-291-052; 101-268-151-629-586; 103-212-983-826-945; 105-393-256-094-440; 105-749-255-720-379; 108-741-260-578-293; 111-642-491-385-477; 112-837-469-907-984; 121-496-376-380-902; 135-624-906-435-66X; 136-601-270-260-722; 142-721-088-979-670; 143-224-375-571-538; 145-398-270-985-315; 159-295-887-189-841; 172-516-822-346-030; 185-513-430-037-78X; 190-624-303-228-420,0,false,,
038-611-674-466-408,TrustCom - BONIK: A Blockchain Empowered Chatbot for Financial Transactions,,2020,conference proceedings article,"2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",,IEEE,,Saiful Islam Bhuiyan; Abdur Razzak; Sadek Ferdous; Mohammad Jabed Morshed Chowdhury; Mohammad A. Hoque; Sasu Tarkoma,"A Chatbot is a popular platform to enable users to interact with a software or website to gather information or execute actions in an automated fashion. In recent years, chatbots are being used for executing financial transactions, however, there are a number of security issues, such as secure authentication, data integrity, system availability and transparency, that must be carefully handled for their wide-scale adoption. Recently, the blockchain technology, with a number of security advantages, has emerged as one of the foundational technologies with the potential to disrupt a number of application domains, particularly in the financial sector. In this paper, we forward the idea of integrating a chatbot with blockchain technology in the view to improve the security issues in financial chatbots. More specifically, we present BONIK, a blockchain empowered chatbot for financial transactions, and discuss its architecture and design choices. Furthermore, we explore the developed Proof-of-Concept (PoC), evaluate its performance, analyse how different security and privacy issues are mitigated using BONIK.",,,1079,1088,Information privacy; Architecture; Financial transaction; Software; Chatbot; Blockchain; Transparency (behavior); Data integrity; Computer security; Computer science,Blockchain; Chatbot; Financial Chatbot; Financial Transaction; Private Blockchain; Hyperledger Fabric,,,,https://arxiv.org/abs/2011.08846 https://doi.org/10.1109/TrustCom50675.2020.00143 https://researchportal.helsinki.fi/en/publications/bonik-a-blockchain-empowered-chatbot-for-financial-transactions https://dblp.uni-trier.de/db/conf/trustcom/trustcom2020.html#BhuiyanRFCHT20 https://arxiv.org/pdf/2011.08846,http://dx.doi.org/10.1109/trustcom50675.2020.00143,,10.1109/trustcom50675.2020.00143,3133222064,,1,029-059-641-609-668; 056-635-093-634-345; 056-928-487-967-475; 087-245-989-849-446; 112-079-416-063-263; 152-662-768-493-397; 166-202-673-282-408; 167-808-180-788-454; 171-576-034-263-697; 177-735-601-581-063,28,true,,green
038-912-486-857-494,Self-explanatory and Retrieval-augmented LLMs for Financial Sentiment Analysis,2025-03-31,2025,conference proceedings article,Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing,,ACM,,Filippo Pallucchini; Xulang Zhang; Rui Mao; Erik Cambria,,,,131,137,Sentiment analysis; Computer science; Information retrieval; Artificial intelligence,,,,,,http://dx.doi.org/10.1145/3672608.3707894,,10.1145/3672608.3707894,,,0,001-187-119-465-497; 003-064-825-636-604; 003-425-944-462-978; 004-853-485-857-424; 004-883-147-840-555; 011-166-060-710-645; 014-840-934-556-089; 019-910-108-042-783; 020-168-511-591-24X; 020-563-084-760-554; 023-331-435-226-414; 025-723-804-273-570; 028-169-927-550-228; 029-668-873-995-071; 035-109-675-952-739; 051-822-405-277-888; 056-226-337-623-814; 057-982-454-558-150; 060-933-879-970-578; 061-886-091-793-032; 074-627-604-608-628; 076-217-003-619-066; 076-795-863-495-475; 081-081-928-966-538; 092-832-898-852-612; 096-704-258-326-787; 114-298-872-576-313; 119-177-392-642-351; 155-498-300-221-705; 163-979-407-953-145; 164-331-023-528-275; 170-335-764-680-325; 181-246-395-796-414; 192-806-908-794-847,1,false,,
038-951-478-476-917,XBRL-Agent: Leveraging Large Language Models for Financial Report Analysis,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Xiao-Yang Liu,,,,,,XBRL; Computer science; Business; Programming language,,,,,,http://dx.doi.org/10.2139/ssrn.4993495,,10.2139/ssrn.4993495,,,0,,0,false,,
039-081-765-981-211,Research Guides: Trade and Private International Law LLM Long Paper Writing Group: Trade and Private International Law LLM Long Paper Writing Group,2018-09-26,2018,,,,,,Jennifer Allison,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,,,,,2901220081,,0,,0,false,,
039-442-503-186-307,Transforming Financial Statement Analysis with Large Language Models: A Survey of Approaches and Challenges,2025-10-01,2025,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Singapore,,Shubhangi Vairagar; Chetana Shravage; Priya Merti; Nikhil M. Ingale; Sakshi N. Gaikwad; Dhruv G. Yaranalkar; Atharva R. Pimple,,,,217,226,Statement (logic); Financial statement; Financial statement analysis; Computer science; Accounting; Management science; Business; Linguistics; Financial analysis; Economics; Philosophy; Audit,,,,,,http://dx.doi.org/10.1007/978-981-96-7502-9_18,,10.1007/978-981-96-7502-9_18,,,0,004-142-369-035-81X; 009-429-532-265-827; 090-650-395-550-663; 164-439-358-556-774; 185-121-488-620-182; 194-347-983-609-632; 197-187-178-011-856,0,false,,
039-446-766-198-391,"Evaluating Binary Decision Biases in Large Language Models: Implications
  for Fair Agent-Based Financial Simulations",2025-01-20,2025,preprint,arXiv (Cornell University),,,,Alicia Vidler; Toby Walsh,"Large Language Models (LLMs) are increasingly being used to simulate human-like decision making in agent-based financial market models (ABMs). As models become more powerful and accessible, researchers can now incorporate individual LLM decisions into ABM environments. However, integration may introduce inherent biases that need careful evaluation. In this paper we test three state-of-the-art GPT models for bias using two model sampling approaches: one-shot and few-shot API queries. We observe significant variations in distributions of outputs between specific models, and model sub versions, with GPT-4o-Mini-2024-07-18 showing notably better performance (32-43% yes responses) compared to GPT-4-0125-preview's extreme bias (98-99% yes responses). We show that sampling methods and model sub-versions significantly impact results: repeated independent API calls produce different distributions compared to batch sampling within a single call. While no current GPT model can simultaneously achieve a uniform distribution and Markovian properties in one-shot testing, few-shot sampling can approach uniform distributions under certain conditions. We explore the Temperature parameter, providing a definition and comparative results. We further compare our results to true random binary series and test specifically for the common human bias of Negative Recency - finding LLMs have a mixed ability to 'beat' humans in this one regard. These findings emphasise the critical importance of careful LLM integration into ABMs for financial markets and more broadly.",,,,,Binary number; Computer science; Econometrics; Economics; Mathematics; Arithmetic,,,,,https://arxiv.org/abs/2501.16356,http://dx.doi.org/10.48550/arxiv.2501.16356,,10.48550/arxiv.2501.16356,,,0,,0,true,,green
039-498-028-166-990,Finbloom: Knowledge Grounding Large Language Model with Real-Time Financial Data,2025-01-01,2025,preprint,,,Elsevier BV,,Ankur Sinha; Chaitanya Agarwal; Pekka Malo,,,,,,Computer science; Finance; Business,,,,,,http://dx.doi.org/10.2139/ssrn.5167956,,10.2139/ssrn.5167956,,,0,057-982-454-558-150; 090-417-304-783-146; 092-770-866-733-36X; 096-704-258-326-787; 099-500-667-461-132,0,false,,
039-538-514-023-008,Research on multimodal information query mechanism of power trading platform based on LLM,2025-09-12,2025,conference proceedings article,"2025 7th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)",,IEEE,,Yuhan Su; Wen Xie; Junliang Lyu; Shijie Ji; Ning Yang; Peng Yuan,,,,852,858,Mechanism (biology); Computer science; Information retrieval; Epistemology; Philosophy,,,,State Grid Corporation of China,,http://dx.doi.org/10.1109/iotaai66837.2025.11213440,,10.1109/iotaai66837.2025.11213440,,,0,001-953-251-381-936; 036-599-724-737-842; 055-259-454-191-648; 061-107-556-007-922; 085-213-273-695-331; 101-497-625-909-109; 153-069-349-527-782,0,false,,
039-715-840-742-395,Uncovering Financial Distress with Textual Risk Disclosures in Annual Reports: Insights from Large Language Models,2025-10-17,2025,journal article,Information Systems Frontiers,13873326; 15729419,Springer Science and Business Media LLC,Netherlands,Xiaoqian Zhu; Hanlin Jin; Hao Sun; Jianping Li,,,,,,Financial distress; Distress; Accounting; Business; Computer science; Psychology; Financial system; Psychotherapist,,,,National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China,,http://dx.doi.org/10.1007/s10796-025-10637-8,,10.1007/s10796-025-10637-8,,,0,000-343-244-304-516; 001-333-369-676-337; 003-277-816-007-362; 006-399-176-500-152; 009-064-513-871-312; 010-191-196-034-023; 012-042-954-373-766; 014-941-526-140-36X; 020-583-328-960-877; 021-412-594-162-528; 025-352-443-683-397; 026-493-857-774-008; 034-142-141-159-726; 037-530-557-213-718; 043-390-882-813-43X; 045-661-384-775-025; 051-652-729-402-973; 051-918-806-072-521; 054-482-129-887-735; 055-451-944-895-517; 057-689-834-354-114; 063-506-745-225-313; 064-833-589-804-333; 065-119-701-845-37X; 066-021-443-046-671; 070-504-773-098-766; 075-840-994-072-492; 078-115-749-440-586; 084-469-272-612-218; 086-489-415-563-371; 087-790-855-801-811; 091-209-848-710-287; 093-924-653-167-136; 096-803-094-933-698; 101-455-550-543-150; 104-640-094-472-865; 106-081-386-593-970; 106-435-573-609-638; 119-115-501-496-59X; 119-602-195-696-494; 124-654-607-235-084; 128-645-115-547-699; 133-028-613-973-671; 134-523-307-265-495; 136-610-389-552-890; 147-601-675-989-449; 149-666-928-057-835; 151-129-294-302-373; 154-293-426-700-837; 157-430-234-108-393; 174-999-179-533-380; 179-134-839-866-746; 182-738-555-672-662; 183-832-369-971-316; 185-753-639-892-712,0,false,,
039-793-155-472-634,Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of Llms for Financial Tasks,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Agam Shah; Sudheer Chava,"Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero-shot. In this work, we investigate the effectiveness of zero-shot LLMs in the financial domain. It compares the performance of ChatGPT in zero-shot mode with fine-tuned RoBERTa with annotated data, while also evaluating it against other open-source generative LLMs. The study addresses three research questions on data annotation, performance gaps, and the feasibility of employing generative models. The findings demonstrate that ChatGPT performs impressively without labeled data, but fine-tuned models generally outperform it. The research highlights the time-intensive nature of annotating with generative models.",,,,,Zero (linguistics); Benchmarking; HERO; Shot (pellet); Zero gravity; Economics; Physics; Computer science; Artificial intelligence; Management; Philosophy; Chemistry; Mechanics; Linguistics; Organic chemistry,,,,,https://arxiv.org/pdf/2305.16633 https://arxiv.org/abs/2305.16633,http://dx.doi.org/10.2139/ssrn.4458613,,10.2139/ssrn.4458613,,,0,006-399-176-500-152; 036-579-021-696-054; 057-982-454-558-150; 087-548-501-519-629; 131-785-775-558-150; 183-026-309-652-990,8,true,,green
039-886-107-477-505,Integrating Generative AI and Large Language Models in Financial Sector Risk Management:  Regulatory Frameworks and Practical Applications,,2025,journal article,Risk Management Magazine,26123665; 27242153,Italian Association of Financial Industry Risk Managers (AIFIRM),,Valentina Lagasio; Jasmine Pirillo; Michele Belloli,"<jats:p>The rapid advancement of artificial intelligence (AI) technologies, particularly generative AI and large language models (LLMs), has ushered in a new era of possibilities for the financial sector. This paper explores the integration of these cutting-edge technologies into financial sector risk management, examining both the potential applications and the necessary regulatory frameworks. We provide a comprehensive analysis of how generative AI and LLMs can revolutionize risk assessment, fraud detection, market analysis, and regulatory compliance. The study delves into the technical aspects of these AI models, their implementation challenges, and the implications for existing risk management practices. Furthermore, we propose a novel framework for the responsible adoption of AI ; in financial risk management, addressing concerns related to model interpretability, data privacy, and algorithmic bias. Our findings suggest that while generative AI and LLMs offer unprecedented opportunities for enhancing risk management capabilities, they also necessitate a recalibration of regulatory approaches to ensure financial stability and consumer protection. This research contributes to the growing body of literature on AI in finance and provides actionable insights for practitioners, policymakers, and researchers in the field.</jats:p>",20,1,30,48,Generative grammar; Risk management; Financial sector; Financial services; Business; Computer science; Finance; Artificial intelligence,,,,,,http://dx.doi.org/10.47473/2020rmm0150,,10.47473/2020rmm0150,,,0,006-399-176-500-152; 008-992-937-398-51X; 009-007-053-591-22X; 010-154-758-590-433; 014-503-482-202-847; 017-303-774-495-787; 018-394-636-867-442; 018-897-718-914-710; 023-254-132-291-87X; 026-214-343-476-092; 030-438-610-495-210; 030-757-941-886-943; 036-753-503-293-443; 042-161-806-453-297; 046-822-137-307-604; 050-870-877-221-655; 053-194-698-585-347; 053-463-804-978-746; 062-857-237-682-508; 065-861-490-550-091; 066-138-018-393-145; 067-313-304-967-826; 069-560-796-007-725; 070-772-552-262-682; 070-929-058-060-455; 073-680-870-877-181; 077-400-397-795-936; 077-573-087-633-507; 077-902-632-083-884; 078-786-134-300-330; 080-401-913-719-722; 084-869-826-041-414; 098-075-989-438-612; 100-887-530-876-719; 106-684-576-397-770; 108-183-670-377-474; 111-125-638-559-975; 114-308-214-069-527; 119-418-108-964-586; 119-476-087-255-320; 121-144-201-749-73X; 132-966-239-856-014; 134-124-089-501-235; 136-572-917-742-125; 151-073-375-714-249; 154-513-179-142-971; 184-530-612-291-71X; 192-664-487-488-356,2,true,cc-by-nc-nd,gold
040-090-466-550-366,Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling,,2024,conference proceedings article,Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,Association for Computational Linguistics,,Subhendu Khatuya; Rajdeep Mukherjee; Akash Ghosh; Manjunath Hegde; Koustuv Dasgupta; Niloy Ganguly; Saptarshi Ghosh; Pawan Goyal,,,,7391,7403,Numeral system; Labelling; Computer science; Finance; Artificial intelligence; Business; Psychology; Criminology,,,,,https://arxiv.org/pdf/2405.06671 https://arxiv.org/abs/2405.06671,http://dx.doi.org/10.18653/v1/2024.naacl-long.410,,10.18653/v1/2024.naacl-long.410,,,0,,1,true,,green
040-737-775-429-943,A Convergence Study on Chatbot Persona and User Experience of Financial Service - Focused on Loan Service -,2019-09-30,2019,journal article,The Korean Society of Science & Art,20051409,Institute of Science Culture Exhibition and Design,,Seong Kyung Lee; Jae Young Yun,,37,4,257,267,World Wide Web; Financial services; Business; User experience design; Loan; Chatbot; Service (business); Convergence (relationship); Persona,,,,,http://dx.doi.org/10.17548/ksaf.2019.09.30.257,http://dx.doi.org/10.17548/ksaf.2019.09.30.257,,10.17548/ksaf.2019.09.30.257,3143613370,,0,,5,false,,
040-786-558-459-805,Traded news and large language models: evidence from China's stock market,2025-03-18,2025,preprint,,,Wiley,,Ziran Zhao,,,,,,China; Stock market; Financial economics; Business; Stock (firearms); Chinese market; Economics; Geography; Context (archaeology); Archaeology,,,,,,http://dx.doi.org/10.22541/au.174230727.76810330/v1,,10.22541/au.174230727.76810330/v1,,,0,,0,false,,
040-973-158-612-78X,Financial Inclusion with Large Language Models: Prompt Design and Evaluation for Easy Japanese Generation,2025-05-23,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Singapore,Germany,Marina Yagi; Sejun Woo; Ying Zhang; Hiroshi Takahashi,,,,255,269,Computer science; Financial inclusion; Inclusion (mineral); Finance; Software engineering; Programming language; Financial services; Business; Gender studies; Sociology,,,,,,http://dx.doi.org/10.1007/978-981-96-7071-0_17,,10.1007/978-981-96-7071-0_17,,,0,029-644-246-929-239; 033-419-680-124-445; 034-567-921-521-937; 038-258-320-042-590; 053-613-341-919-467; 054-715-810-937-697; 058-157-311-456-444; 061-348-328-017-384; 068-864-486-586-772; 078-115-534-983-444; 080-816-735-853-965; 082-178-119-516-788; 106-721-838-540-152; 126-097-796-876-596; 165-762-042-333-335,0,false,,
041-001-644-247-888,Integrate AI-based chatbots into accounting services: Enhance customer communication and financial management support,2025-04-30,2025,journal article,Journal of Computational Methods in Sciences and Engineering,14727978; 18758983,SAGE Publications,Netherlands,Zicong He; Xue Jin,"<jats:p>As businesses increasingly adopt digital tools to streamline operations, artificial intelligence (AI)-based chatbots have emerged as vital components for enhancing customer communication and supporting financial management within accounting services. This research focuses on reliable AI-powered accounting chatbots capable of handling complex financial tasks while enhancing customer communication and user satisfaction. The goal of this study is to establish AI-based chatbots in accounting services to improve financial management assistance and customer communication. This paper presents a novel Raven Roosting-tuned Adaptive Bidirectional Long Short-Term Memory (RR-ABiLSTM) model designed to classify financial queries and enhance contextual understanding in conversations to improve customer communications. The dataset encompasses both structured and unstructured data from accounting conversations, constituting a domain-specific corpus focusing on common accounting tasks. Data preprocessing included text cleaning and tokenization applied to the acquired data. Subsequently, feature extraction was performed using Word2Vec. The RR algorithm was utilized to optimize hyperparameters and feature selection, while BiLSTM ensures a deep understanding of contextual relationships in conversations, thereby enhancing accuracy and efficiency in processing financial queries. Furthermore, a dynamic training mechanism was integrated, allowing the chatbot to continually adapt to increasing consumer demands without downtime. The proposed method was implemented using Python software, and its performance was compared with traditional algorithms. The overall metrics—F1-score (87.75%), precision (89.25%), recall (86.24%), and accuracy (90%)—illustrate that the suggested model significantly improves customer engagement, reduces the workload of accountants, and enhances the overall efficiency of accounting services by providing reliable financial support.</jats:p>",25,5,4464,4478,Computer science; Chatbot; Word2vec; Customer satisfaction; Financial services; Financial accounting; Artificial intelligence; Knowledge management; Machine learning; Accounting information system; Finance; Accounting; Marketing; Business; Embedding; Economics,,,,The Scientific research projects of Tianjin Municipal Education Commission,,http://dx.doi.org/10.1177/14727978251338982,,10.1177/14727978251338982,,,0,001-625-186-322-776; 003-493-341-964-535; 007-581-025-024-301; 024-800-443-292-177; 036-226-826-025-342; 063-283-796-550-524; 066-742-026-941-317; 071-231-085-800-115; 073-286-828-690-377; 074-284-207-884-985; 077-946-279-594-919; 085-625-594-010-891; 086-439-164-597-028; 088-375-166-385-099; 112-616-290-417-943; 126-618-015-514-527; 128-677-212-112-650; 130-751-685-397-830; 140-122-221-463-888; 142-314-870-995-875; 161-851-577-998-586; 189-331-807-278-70X; 189-465-534-889-698,0,false,,
041-104-515-157-060,Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Kausik Lakkaraju; Sai Krishna Revanth Vuruma; Vishal Pallagani; Bharath Muppasani; Biplav Srivastava,"Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT and Bard, are becoming available to users that have the potential to revolutionize the quality of decision-making achieved by the public. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We asked 13 questions representing banking products in personal finance: bank account, credit card, and certificate of deposits and their inter-product interactions, and decisions related to high-value purchases, payment of bank dues, and investment advice, and in different dialects and languages (English, African American Vernacular English, and Telugu). We find that although the outputs of the chatbots are fluent and plausible, there are still critical gaps in providing accurate and reliable financial information using LLM-based chatbots.",,,,,Context (archaeology); Telugu; Certificate; Finance; Financial inclusion; Payment; Vernacular; Quality (philosophy); Business; Financial services; Transparency (behavior); Credit card; Actuarial science; Public relations; Computer science; Political science; Computer security; Paleontology; Linguistics; Philosophy; Algorithm; Epistemology; Artificial intelligence; Biology,,,,,https://arxiv.org/abs/2307.07422,http://dx.doi.org/10.48550/arxiv.2307.07422,,10.48550/arxiv.2307.07422,,,0,,0,true,cc-by-nc-nd,green
041-140-815-845-12X,Financial Product Ontology Population with Large Language Models,,2024,conference proceedings article,Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing,,Association for Computational Linguistics,,Chanatip Saetia; Jiratha Phruetthiset; Tawunrat Chalothorn; Monchai Lertsutthiwong; Supawat Taerungruang; Pakpoom Buabthong,,,,53,60,Ontology; Computer science; Product (mathematics); Population; Finance; Data science; Business; Medicine; Mathematics; Philosophy; Geometry; Environmental health; Epistemology,,,,,,http://dx.doi.org/10.18653/v1/2024.textgraphs-1.4,,10.18653/v1/2024.textgraphs-1.4,,,0,,5,false,,
041-195-296-440-89X,Sentiment-Driven Nonlinear Relationship Between Financial Market Emotion and Returns: Dynamic Adaptive Modeling Based on LLMs,2025-07-18,2025,conference proceedings article,"Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy",,ACM,,Junyi Fang; Xiaojian Liu; Minshuai Zhou,,,,92,98,Nonlinear system; Financial market; Computer science; Economics; Econometrics; Finance; Quantum mechanics; Physics,,,,,,http://dx.doi.org/10.1145/3767052.3767067,,10.1145/3767052.3767067,,,0,006-399-176-500-152; 006-667-076-920-617; 022-486-017-682-206; 030-610-744-935-770; 039-381-728-821-096; 062-900-965-825-344; 078-981-577-025-706; 086-260-434-815-196; 089-751-234-475-049; 089-844-745-151-655; 100-446-896-445-856; 102-563-404-071-620; 136-134-457-808-751; 141-982-660-885-472; 160-838-562-631-770; 195-187-424-640-373; 196-120-155-272-989,0,false,,
041-478-337-742-369,"Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with
  LLMs",2024-06-05,2024,preprint,arXiv (Cornell University),,,,Alexander Bakumenko; Kateřina Hlaváčková-Schindler; Claudia Plant; Nina C. Hubig,"Detecting anomalies in general ledger data is of utmost importance to ensure trustworthiness of financial records. Financial audits increasingly rely on machine learning (ML) algorithms to identify irregular or potentially fraudulent journal entries, each characterized by a varying number of transactions. In machine learning, heterogeneity in feature dimensions adds significant complexity to data analysis. In this paper, we introduce a novel approach to anomaly detection in financial data using Large Language Models (LLMs) embeddings. To encode non-semantic categorical data from real-world financial records, we tested 3 pre-trained general purpose sentence-transformer models. For the downstream classification task, we implemented and evaluated 5 optimized ML models including Logistic Regression, Random Forest, Gradient Boosting Machines, Support Vector Machines, and Neural Networks. Our experiments demonstrate that LLMs contribute valuable information to anomaly detection as our models outperform the baselines, in selected settings even by a large margin. The findings further underscore the effectiveness of LLMs in enhancing anomaly detection in financial journal entries, particularly by tackling feature sparsity. We discuss a promising perspective on using LLM embeddings for non-semantic data in the financial context and beyond.",,,,,Anomaly detection; Anomaly (physics); Encoding (memory); Business; Computer science; Finance; Artificial intelligence; Physics; Condensed matter physics,,,,,https://arxiv.org/abs/2406.03614,http://dx.doi.org/10.48550/arxiv.2406.03614,,10.48550/arxiv.2406.03614,,,0,,0,true,,green
041-786-642-472-320,Effect of a Cognitive Behavioral Therapy-Based AI Chatbot on Depression and Loneliness in Chinese University Students: Randomized Controlled Trial With Financial Stress Moderation.,2025-08-29,2025,journal article,JMIR mHealth and uHealth,22915222,JMIR Publications Inc.,Canada,Yahui Wang; Xuhong Li; Qiaochu Zhang; Dannii Yeung; Yihan Wu,"<AbstractText Label=""Background"" NlmCategory=""UNASSIGNED"">Mental health concerns are prevalent among university students, with financial stress further compounding these issues. While cognitive behavioral therapy (CBT) is effective for these conditions, its delivery through artificial intelligence (AI) chatbots represents a promising approach, especially in non-Western contexts.</AbstractText>;           <AbstractText Label=""Objective"" NlmCategory=""UNASSIGNED"">This study aims to investigate the efficacy of a culturally adapted, CBT-based AI chatbot for improving the well-being of Chinese university students and to examine whether financial stress moderates its effectiveness.</AbstractText>;           <AbstractText Label=""Methods"" NlmCategory=""UNASSIGNED"">In this randomized controlled trial, 100 university students (mean age 20.8, SD 2.2 years; 62/100, 62% female) were allocated to either an intervention (n=50) or a waitlist control group (n=50). The intervention group interacted with a CBT-based AI chatbot for 7 consecutive days. Depression (Center for Epidemiologic Studies Depression Scale), anxiety (Generalized Anxiety Disorder-7 scale), and loneliness (UCLA Loneliness Scale) were assessed at baseline, day 3, and day 7. Financial stress was measured using the Psychological Inventory of Financial Scarcity.</AbstractText>;           <AbstractText Label=""Results"" NlmCategory=""UNASSIGNED"">Significant group×time interactions were found for depression (F2,196=8.63; P&lt;.001; η²p=.08) and loneliness (F2,196=5.57; P=.004; η²p=.05), but not for anxiety (F2,196=1.31; P=.27; η²p=.01). Post hoc comparisons showed significant reductions in both depression (t=3.85; P&lt;.001) and loneliness (t=4.28; P&lt;.001) from baseline to postintervention in the intervention group, with corresponding effect sizes of Cohen d=0.71 (95% CI 0.30-1.12) and Cohen d=0.60 (95% CI 0.20-1.00), respectively. No significant changes were observed in the waitlist control group. Exploratory subgroup analyses revealed that participants with high financial stress demonstrated significantly greater improvements in depression (F2,52=11.56; P&lt;.001; η²p=.31) and loneliness (F2,52=11.18; P&lt;.001; η²p=.30) compared to those with low financial stress.</AbstractText>;           <AbstractText Label=""Conclusions"" NlmCategory=""UNASSIGNED"">The culturally adapted, CBT-based AI chatbot effectively reduced depression and loneliness in Chinese university students, with stronger effects among those experiencing high financial stress. These findings highlight the potential of AI-driven interventions to provide accessible mental health support, particularly for financially stressed students.</AbstractText>;           <CopyrightInformation>© Yahui Wang, Xuhong Li, Qiaochu Zhang, Dannii Yeung, Yihan Wu. Originally published in JMIR mHealth and uHealth (https://mhealth.jmir.org).</CopyrightInformation>",13,,e63806,e63806,Loneliness; Randomized controlled trial; Preprint; Moderation; Psychology; Depression (economics); Chatbot; Clinical psychology; Medicine; Psychotherapist; Computer science; World Wide Web; Social psychology; Internal medicine; Economics; Macroeconomics,artificial intelligence; chatbot; cognitive behavioral therapy; depression; financial stress; loneliness; randomized controlled trial; university students,Humans; Female; Cognitive Behavioral Therapy/methods; Male; Students/psychology; Universities/organization & administration; Loneliness/psychology; Depression/psychology; Young Adult; China; Adult; Financial Stress/psychology; Adolescent; Generative Artificial Intelligence,,,,http://dx.doi.org/10.2196/63806,40882177,10.2196/63806,,PMC12396778,0,002-495-408-459-633; 003-070-043-879-063; 010-266-448-032-643; 010-621-892-769-258; 011-024-355-244-350; 011-765-790-023-294; 012-093-832-883-127; 014-861-992-687-362; 015-651-265-481-457; 016-397-259-832-631; 017-989-096-256-486; 018-136-704-753-059; 022-026-670-183-47X; 024-824-321-754-819; 025-946-181-296-714; 028-335-216-907-164; 030-321-530-976-920; 031-927-989-576-915; 031-930-075-049-876; 032-529-872-665-984; 034-303-123-415-241; 035-012-383-484-119; 035-652-790-766-67X; 037-808-915-813-662; 038-011-363-344-105; 038-604-541-386-791; 040-956-394-515-266; 044-717-016-042-489; 044-764-012-811-449; 044-956-222-920-244; 047-045-400-977-182; 049-753-956-313-795; 050-072-637-552-168; 050-709-534-530-789; 057-276-246-999-965; 059-403-858-218-623; 061-075-088-476-045; 061-568-795-979-325; 062-163-516-288-929; 062-845-681-402-439; 063-804-941-142-094; 065-564-747-718-291; 067-623-482-804-011; 067-649-421-660-769; 067-936-016-183-526; 071-508-145-299-473; 073-284-154-553-691; 075-235-496-383-241; 075-531-300-933-034; 077-867-777-581-840; 080-795-268-641-030; 082-178-119-516-788; 085-414-272-639-806; 092-511-989-047-021; 107-183-138-020-114; 110-502-219-503-63X; 112-320-918-710-297; 114-080-988-373-073; 118-493-299-447-613; 129-216-429-598-334; 130-765-613-576-01X; 135-919-191-079-996; 145-811-821-327-826; 151-100-738-765-24X; 190-597-319-725-500,0,true,cc-by,gold
041-882-695-246-220,Serverless Architecture in LLMs: Transforming the Financial Industry's AI Landscape,2023-10-27,2023,journal article,International Journal of Science and Research (IJSR),23197064,International Journal of Science and Research,,Satish Kathiriya; Narayana Challla; Siva Karthik Devineni,,12,10,2131,2136,Architecture; Landscape architecture; Business; Geography; Engineering; Archaeology; Civil engineering,,,,,,http://dx.doi.org/10.21275/sr24302224043,,10.21275/sr24302224043,,,0,,3,true,,gold
041-894-694-778-004,A Dutch Financial Large Language Model,2024-11-20,2024,preprint,,,Qeios Ltd,,Sander Noels; Jorne De Blaere; Tijl De Bie,"<jats:p>This paper presents FinGEITje, the first Dutch financial Large Language Model (LLM) specifically designed and optimized for various financial tasks. Together with the model, we release a specialized Dutch financial instruction tuning dataset with over 140,000 samples, constructed employing an automated translation and data processing method. The open-source data construction method is provided, facilitating the creation of financial instruction datasets in different languages. To evaluate model performance, the study introduces the first Dutch financial evaluation benchmark, along with an automated evaluation method that utilizes an LLM as an independent evaluator, reducing manual intervention in performance evaluation. The experimental results highlight the superior performance of FinGEITje across five critical Dutch and English financial tasks.</jats:p>",,,,,Computer science; Benchmark (surveying); Finance; Financial modeling; Artificial intelligence; Machine learning; Business; Geodesy; Geography,,,,,https://www.qeios.com/read/DKB9SK/pdf https://doi.org/10.32388/dkb9sk http://arxiv.org/pdf/2410.12835 http://arxiv.org/abs/2410.12835,http://dx.doi.org/10.32388/dkb9sk,,10.32388/dkb9sk,,,0,004-142-287-080-115; 004-853-485-857-424; 018-866-913-622-957; 057-982-454-558-150; 073-666-676-035-633; 075-356-529-898-550; 089-312-271-916-486; 090-417-304-783-146; 092-770-866-733-36X; 139-375-630-444-960; 146-335-085-869-417; 175-144-122-847-568; 197-210-699-395-528; 197-673-280-254-524; 197-899-744-286-99X,0,true,cc-by,green
041-955-026-662-352,No More Trade-Offs. GPT and Fully Informative Privacy Policies,2023-12-27,2023,preprint,arXiv (Cornell University),,,,Przemysław Pałka; Marco Lippi; Francesca Lagioia; Rūta Liepiņa; Giovanni Sartor,"The paper reports the results of an experiment aimed at testing to what extent ChatGPT 3.5 and 4 is able to answer questions regarding privacy policies designed in the new format that we propose. In a world of human-only interpreters, there was a trade-off between comprehensiveness and comprehensibility of privacy policies, leading to the actual policies not containing enough information for users to learn anything meaningful. Having shown that GPT performs relatively well with the new format, we provide experimental evidence supporting our policy suggestion, namely that the law should require fully comprehensive privacy policies, even if this means they become less concise.",,,,,Business; Internet privacy; Computer science,,,,,https://arxiv.org/abs/2402.00013,http://dx.doi.org/10.48550/arxiv.2402.00013,,10.48550/arxiv.2402.00013,,,0,,0,true,,green
042-392-598-266-419,Securing the LLM Backend: Mitigating Emerging Threats and Ensuring Data Privacy for AI-Powered Financial Applications on AWS,2023-06-30,2023,journal article,International Journal Science and Technology,28287045; 28287223,Asosiasi Dosen Muda Indonesia,,null Gunjan Kumar,"<jats:p>Large Language Models (LLMs) have a significant impact on the financial technology (fintech) industry as the rapid growth of these automation types has simplified the processes of automation, customer interaction, and predictive analytics. Still, such breakthroughs bring a series of cybersecurity and privacy risk factors that compromise the data integrity, regulatory adherence, and institutional trust. This paper analyzes the security risk factors that are inherent to LLM backbend’s that are deployed in AI-based financial applications on Amazon Web Services (AWS). Using a hybrid-methodology combining qualitative threat modeling with quantitative analysis of AWS-specific security settings, we use the STRIDE and MITRE ATT&amp;CK framework to name key vulnerabilities, such as prompt injection, data exfiltration, model inversion, and privilege escalation. The next steps that we take are to assess the effectiveness of the AWS-native mitigation aspects like in-use encryption, granular Identity and Access Control (IAM) controls, network segregation, and continuous auditing. The findings show that the integration of the governance of LLM with the cloud security architecture of AWS significantly increases data confidentiality and contributes to the international financial regulation, in particular, GDPR and PCI-DSS. This paper introduces a security-by-design model of LLM backends, where explainability and data minimization as well as proactive monitoring are crucial. Therefore, the paper highlights the critical importance of safe AI-cloud integration in privacy, robustness, and trust protection in financial ecosystems</jats:p>",2,2,107,119,,,,,,,http://dx.doi.org/10.56127/ijst.v2i2.2334,,10.56127/ijst.v2i2.2334,,,0,,0,false,,
042-485-334-015-854,"Data-Driven Portfolio Management for Motion Pictures Industry: A New
  Data-Driven Optimization Methodology Using a Large Language Model as the
  Expert",2024-04-10,2024,preprint,arXiv (Cornell University),,,,Mohammad Alipour-Vaezi; Kwok-Leung Tsui,"Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI). To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project. Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method. Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm. In this paper, firstly, the fame score of the celebrities is determined using a large language model. Then, to tackle the asymmetric character of MPI's data, projects are classified. Furthermore, the box office prediction takes place for each class of projects. Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed.",,,,,Portfolio; Computer science; Project portfolio management; Class (philosophy); Motion (physics); Character (mathematics); Portfolio optimization; Distributor; Artificial intelligence; Industrial engineering; Operations research; Machine learning; Data mining; Project management; Systems engineering; Engineering; Mechanical engineering; Geometry; Mathematics; Financial economics; Economics,,,,,https://arxiv.org/abs/2404.07434,http://dx.doi.org/10.48550/arxiv.2404.07434,,10.48550/arxiv.2404.07434,,,0,,0,true,,green
042-503-820-737-568,Revolutionizing Financial Literacy Through Exploring Finfluencers' Intentions in Chatbot-Driven Financial Information Dissemination,2024-08-30,2024,book chapter,Advances in Business Information Systems and Analytics,23273275; 23273283,IGI Global,,Annie Issac; R. Seranmadevi,"<jats:p>Financial literacy, a cornerstone for stability and empowerment in today's economic landscape, is set to be revolutionized. Finfluencers, individuals with significant financial acumen and a robust digital platform presence, are shaping audiences' financial attitudes and behaviors. This study introduces a conceptual framework based on the decomposed theory of planned behavior (DTPB) that holds the potential to transform how we understand and implement financial education. The unique aspect of this framework is its incorporation of Finfluencers as critical influencers in adopting AI chatbots for financial education. For instance, the model can predict the likelihood of a Finfluencer endorsing an AI chatbot for financial education based on their attitudes, subjective norms, and perceived behavioral control. Despite the widespread use of DTPB across various disciplines, a research gap exists in understanding the determinants of intention and behavior concerning adopting AI chatbots for financial education.</jats:p>",,,159,188,Chatbot; Financial literacy; Finance; Influencer marketing; Cornerstone; Conceptual framework; Psychology; Business; Computer science; Marketing; Sociology; Artificial intelligence; Social science; Art; Relationship marketing; Visual arts; Marketing management,,,,,,http://dx.doi.org/10.4018/979-8-3693-4187-2.ch008,,10.4018/979-8-3693-4187-2.ch008,,,0,001-376-309-128-005; 002-191-820-163-217; 006-108-688-558-558; 009-299-623-546-736; 009-830-020-501-364; 012-401-087-885-019; 015-541-066-604-402; 017-732-196-813-661; 018-102-232-587-930; 019-643-036-015-642; 020-245-903-806-891; 021-212-751-046-613; 021-596-968-585-322; 021-916-640-763-13X; 023-742-217-441-622; 030-240-639-525-686; 030-360-159-282-183; 032-474-594-439-710; 032-671-812-443-383; 033-419-680-124-445; 034-585-106-640-787; 038-825-720-533-354; 040-267-701-927-233; 045-181-278-856-451; 046-199-740-568-444; 049-861-032-211-332; 052-880-723-028-803; 052-941-712-475-372; 056-227-746-812-039; 057-532-181-657-330; 059-907-235-328-737; 059-956-472-760-84X; 061-173-128-168-696; 062-787-826-377-092; 071-438-348-558-892; 076-253-750-044-831; 077-305-660-304-028; 078-835-256-720-171; 078-975-414-714-020; 082-320-914-108-26X; 082-871-741-929-510; 083-905-372-287-629; 084-454-103-091-229; 086-921-410-681-846; 091-420-455-496-64X; 097-113-084-617-008; 097-195-316-185-323; 097-290-406-596-391; 104-437-587-841-653; 126-503-125-562-810; 133-480-117-084-585; 134-136-252-227-167; 134-971-123-528-683; 142-113-216-423-118; 149-335-730-971-529; 156-150-811-120-542; 158-154-417-527-126; 160-520-475-811-482; 162-731-468-279-371; 162-814-188-063-529; 163-814-613-366-253; 165-093-964-397-313; 171-992-794-514-463; 174-474-049-522-710; 183-858-858-083-18X; 190-644-058-258-375; 193-050-538-820-292; 193-222-672-874-089; 194-849-557-396-072,0,false,,
042-883-615-383-809,Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible AI Engineering Approach,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Qinghua Lu; Yuxiu Luo; Liming Zhu; Mingjian Tang; Xiwei Xu; Jon Whittle,"The recent release of ChatGPT has gained huge attention and discussion worldwide, with responsible AI being a key topic of discussion. How can we ensure that AI systems, including ChatGPT, are developed and adopted in a responsible way? To tackle the responsible AI challenges, various ethical principles have been released by governments, organisations, and companies. However, those principles are very abstract and not practical enough. Further, significant efforts have been put on algorithm-level solutions that only address a narrow set of principles, such as fairness and privacy. To fill the gap, we adopt a pattern-oriented responsible AI engineering approach and build a Responsible AI Pattern Catalogue to operationalise responsible AI from a system perspective. In this article, we first summarise the major challenges in operationalising responsible AI at scale and introduce how we use the Responsible AI Pattern Catalogue to address those challenges. We then examine the risks at each stage of the chatbot development process and recommend pattern-driven mitigations to evaluate the the usefulness of the Responsible AI Pattern Catalogue in a real-world setting.",,,,,Chatbot; Process (computing); Set (abstract data type); Perspective (graphical); Computer science; Scale (ratio); Key (lock); Data science; Process management; Engineering; Artificial intelligence; Computer security; Cartography; Programming language; Geography; Operating system,,,,,https://arxiv.org/abs/2301.05517,http://dx.doi.org/10.48550/arxiv.2301.05517,,10.48550/arxiv.2301.05517,,,0,,0,true,other-oa,green
043-134-374-291-732,ECC Analyzer: Extracting Trading Signal from Earnings Conference Calls using Large Language Model for Stock Volatility Prediction,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Yupeng Cao; Zhi Chen; Qingyun Pei; Nathan Lee; K. P. Subbalakshmi; Papa Momar Ndiaye,"In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock volatility is a critical challenge that has attracted both academics and investors. While previous studies have used multimodal deep learning-based models to obtain a general view of ECCs for volatility predicting, they often fail to capture detailed, complex information. Our research introduces a novel framework: ECC Analyzer, which utilizes large language models (LLMs) to extract richer, more predictive content from ECCs to aid the model's prediction performance. We use the pre-trained large models to extract textual and audio features from ECCs and implement a hierarchical information extraction strategy to extract more fine-grained information. This strategy first extracts paragraph-level general information by summarizing the text and then extracts fine-grained focus sentences using Retrieval-Augmented Generation (RAG). These features are then fused through multimodal feature fusion to perform volatility prediction. Experimental results demonstrate that our model outperforms traditional analytical benchmarks, confirming the effectiveness of advanced LLM techniques in financial analysis.",,,257,265,Stock (firearms); Earnings; Volatility (finance); Computer science; Spectrum analyzer; Econometrics; Accounting; Economics; Telecommunications; Engineering; Mechanical engineering,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698689 https://doi.org/10.1145/3677052.3698689,http://dx.doi.org/10.1145/3677052.3698689,,10.1145/3677052.3698689,,,0,002-863-334-625-542; 017-358-296-871-204; 018-856-194-385-924; 026-793-038-161-309; 028-208-429-703-032; 036-463-789-901-057; 050-411-223-690-003; 057-040-097-643-496; 075-611-197-220-035; 078-760-134-528-110; 087-490-675-860-289; 098-742-395-052-487; 102-068-013-162-56X; 110-745-333-473-098; 132-107-884-727-463; 133-601-998-373-261; 134-640-067-415-839; 139-239-027-110-484; 148-981-708-745-760; 156-073-421-004-902; 174-898-023-191-852,9,true,other-oa,hybrid
043-231-667-938-979,Evolving Portfolio Heuristics: A Self-Correcting LLM Framework for Portfolio Optimization,2025-08-19,2025,conference proceedings article,2025 7th International Conference on Data-driven Optimization of Complex Systems (DOCS),,IEEE,,Xinan Chen; Zihao Wang; Chizhi Zhang,,,,540,545,Portfolio optimization; Portfolio; Computer science; Heuristics; Mathematical optimization; Mathematics; Economics; Financial economics; Operating system,,,,,,http://dx.doi.org/10.1109/docs67533.2025.11200704,,10.1109/docs67533.2025.11200704,,,0,003-844-959-517-798; 005-533-671-021-800; 007-987-849-861-655; 016-530-993-292-887; 044-503-041-756-113; 052-545-229-802-690; 074-799-409-440-744; 095-596-233-714-503; 112-705-069-209-103; 123-216-073-558-48X; 139-239-027-110-484; 142-637-644-580-520,0,false,,
043-279-987-486-441,Exploring Accuracy-Fairness Trade-off in Large Language Models,2024-11-20,2024,preprint,arXiv (Cornell University),,,,Qingquan Zhang; Qiqi Duan; Bo Yuan; Yuhui Shi; Jialin Liu,"Large Language Models (LLMs) have made significant strides in the field of artificial intelligence, showcasing their ability to interact with humans and influence human cognition through information dissemination. However, recent studies have brought to light instances of bias inherent within these LLMs, presenting a critical issue that demands attention. In our research, we delve deeper into the intricate challenge of harmonising accuracy and fairness in the enhancement of LLMs. While improving accuracy can indeed enhance overall LLM performance, it often occurs at the expense of fairness. Overemphasising optimisation of one metric invariably leads to a significant degradation of the other. This underscores the necessity of taking into account multiple considerations during the design and optimisation phases of LLMs. Therefore, we advocate for reformulating the LLM training process as a multi-objective learning task. Our investigation reveals that multi-objective evolutionary learning (MOEL) methodologies offer promising avenues for tackling this challenge. Our MOEL framework enables the simultaneous optimisation of both accuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs. In summary, our study sheds valuable lights on the delicate equilibrium between accuracy and fairness within LLMs, which is increasingly significant for their real-world applications. By harnessing MOEL, we present a promising pathway towards fairer and more efficacious AI technologies.",,,,,Computer science; Economics; Econometrics,,,,,https://arxiv.org/abs/2411.14500,http://dx.doi.org/10.48550/arxiv.2411.14500,,10.48550/arxiv.2411.14500,,,0,,0,true,,green
043-639-956-143-805,Using Large Language Models for the Assessment of Sustainable Forest Investment Projects,,2025,conference proceedings article,Proceedings of the Annual Hawaii International Conference on System Sciences,25726862,Hawaii International Conference on System Sciences,,Maria Letizia Jannibelli; Jiayu Luo; Kilian Sprenkamp; Liudmila Zavolokina,,,,,,Investment (military); Computer science; Political science; Law; Politics,,,,,,http://dx.doi.org/10.24251/hicss.2025.562,,10.24251/hicss.2025.562,,,0,,0,false,,
043-913-171-248-905,Integrating Large Language Models with Machine Learning for Explainable Banking Security and Financial Risk Assessment,2025-11-24,2025,journal article,International Interdisciplinary Business Economics Advancement Journal,23759607,European International Journal of Multidisciplinary Research and Management Studies,,Sakib Salam Jamee; Md Arif; Md Mohibur Rahman; I K M SAAMEEN YASSAR; Md Arif Hossain,"<jats:p>This study proposes and empirically evaluates a hybrid banking security framework that integrates traditional machine learning models with a large language model (LLM) for enhanced risk assessment and decision support. Using two open-source datasets from the UCI Machine Learning Repository—the Default of Credit Card Clients and Bank Marketing datasets—we construct engineered behavioral and temporal features, including payment–to–bill ratios, bill trend slopes, and volatility measures, to capture client financial and interaction patterns. Gradient Boosted Trees, Random Forest, and a feedforward Neural Network are trained on these structured features and evaluated using accuracy, precision, recall, F1-score, and area under the ROC curve (AUC-ROC). The Gradient Boosted Trees model achieves the best performance, with an accuracy of 0.87, F1-score of 0.79, and AUC-ROC of 0.91, outperforming both Random Forest and Neural Network baselines. To incorporate interpretability and contextual reasoning, we transform structured records into narrative client profiles and use a pre-trained LLM to generate risk classifications, textual explanations, and security recommendations. Alignment analysis shows that LLM-generated risk labels agree with ground-truth outcomes in approximately 81% of cases, indicating that the LLM can serve as a credible auxiliary assessor. The combined system provides both high-quality quantitative risk scores and human-readable narratives, thereby improving transparency, supporting regulatory and compliance needs, and enabling more targeted security interventions. Overall, the results demonstrate that LLM-augmented machine learning can substantially strengthen banking security systems by uniting strong predictive performance with operationally useful interpretability.</jats:p>",6,11,8,17,,,,,,,http://dx.doi.org/10.55640/business/volume06issue11-02,,10.55640/business/volume06issue11-02,,,0,,0,false,,
044-260-374-772-441,AI in Investment Analysis: LLMs for Equity Stock Ratings,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Kassiani Papasotiriou; Srijan Sood; Shayleen Reynolds; Tucker Balch,"Investment Analysis is a cornerstone of the Financial Services industry. The rapid integration of advanced machine learning techniques, particularly Large Language Models (LLMs), offers opportunities to enhance the equity rating process. This paper explores the application of LLMs to generate multi-horizon stock ratings by ingesting diverse datasets. Traditional stock rating methods rely heavily on the expertise of financial analysts, and face several challenges such as data overload, inconsistencies in filings, and delayed reactions to market events. Our study addresses these issues by leveraging LLMs to improve the accuracy and consistency of stock ratings. Additionally, we assess the efficacy of using different data modalities with LLMs for the financial domain. We utilize varied datasets comprising fundamental financial, market, and news data from January 2022 to June 2024, along with GPT-4-32k (v0613) (with a training cutoff in Sep. 2021 to prevent information leakage). Our results show that our benchmark method outperforms traditional stock rating methods when assessed by forward returns, specially when incorporating financial fundamentals. While integrating news data improves short-term performance, substituting detailed news summaries with sentiment scores reduces token use without loss of performance. In many cases, omitting news data entirely enhances performance by reducing bias. Our research shows that LLMs can be leveraged to effectively utilize large amounts of multimodal financial data, as showcased by their effectiveness at the stock rating prediction task. Our work provides a reproducible and efficient framework for generating accurate stock ratings, serving as a cost-effective alternative to traditional methods. Future work will extend to longer timeframes, incorporate diverse data, and utilize newer models for enhanced insights.",,,419,427,Stock market; Computer science; Stock (firearms); Equity (law); Finance; Financial market; Business; Engineering; Political science; Law; Mechanical engineering; Paleontology; Horse; Biology,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698694 https://doi.org/10.1145/3677052.3698694 http://arxiv.org/pdf/2411.00856 http://arxiv.org/abs/2411.00856,http://dx.doi.org/10.1145/3677052.3698694,,10.1145/3677052.3698694,,,0,000-573-862-172-278; 017-676-196-726-599; 032-098-673-230-943; 034-111-475-208-944; 040-359-781-194-479; 051-008-067-001-638; 053-522-074-816-530; 058-121-257-292-666; 062-166-998-682-809; 071-786-953-602-215; 110-273-402-544-038; 135-317-697-464-834; 181-603-428-251-377; 190-050-627-029-962; 192-806-908-794-847,4,true,,bronze
044-456-506-571-452,Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Boyu Zhang; Hongyang Yang; Xiao-Yang Liu,"Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital.",,,,,Sentiment analysis; Computer science; Language model; Natural language processing; Artificial intelligence; Finance; Economics,,,,,https://arxiv.org/pdf/2306.12659 https://arxiv.org/abs/2306.12659,http://dx.doi.org/10.2139/ssrn.4489831,,10.2139/ssrn.4489831,,,0,006-399-176-500-152,54,true,,green
044-772-977-733-838,Developing Evaluation Metrics for Cross-lingual LLM-based Detection of Subtle Sentiment Manipulation in Online Financial Content,2023-09-11,2023,journal article,Journal of Advanced Computing Systems,30663962,Scientific Publication Center,,Jiayu Liang; Chenyao Zhu; Qichang Zheng; Tianjun Mo,,3,9,24,38,Computer science; Content (measure theory); Artificial intelligence; Natural language processing; Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.69987/jacs.2023.30903,,10.69987/jacs.2023.30903,,,0,,0,true,cc-by,gold
045-359-525-578-678,Integrating Social Media Sentiment Analysis with Gpt-4 into the Black-Litterman Model for Cryptocurrency Portfolio Management,2024-01-01,2024,preprint,,,Elsevier BV,,Jihun Yu; Huisu Jang,,,,,,Black–Litterman model; Cryptocurrency; Portfolio; Social media; Computer science; Sentiment analysis; Portfolio optimization; Economics; Artificial intelligence; Financial economics; World Wide Web; Replicating portfolio,,,,,,http://dx.doi.org/10.2139/ssrn.4894905,,10.2139/ssrn.4894905,,,0,010-798-602-218-532; 016-712-198-508-113; 020-297-217-510-841; 021-201-453-050-215; 030-311-314-578-937; 055-585-079-580-089; 062-027-974-769-598; 071-255-758-310-724; 090-578-562-051-13X; 092-453-382-594-560; 111-343-234-678-99X; 131-886-771-092-106; 138-541-406-733-869; 145-931-944-659-640; 163-441-237-134-715,0,false,,
045-928-961-496-027,Gender-based conversational interface preferences in live chat systems for financial services,2022-08-23,2022,journal article,Journal of Financial Services Marketing,13630539; 14791846,Springer Science and Business Media LLC,United Kingdom,Muhanad Shakir Manshad; Daniel C. Brannon,,28,4,822,834,Chatbot; Continuance; USable; Usability; Interface (matter); Financial services; Preference; Computer science; Human–computer interaction; Demographics; Marketing; Psychology; World Wide Web; Business; Social psychology; Finance; Economics; Sociology; Bubble; Maximum bubble pressure method; Parallel computing; Demography; Microeconomics,,,,,,http://dx.doi.org/10.1057/s41264-022-00175-8,,10.1057/s41264-022-00175-8,,,0,000-993-283-623-848; 002-462-717-281-874; 003-630-555-074-317; 005-348-289-357-341; 005-566-427-089-775; 006-129-272-987-925; 007-992-203-081-808; 009-395-554-394-627; 010-232-879-372-214; 010-948-609-676-705; 012-224-166-476-219; 014-868-950-006-380; 017-785-202-097-833; 021-923-402-957-444; 022-240-721-276-279; 024-395-273-487-863; 031-420-886-532-773; 032-049-177-406-929; 033-385-565-167-009; 033-730-097-498-02X; 033-826-575-023-317; 034-500-738-683-157; 036-070-607-027-075; 036-922-470-611-955; 038-590-364-526-807; 038-925-283-945-155; 039-208-171-682-658; 042-506-256-335-691; 046-310-813-899-395; 047-062-690-685-683; 047-997-496-303-956; 050-203-878-944-004; 053-024-399-928-715; 053-506-159-592-564; 055-383-336-818-17X; 058-180-782-158-887; 058-816-912-109-92X; 063-823-876-329-322; 066-148-113-710-080; 068-255-013-275-268; 076-988-878-924-325; 081-448-150-090-51X; 088-330-996-413-096; 102-563-563-843-86X; 102-871-083-270-569; 115-921-906-361-758; 116-582-512-579-44X; 118-268-248-308-954; 118-419-743-827-01X; 120-245-812-968-428; 121-757-187-422-109; 122-437-520-154-237; 129-129-125-953-602; 159-549-280-195-935; 165-649-942-369-611,5,false,,
046-079-202-962-034,Using GPT-4 for Financial Advice,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Christian Fieberg; Lars Hornuf; David Streich,"We show that the recently released text-based artificial intelligence tool GPT-4 can provide suitable financial advice. The tool suggests specific investment portfolios that reflect an investor's individual circumstances such as risk tolerance, risk capacity, and sustainability preference. Notably, while the suggested portfolios display home bias and are rather insensitive to the investment horizon, historical risk-adjusted performance is on par with a professionally managed benchmark portfolio. Given the current inability of GPT-4 to provide full-service financial advice, it may be used by financial advisors as a back-office tool for portfolio recommendation.",,,,,Advice (programming); Portfolio; Finance; Preference; Financial services; Financial plan; Investment (military); Business; Actuarial science; Financial risk; Economics; Computer science; Microeconomics; Programming language; Politics; Political science; Law,,,,,https://www.econstor.eu/bitstream/10419/279279/1/cesifo1_wp10529.pdf https://hdl.handle.net/10419/279279,http://dx.doi.org/10.2139/ssrn.4488891,,10.2139/ssrn.4488891,,,0,027-163-358-030-949; 037-564-355-939-268; 046-019-020-887-681; 059-804-970-444-281; 074-403-071-397-078; 078-934-110-822-38X; 106-520-923-508-340; 106-910-594-244-13X; 191-424-188-274-666,4,true,,green
046-188-426-773-681,FMDLlama: Financial Misinformation Detection Based on Large Language Models,2025-05-08,2025,conference proceedings article,Companion Proceedings of the ACM on Web Conference 2025,,ACM,,Zhiwei Liu; Xin Zhang; Kailai Yang; Qianqian Xie; Jimin Huang; Sophia Ananiadou,,,,1153,1157,Misinformation; Computer science; Finance; Computer security; Business,,,,The University of Manchester; New Energy and Industrial Technology Development Organization; Manchester-Melbourne-Toronto Research Funding; The National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGeneration EU Program,,http://dx.doi.org/10.1145/3701716.3715599,,10.1145/3701716.3715599,,,0,024-813-346-139-886; 048-018-587-431-771; 162-815-657-649-464; 190-760-609-208-293; 191-957-237-138-372,1,false,,
046-575-030-273-998,Research on Anti-Fraud Recognition Method for Financial Transactions Based on Large Language Model RAG Technology,,2025,journal article,Operations Research and Fuzziology,21631476; 21631530,Hans Publishers,,江 陶,,15,1,386,396,Business; Computer science; Computer security; Natural language processing,,,,,,http://dx.doi.org/10.12677/orf.2025.151036,,10.12677/orf.2025.151036,,,0,154-271-123-602-132,0,false,,
046-637-337-319-349,The Study on the Factors Influencing on the Behavioral Intention of Chatbot Service for the Financial Sector : Focusing on the UTAUT Model,2019-01-31,2019,journal article,Journal of Digital Contents Society,15982009; 2287738x,Digital Contents Society,,null 김진우; Hye In Jo; Lee Bong Gyou,"인공지능 기술의 발달과 함께 다양한 산업에서 인공지능을 결합한 서비스 도입을 시도하고 있으며, 금융권에서는 챗봇을 이용하여 새로운 혁신을 시도하고 있다. 본 연구에서는 UTAUT 모델을 사용하여 성과기대, 노력기대, 사회적 영향, 정보의 신뢰성, 보안의 신뢰성이 챗봇 서비스의 사용의도에 유의미한 영향을 미치는 것을 확인하였다. 본 연구결과는 추후 금융권 챗봇 서비스의 성공적인 도입에 관한 연구에 참고 자료가 될 수 있을 것으로 기대된다.",20,1,41,50,Psychology; Chatbot; Service (business); Financial sector; Knowledge management,,,,,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002433766 http://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07610111,http://dx.doi.org/10.9728/dcs.2019.20.1.41,,10.9728/dcs.2019.20.1.41,2981622154,,0,001-789-245-395-520; 010-867-816-135-157; 025-202-330-167-763; 034-594-816-863-12X; 050-385-505-448-649; 051-517-331-813-156; 064-433-713-186-809; 070-217-337-440-113; 080-468-152-445-793; 105-889-297-410-314; 114-348-343-803-085; 132-991-518-582-348; 134-692-988-441-892; 143-875-130-007-110; 152-896-704-064-472,28,false,,
046-860-759-440-722,"FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for
  Trading Agents",2025-02-11,2025,preprint,arXiv (Cornell University),,,,Mostapha Benhenda,"This paper presents a novel risk-sensitive trading agent combining reinforcement learning and large language models (LLMs). We extend the Conditional Value-at-Risk Proximal Policy Optimization (CPPO) algorithm, by adding risk assessment and trading recommendation signals generated by a LLM from financial news. Our approach is backtested on the Nasdaq-100 index benchmark, using financial news data from the FNSPID dataset and the DeepSeek V3, Qwen 2.5 and Llama 3.3 language models. The code, data, and trading agents are available at: https://github.com/benstaf/FinRL_DeepSeek",,,,,Reinforcement learning; Reinforcement; Computer science; Business; Artificial intelligence; Psychology; Social psychology,,,,,https://arxiv.org/abs/2502.07393,http://dx.doi.org/10.48550/arxiv.2502.07393,,10.48550/arxiv.2502.07393,,,0,,0,true,,green
047-272-621-306-534,AUTOMATION OF ACCOUNTING FOR INCOME AND EXPENSES IN TRADE BASED ON A CHATBOT,2024-02-11,2024,journal article,Экономика и предпринимательство,19992300,INTERECONOM Publishing,,А.Б. МАЛИНА; М.Е. ТУРАЕВА; М.В. КАГИРОВА,"<jats:p>Представленная работа отражает результаты исследования, направленного на теоретическое обоснование необходимости автоматизации процессов бухгалтерского учета в условиях рынка, разработку веб-приложения (чат-бота) для автоматизации операций, связанных с учетом доходов и расходов в торговом предприятии. Основными методами исследования явились анализ и синтез. Для разработки информационной системы были использованы следующие инструменты: язык программирования Python, а также встроенная библиотека telebot (pyTelegramBotAPI), база данных SQLite3. SQLite3, библиотека Pandas. Результатом работы является система, позволяющая пользователям получать актуальную информацию о финансовом состоянии предприятия в режиме реального времени, что сокращает временные затраты на принятие оптимального управленческого решения; автоматически обрабатывать данные, что снижает вероятность ошибок и повышает точность учета.</jats:p>;                                                                                             <jats:p>The presented work reflects the results of a study aimed at theoretically substantiating the need to automate accounting processes in market conditions, developing a web application (chatbot) to automate operations related to accounting for income and expenses in a trading enterprise. The main research methods were analysis and synthesis. To develop the information system, the following tools were used: the Python programming language, as well as the built-in telebot library (pyTelegramBotAPI), and the SQLite3 database. SQLite3, Pandas library. The result of the work is a system that allows users to receive up-to-date information about the financial condition of the enterprise in real time, which reduces the time spent on making the optimal management decision; automatically process data, which reduces the likelihood of errors and increases accounting accuracy.</jats:p>",,1(162),1017,1025,Chatbot; Automation; Business; Accounting; Computer science; World Wide Web; Engineering; Mechanical engineering,,,,,,http://dx.doi.org/10.34925/eip.2024.162.1.195,,10.34925/eip.2024.162.1.195,,,0,,0,false,,
047-355-320-143-501,Analysis the Effects of Voice-based AI Financial Service Chatbot's Cuteness in Speech Style and Gender Congruence on Customers' Responses in a Service Failure Situation,2023-11-30,2023,journal article,Journal of the Korea Academia-Industrial cooperation Society,19754701; 22884688,The Korea Academia-Industrial Cooperation Society,,Sungjoon Lee,"본 연구는 서비스 실패 상황 속에서 음성 기반 AI 금융 챗봇의 말투에 있어서의 귀여움 수준과 챗봇과의 젠더 일치성 여부가 고객 반응으로서 허용 영역에 어떤 영향을 가지는 지를 실증적으로 검증해보고자 하였다. 또한 이런 영향 과정에서 성과기대, 부드러움 및 호기심이 매개 효과를 보이는 지 역시 검증하였다. 128명의 AI 챗봇 서비스를 인지하는 20대를 대상으로 2(말투 귀여움 수준 높음/낮음) X 2(젠더 일치/젠더 불일치) 집단간 설계(Between-Subjects Design) 방식의 온라인 실험 방법을 통해 자료를 수집하였고, 수집된 자료들은 SPSS Process Macro를 통하여 분석하였다. 연구 결과, 서비스 실패 상황 속에서 음성 기반 AI 금융 챗봇의 말투의 귀여움 수준이 높아질 경우 고객의 허용 영역은 줄어드는 것으로 나타났다. 또한 챗봇과의 젠더 일치성 여부는 허용 정도 영향 과정에서 말투의 귀여움과 유의미한 상호작용 효과를 보이는 것으로 나타났다. 성과기대, 부드러움 및 호기심 변인들은 모두 매개 효과를 보이지는 않는 것으로 나타났다. 이러한 결과들이 AI 챗봇 서비스 기획에 있어 지니는 실무적 시사점들 또한 논의된다.",24,11,809,824,Chatbot; Congruence (geometry); Service (business); Computer science; Psychology; Style (visual arts); Customer service; Speech recognition; Natural language processing; Social psychology; Business; Marketing; Archaeology; History,,,,,,http://dx.doi.org/10.5762/kais.2023.24.11.809,,10.5762/kais.2023.24.11.809,,,0,,0,true,,gold
047-881-605-359-605,IHSI - Ekybot: Framework Proposal for Chatbot in Financial Enterprises,2020-01-22,2020,book,Advances in Intelligent Systems and Computing,21945357; 21945365,Springer International Publishing,,Maritzol Tenemaza; Sergio Luján-Mora; Angélica de Antonio; Jaime Ramírez; Omar Zarabia,"The high proportion of customers of financial enterprises such as banking or insurance have forced to think in new forms to attend the customers. Chatbots are modern and useful tools with which customers are attended using natural language. In this work, we present a financial chatbot called Ekybot, which uses different channels as Skype and Facebook. Ekybot assist customers and enhanced services of the financial enterprises. The most interesting finding of this work is the framework that was developed on a variety of cognitive services.",,,254,259,Finance; Variety (cybernetics); Work (electrical); Dialog system; Natural language; Chatbot; Cognitive services; Computer science,,,,,https://rd.springer.com/chapter/10.1007/978-3-030-39512-4_40 https://dblp.uni-trier.de/db/conf/ihsi/ihsi2020.html#TenemazaLARZ20 https://link.springer.com/chapter/10.1007/978-3-030-39512-4_40,http://dx.doi.org/10.1007/978-3-030-39512-4_40,,10.1007/978-3-030-39512-4_40,3003126346,,0,008-775-038-975-001; 011-303-731-710-610; 073-492-111-847-784; 086-082-316-890-399; 110-523-122-951-914,4,false,,
048-246-407-731-123,Advancing Quantitative Trading Strategies Using Fine-Tuned Open-Source Large Language Models: A Hybrid Approach with Numerical and Textual Data Integration Using RAG and LoRA Techniques,2025-08-22,2025,book chapter,Blockchain Technologies,26618338; 26618346,Springer Nature Singapore,,Seth H. Huang; Jimin Kim; Ka Lok Kellogg Wong,,,,43,57,Computer science; Open source; Programming language; Software,,,,,,http://dx.doi.org/10.1007/978-981-96-5833-6_3,,10.1007/978-981-96-5833-6_3,,,0,,0,false,,
048-853-381-299-014,LLM - Based Code Generation for Querying Temporal Tabular Financial Data,2024-10-22,2024,conference proceedings article,2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr),,IEEE,,Mohamed Lashuel; Gulrukh Kurdistan; Aaron Green; John S. Erickson; Oshani Seneviratne; Kristin P. Bennett,,,,1,8,Computer science; Code (set theory); Code generation; Temporal database; Programming language; Data mining; Computer security; Key (lock); Set (abstract data type),,,,,,http://dx.doi.org/10.1109/cifer62890.2024.10772804,,10.1109/cifer62890.2024.10772804,,,0,,0,false,,
049-179-847-385-091,Chatbot for Trading Cryptocurrency,2023-05-25,2023,conference proceedings article,"2023 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",,IEEE,,K. Jaspin; Sanjeev S; Shyam S,"Many chatbots have been created, offering a variety of services via various channels. A brand-new conversational agent in the quickly evolving realm of technology is a chatbot. Chatbots are getting more and more common because of their intelligence and machine learning. A chatbot is an extension of human interaction technologies like phone calls and social media. Similar to digital or virtual currency, cryptocurrency is a fresh extension created to function as a means of exchange. Investors and other interested parties are keen to learn more about this new form of currency's capabilities in the current world of digital exchange. A chatbot is one method that might be used to swiftly and automatically retrieve information. A chatbot assists the users by providing services to themselves, and they always favour text-based support. Cryptocurrency is a decentralized, blockchain-based, encrypted form of digital money. Blockchain refers to a digital ledger that is accessible only to authorized users in the context of cryptocurrencies. This ledger records transactions involving a range of resources, including money, property, and even intangible assets. The chatbot is deployed for the purpose that it serves the customers at any time. Humans can work only for a limited time whereas, the chatbot is available 24x7 for customer support and assistance. The chatbot helps users in trading cryptocurrencies without any human involvement. It can handle all the queries related to cryptocurrencies that are raised by the customer. And the chatbot can also converse in multiple languages. This helps a lot of customers to use the chatbot in an easy and effective manner. Customers feel so easy to work with the chatbot that it responds to them in the language that they choose to converse. The chatbot stores all the information related to the customer like details of the customer, time stayed on the chatbot, the way customer interacts with the chatbot, etc. The chatbot can predict how likely the user of the chatbot can become a customer to the company of the chatbot. The agents of the company can view the live visitors using the chatbot and their conversation that they have with the chatbot.",,,1,9,Chatbot; Cryptocurrency; Computer science; World Wide Web; Computer security; Currency; Internet privacy; Monetary economics; Economics,,,,,,http://dx.doi.org/10.1109/accai58221.2023.10200405,,10.1109/accai58221.2023.10200405,,,0,003-259-817-009-758; 010-688-076-061-127; 010-912-687-394-717; 012-589-016-361-768; 025-122-626-487-678; 028-861-729-480-303; 029-758-860-180-93X; 055-853-659-832-519; 067-667-949-163-225; 088-330-996-413-096; 088-701-836-552-724; 116-319-377-026-261; 130-654-518-373-546; 136-544-581-381-777,0,false,,
049-490-676-370-435,Enhancing Personalized Financial Advisory Application with Generative AI and Chatbot: A Usability Study,2024-11-14,2024,conference proceedings article,2024 8th International Conference on Information Technology (InCIT),,IEEE,,Niorn Suchonwanich; Siranee Nuchitprasitchai; Kanchana Viriyapant,,,,97,102,Chatbot; Usability; Computer science; World Wide Web; Human–computer interaction,,,,,,http://dx.doi.org/10.1109/incit63192.2024.10810647,,10.1109/incit63192.2024.10810647,,,0,011-140-165-421-537; 020-716-235-457-427; 038-234-947-349-434; 041-492-606-932-699; 082-843-724-191-00X; 121-026-764-828-645; 127-825-566-215-230; 143-180-962-085-217; 189-453-271-962-065,3,false,,
049-815-188-312-298,AI-Powered Virtual Assistants in the Realms of Banking and Financial Services,2021-10-13,2021,book chapter,Virtual Assistant,,IntechOpen,,Margherita Mori,"<jats:p>This chapter aims at providing a framework for analysis on evolutionary trends in finance that have to do with technological progress and especially with artificial intelligence (AI) applications. The starting point can be identified with a survey on how they have modified the business areas involving banking and financial services and on what can be expected – in terms of future strategic shifts and behavioral changes – on both the supply and the demand sides. The next step revolves around a wider and deeper investigation on the role that virtual assistants have started to – and are likely to further – play in the areas under scrutiny: special attention is requested upon the provision of enhanced customer service support, including conversational AI and sound branding; implications encompass developments that are on the cards, based upon digitalization as a must – not just an option – as shown by the Covid-19 pandemic. Conclusions allow to emphasize the significance, advancing features and value of this conceptual paper, as it leads to sort out best practices and success stories that are worth disseminating and replicating to benefit not only individuals and enterprises having direct interest in them, but society as a whole.</jats:p>",,,,,Finance; Financial services; Business,AI applications; banking; conversational AI; digitalization; financial services; sound branding; virtual assistant,,,,https://www.intechopen.com/chapters/74875,http://dx.doi.org/10.5772/intechopen.95813,,10.5772/intechopen.95813,3121815051,,0,017-215-873-199-430; 053-404-962-550-382; 070-481-937-751-946; 085-782-809-503-810; 128-706-486-107-162; 140-926-755-181-600; 150-273-893-937-194,9,true,cc-by,hybrid
049-861-081-507-930,Leveraging Large Language Models to Democratize Access to Costly Financial Datasets for Academic Research,2024-01-01,2024,preprint,,,Elsevier BV,,Julian Wang; Victor Xiaoqi Wang,,,,,,Computer science; Finance; Business,,,,,,http://dx.doi.org/10.2139/ssrn.5012660,,10.2139/ssrn.5012660,,,0,001-269-356-226-165; 005-989-438-190-750; 015-455-688-451-77X; 015-750-020-517-224; 023-681-536-362-765; 030-888-724-703-143; 036-412-516-933-993; 044-391-144-370-017; 047-563-298-370-762; 052-760-440-140-065; 055-737-638-512-69X; 070-298-405-899-602; 083-131-854-622-323; 085-737-918-019-697; 086-489-415-563-371; 089-469-631-009-958; 098-128-525-008-899; 099-651-796-563-74X; 101-693-235-006-615; 107-002-716-250-830; 123-568-398-968-419; 127-474-892-930-633; 153-920-177-733-534; 157-787-315-927-958; 158-382-354-589-147; 158-921-787-039-079; 164-551-795-839-887; 171-971-283-545-409; 176-983-397-617-220; 191-424-188-274-666; 196-385-133-923-289,0,false,,
050-137-187-827-957,Integrating Social Media Sentiment Analysis with Gpt-4 into the Black-Litterman Model for Cryptocurrency Portfolio Management,2024-01-01,2024,preprint,,,Elsevier BV,,Jihun Yu; Huisu Jang,,,,,,Cryptocurrency; Black–Litterman model; Portfolio; Social media; Computer science; Portfolio optimization; Economics; Financial economics; World Wide Web; Replicating portfolio,,,,,,http://dx.doi.org/10.2139/ssrn.5055921,,10.2139/ssrn.5055921,,,0,,0,false,,
050-468-009-214-601,Opaque by Design? Leveraging Large Language Models to Analyze Financial Reports Readability Under Intellectual Property Litigation,,2025,preprint,,,Elsevier BV,,Lev Muchnik; Yevgeny Mugerman; Boaz Noiman; Orly Sade,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5839543,,10.2139/ssrn.5839543,,,0,,0,false,,
050-469-823-936-89X,Using Chatbot in Trading System for Small and Medium Enterprise (SMEs) by Convolution Neural Network Technique,2019-06-22,2019,conference proceedings article,Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference,,ACM,,Sathit Prasomphan,"In several businesses, chatbots are used for providing customer information, answering questions, helping customer reservations, virtual assistants; serve as call centers to serve ten million customers automatically. This research presents a method for developing chatbots to serve their users. A deep learning based conversational artificial intelligence technique was used as tools for learning conversation between machine and customer. The convolution neural network technique by using Tensorflow training was used to improve the accuracy of these chatbots. Moreover, in this research, we developed a system for online sales assistant application system via Facebook Page.",,,93,98,Human–computer interaction; Deep learning; Artificial intelligence; Conversation; Word embedding; Chatbot; Computer science; Small and medium-sized enterprises; Convolutional neural network,,,,,https://dl.acm.org/doi/pdf/10.1145/3341069.3341092,http://dx.doi.org/10.1145/3341069.3341092,,10.1145/3341069.3341092,2968459831,,0,021-092-397-907-180; 077-899-984-222-177,3,false,,
050-494-636-226-619,"Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment
  Strategies Through the Lens of the Five-Factor Model",2024-10-27,2024,preprint,arXiv (Cornell University),,,,Harris Borman; Anna Leontjeva; Luiz Pizzato; Max Kun Jiang; Dan Jermyn,"Large Language Models (LLMs) have demonstrated the ability to adopt a personality and behave in a human-like manner. There is a large body of research that investigates the behavioural impacts of personality in less obvious areas such as investment attitudes or creative decision making. In this study, we investigated whether an LLM persona with a specific Big Five personality profile would perform an investment task similarly to a human with the same personality traits. We used a simulated investment task to determine if these results could be generalised into actual behaviours. In this simulated environment, our results show these personas produced meaningful behavioural differences in all assessed categories, with these behaviours generally being consistent with expectations derived from human research. We found that LLMs are able to generalise traits into expected behaviours in three areas: learning style, impulsivity and risk appetite while environmental attitudes could not be accurately represented. In addition, we showed that LLMs produce behaviour that is more reflective of human behaviour in a simulation environment compared to a survey environment.",,,,,Dream; Persona; Investment (military); Lens (geology); Factor (programming language); Economics; Art; Computer science; Psychology; Political science; Humanities; Physics; Optics; Law; Neuroscience; Politics; Programming language,,,,,https://arxiv.org/abs/2411.05801,http://dx.doi.org/10.48550/arxiv.2411.05801,,10.48550/arxiv.2411.05801,,,0,,0,true,,green
050-536-563-095-269,Financial Sentiment Analysis on News and Reports Using Large Language Models and FinBERT,2024-07-26,2024,conference proceedings article,"2024 IEEE 6th International Conference on Power, Intelligent Computing and Systems (ICPICS)",,IEEE,,Yanxin Shen; Pulin Kirin Zhang,,,,717,721,Sentiment analysis; Computer science; Natural language processing; Finance; Data science; Business,,,,,,http://dx.doi.org/10.1109/icpics62053.2024.10796670,,10.1109/icpics62053.2024.10796670,,,0,026-829-003-233-984; 034-270-957-372-34X; 037-853-357-392-375; 039-110-388-312-10X; 047-729-043-873-226; 056-226-337-623-814; 083-314-867-143-630; 083-449-940-348-930; 084-225-727-447-001; 085-260-413-523-411; 124-886-932-205-623; 159-315-225-396-281; 192-743-385-236-655,23,false,,
051-076-707-108-885,"RiskLabs: Predicting Financial Risk Using Large Language Model Based on
  Multi-Sources Data",2024-04-10,2024,preprint,arXiv (Cornell University),,,,Yupeng Cao; Zhi Chen; Prashant Kumar; Qingyun Pei; Yangyang Yu; Haohang Li; Fabrizio Dimino; Lorenzo Ausiello; K. P. Subbalakshmi; Papa Momar Ndiaye,"The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering (Q$\&$A), and stock movement prediction (binary classification), with a notable gap in the application of LLMs for financial risk prediction. Addressing this gap, in this paper, we introduce \textbf{RiskLabs}, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates. Our approach involves a multi-stage process: initially extracting and analyzing ECC data using LLMs, followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes. Using multimodal fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction. Empirical experiment results demonstrate RiskLab's effectiveness in forecasting both volatility and variance in financial markets. Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of LLMs in this context. Our findings not only contribute to the AI in finance application but also open new avenues for applying LLMs in financial risk assessment.",,,,,Finance; Financial market; Financial risk; Context (archaeology); Computer science; Earnings; Risk management; Artificial intelligence; Machine learning; Business; Geography; Archaeology,,,,,https://arxiv.org/abs/2404.07452,http://dx.doi.org/10.48550/arxiv.2404.07452,,10.48550/arxiv.2404.07452,,,0,,1,true,,green
051-148-106-444-496,BONIK: A Blockchain Empowered Chatbot for Financial Transactions,2020-11-17,2020,preprint,arXiv: Cryptography and Security,,,,Saiful Islam Bhuiyan; Abdur Razzak; Sadek Ferdous; Mohammad Jabed Morshed Chowdhury; Mohammad A. Hoque; Sasu Tarkoma,"A Chatbot is a popular platform to enable users to interact with a software or website to gather information or execute actions in an automated fashion. In recent years, chatbots are being used for executing financial transactions, however, there are a number of security issues, such as secure authentication, data integrity, system availability and transparency, that must be carefully handled for their wide-scale adoption. Recently, the blockchain technology, with a number of security advantages, has emerged as one of the foundational technologies with the potential to disrupt a number of application domains, particularly in the financial sector. In this paper, we forward the idea of integrating a chatbot with blockchain technology in the view to improve the security issues in financial chatbots. More specifically, we present BONIK, a blockchain empowered chatbot for financial transactions, and discuss its architecture and design choices. Furthermore, we explore the developed Proof-of-Concept (PoC), evaluate its performance, analyse how different security and privacy issues are mitigated using BONIK.",,,,,Financial transaction; Software; Chatbot; Blockchain; Transparency (behavior); Financial sector; Data integrity; Computer security; Computer science,,,,,https://arxiv.org/abs/2011.08846 https://arxiv.org/pdf/2011.08846.pdf,https://arxiv.org/abs/2011.08846,,,3092292304,,0,029-059-641-609-668; 056-635-093-634-345; 056-928-487-967-475; 087-245-989-849-446; 112-079-416-063-263; 152-662-768-493-397; 167-808-180-788-454; 171-576-034-263-697; 177-735-601-581-063,0,true,,unknown
051-160-099-480-641,Investigating the Predictive Capabilities of Large Language Models in Day Trading by Leveraging Multimodal Data,2025-03-21,2025,conference proceedings article,2025 7th International Conference on Natural Language Processing (ICNLP),,IEEE,,Adam Horn; Tim Schlippe,,,,34,38,Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/icnlp65360.2025.11108638,,10.1109/icnlp65360.2025.11108638,,,0,058-762-702-766-04X; 083-368-266-683-328; 162-778-502-552-780,0,false,,
051-301-127-296-806,Intelligent Virtual Assistant to Facilitate Learning in International Trade Contexts,2025-10-08,2025,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Switzerland,Germany,Angel Fiallos; Erika Anton,,,,315,325,Computer science; Thesaurus; World Wide Web; Human–computer interaction; Multimedia; Information retrieval; Artificial intelligence,,,,,,http://dx.doi.org/10.1007/978-3-032-07175-0_21,,10.1007/978-3-032-07175-0_21,,,0,034-207-794-611-075; 068-864-486-586-772; 099-977-184-593-075; 102-527-805-380-549; 112-817-312-216-36X,0,false,,
051-651-564-732-696,Enhanced Financial Sentiment Analysis and Trading Strategy Development Using Large Language Models,,2024,conference proceedings article,"Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",,Association for Computational Linguistics,,Kemal Kirtac; Guido Germano,,,,1,10,Sentiment analysis; Computer science; Trading strategy; Finance; Artificial intelligence; Business,,,,,,http://dx.doi.org/10.18653/v1/2024.wassa-1.1,,10.18653/v1/2024.wassa-1.1,,,0,,5,false,,
051-794-912-621-625,The Effect of the Anthropomorphism Level and Personalization Level on AI Financial Chatbot Recommendation Messages on Customer Response,2020-04-30,2020,journal article,The Korean Journal of Advertising and Public Relations,17382475,Korean Association for Advertising and Public Relations,,Sung-Hyuk Byun; Chang-Hoan Cho,,22,2,466,502,World Wide Web; Personalization; Chatbot; Customer response; Computer science,,,,,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002586683,http://dx.doi.org/10.16914/kadpr.2020.22.2.466,,10.16914/kadpr.2020.22.2.466,3027067247,,0,,2,false,,
052-040-164-536-016,"Large Language Models in Corporate Investment: An Analysis of ChatGPT's Transformative Applications, Strategic Impacts, and Future Opportunities",,2025,journal article,IT Professional,15209202; 1941045x,Institute of Electrical and Electronics Engineers (IEEE),United States,Xudong Zhuang; Yu Wu,,27,2,21,27,Transformative learning; Investment (military); Computer science; Strategic planning; Business; Knowledge management; Process management; Political science; Marketing; Sociology; Pedagogy; Politics; Law,,,,,,http://dx.doi.org/10.1109/mitp.2025.3543555,,10.1109/mitp.2025.3543555,,,0,027-834-026-409-711; 033-371-682-030-573; 035-516-674-342-099; 044-758-674-793-904; 053-613-341-919-467; 061-138-405-343-920; 071-860-959-402-137; 081-490-757-862-263; 081-697-470-656-444; 082-600-317-526-224; 108-453-997-659-622; 125-620-473-857-443; 148-550-377-754-632; 189-995-941-040-729; 198-191-144-386-084,1,false,,
052-509-590-819-284,Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis,2025-07-24,2025,preprint,arXiv (Cornell University),,,,Md Talha Mohsin,"Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.",,,,,Finance; Artificial intelligence; Natural language processing; Computer science; Economics,,,,,https://arxiv.org/abs/2507.22936,http://dx.doi.org/10.48550/arxiv.2507.22936,,10.48550/arxiv.2507.22936,,,0,,0,true,,green
053-290-499-991-828,The economic trade-offs of large language models: A case study,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Kristen Howell; Gwen Christian; Pavel Fomitchov; Gitit Kehat; Julianne Marzulla; Leanne Rolston; Jadin Tredup; Ilana Zimmerman; Ethan Selfridge; Joseph Bradley,"Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. Large Language Models (LLMs) are a natural fit for this use case; however, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM - prompt engineering, fine-tuning, and knowledge distillation - using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.",,,,,Context (archaeology); Usability; Computer science; Service (business); Function (biology); Product (mathematics); Inference; Space (punctuation); Dual (grammatical number); Marketing; Artificial intelligence; Business; Human–computer interaction; Art; Literature; Operating system; Paleontology; Geometry; Mathematics; Evolutionary biology; Biology,,,,,https://arxiv.org/abs/2306.07402,http://dx.doi.org/10.48550/arxiv.2306.07402,,10.48550/arxiv.2306.07402,,,0,,0,true,cc-by-sa,green
053-664-178-101-963,"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",,2025,conference proceedings article,Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence,,International Joint Conferences on Artificial Intelligence Organization,,Jemin Lee; Sihyeong Park; Jinse Kwon; Jihun Oh; Yongin Kwon,"<jats:p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. ; ; In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. ; ; Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; ; ; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance;; ; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model’s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.</jats:p>",,,8113,8121,Quantization (signal processing); Computer science; Language model; Task (project management); Enhanced Data Rates for GSM Evolution; Cognitive psychology; Artificial intelligence; Natural language processing; Speech recognition; Psychology; Algorithm; Engineering; Systems engineering,,,,,,http://dx.doi.org/10.24963/ijcai.2025/902,,10.24963/ijcai.2025/902,,,0,,0,false,,
053-674-685-293-954,"Large Language Models for Financial Aid in Financial Time-series
  Forecasting",2024-10-24,2024,preprint,arXiv (Cornell University),,,,Md Khairul Islam; Ayush Karmacharya; Timothy Sue; Judy Fox,"Considering the difficulty of financial time series forecasting in financial aid, much of the current research focuses on leveraging big data analytics in financial services. One modern approach is to utilize ""predictive analysis"", analogous to forecasting financial trends. However, many of these time series data in Financial Aid (FA) pose unique challenges due to limited historical datasets and high dimensional financial information, which hinder the development of effective predictive models that balance accuracy with efficient runtime and memory usage. Pre-trained foundation models are employed to address these challenging tasks. We use state-of-the-art time series models including pre-trained LLMs (GPT-2 as the backbone), transformers, and linear models to demonstrate their ability to outperform traditional approaches, even with minimal (""few-shot"") or no fine-tuning (""zero-shot""). Our benchmark study, which includes financial aid with seven other time series tasks, shows the potential of using LLMs for scarce financial datasets.",,,,,Series (stratigraphy); Finance; Financial modeling; Economics; Geology; Paleontology,,,,,https://arxiv.org/abs/2410.19025,http://dx.doi.org/10.48550/arxiv.2410.19025,,10.48550/arxiv.2410.19025,,,0,,0,true,,green
053-742-588-043-488,Qwen-Fin: Chinese Financial Sentiment Analysis Based on Fine-Tuned Large Language Model,2024-12-09,2024,conference proceedings article,2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),,IEEE,,Zongwu Ke; Zeyu Zhang; Long Zhou; Minqi Wu; Tao Zhang,,,,812,817,Computer science; Language model; Sentiment analysis; Fin; Natural language processing; Artificial intelligence; Engineering; Mechanical engineering,,,,,,http://dx.doi.org/10.1109/wi-iat62293.2024.00132,,10.1109/wi-iat62293.2024.00132,,,0,021-727-732-019-516; 044-456-506-571-452; 065-802-740-970-276; 071-423-367-050-689; 133-839-654-498-028,0,false,,
053-748-610-097-704,FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Giorgos Iacovides; Wuyang Zhou; Danilo Mandic,,,,647,655,,,,,,,http://dx.doi.org/10.1145/3768292.3770367,,10.1145/3768292.3770367,,,0,006-399-176-500-152; 023-201-346-648-150; 034-353-410-598-189; 044-456-506-571-452; 057-982-454-558-150; 080-828-384-981-970; 090-829-135-229-996; 110-558-988-490-565; 150-274-768-001-56X; 168-485-640-067-096; 195-563-282-301-128,0,false,,
053-797-347-252-180,FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning,,2025,conference proceedings article,Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Shaoyu Dou; Yutian Shen; Mofan Chen; Zixuan Wang; Jiajie Xu; Qi Guo; Kailai Shao; Chao Chen; Haixiang Hu; Haibo Shi; Min Min; Liwen Zhang,,,,47,69,,,,,,,http://dx.doi.org/10.18653/v1/2025.finnlp-2.5,,10.18653/v1/2025.finnlp-2.5,,,0,,0,false,,
054-471-965-501-427,Unstructured Data and AI: Fine-Tuning LLMs to Enhance the Investment Process,2024-05-01,2024,report,,,CFA Institute,,Brian Pisaneschi,<jats:p>Access to powerful LLMs like ChatGPT is reshaping roles in the investment profession. This report discusses how to ethically build investment models in the open-source community. It defines alternative data and presents an ESG investing case study.</jats:p>,,,,,Process (computing); Investment (military); Computer science; Business; Political science; Politics; Law; Operating system,,,,,,http://dx.doi.org/10.56227/24.1.9,,10.56227/24.1.9,,,0,,1,false,,
054-756-960-571-302,"Review of: ""A Dutch Financial Large Language Model""",2025-01-27,2025,review,,,Qeios Ltd,,Yijie Bei,,,,,,Finance; Business,,,,,,http://dx.doi.org/10.32388/4hde0x,,10.32388/4hde0x,,,0,,0,false,,
054-832-147-976-069,Fine-Tuning Large Language Models for Financial Markets via Ontological Reasoning,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Teodoro Baldazzi; Luigi Bellomarini; Stefano Ceri; Andrea Colombo; Andrea Gentili; Emanuel Sallinger,,,,,,Financial market; Computer science; Finance; Economics,,,,,,http://dx.doi.org/10.2139/ssrn.5274196,,10.2139/ssrn.5274196,,,0,011-697-956-523-714; 112-245-634-858-364; 133-877-903-001-770; 151-622-034-226-932; 153-258-065-246-295,0,false,,
054-883-635-921-366,"Large Language Models for Financial and Investment Management: Models, Opportunities, and Challenges",2024-11-08,2024,journal article,The Journal of Portfolio Management,00954918; 21688656,With Intelligence LLC,United States,Yaxuan Kong; Yuqi Nie; Xiaowen Dong; John M. Mulvey; H. Vincent Poor; Qingsong Wen; Stefan Zohren,,51,2,211,231,Investment (military); Business; Financial modeling; Finance; Investment management; Economics; Political science; Politics; Market liquidity; Law,,,,,,http://dx.doi.org/10.3905/jpm.2024.1.646,,10.3905/jpm.2024.1.646,,,0,,6,false,,
054-896-718-504-421,Evaluating the Trade-off Between Analogical Reasoning Ability and Efficiency in Large Language Models,2025-01-01,2025,journal article,IEEE Transactions on Cognitive and Developmental Systems,23798920; 23798939,Institute of Electrical and Electronics Engineers Inc.,United States,Kara Combs; Isaiah Goble; Spencer V. Howlett; Yuki Adams; Trevor Bihl,,,,1,10,Computer science; Analogical reasoning; Natural language processing; Artificial intelligence; Analogy; Linguistics; Philosophy,,,,,,,,,,,0,,0,true,cc-by,hybrid
055-283-119-695-742,Large Language Models and the Elliott Wave Principle: A Multi-Agent Deep Learning Approach to Big Data Analysis in Financial Markets,2024-12-19,2024,journal article,Applied Sciences,20763417,MDPI AG,,Michał Wawer; Jarosław A. Chudziak; Ewa Niewiadomska-Szynkiewicz,"<jats:p>Traditional technical analysis methods face limitations in accurately predicting trends in today’s complex financial markets. Meanwhile, existing AI-driven approaches, while powerful in processing large datasets, often lack interpretability due to their black-box nature. This paper presents ElliottAgents, a multi-agent system that combines the Elliott wave principle with LLMs, showcasing the application of deep reinforcement learning (DRL) and natural language processing (NLP) in financial analysis. By integrating retrieval-augmented generation (RAG) and deep reinforcement learning (DRL), the system processes vast amounts of market data to identify Elliott wave patterns and generate actionable insights. The system employs a coordinated team of specialized agents, each responsible for specific aspects of analysis, from pattern recognition to investment strategy formulation. We tested ElliottAgents on both stock and cryptocurrency markets, evaluating its effectiveness in pattern identification and trend prediction across different time scales. Our experimental results demonstrate improvements in prediction accuracy when combining classical technical analysis with AI-driven approaches, particularly when enhanced by DRL-based backtesting process. This research contributes to the advancement of financial technology by introducing a scalable, interpretable framework that enhances market analysis capabilities, offering a promising new methodology for both practitioners and researchers.</jats:p>",14,24,11897,11897,Financial market; Computer science; Economics; Finance,,,,,,http://dx.doi.org/10.3390/app142411897,,10.3390/app142411897,,,0,016-120-464-582-123; 017-298-066-610-795; 029-022-196-371-217; 037-832-250-871-998; 059-573-977-797-121; 062-912-751-723-965; 064-177-874-619-070; 067-463-373-524-731; 071-734-046-980-146; 079-927-095-270-775; 088-406-829-017-430; 094-059-713-058-39X; 131-026-414-989-425; 134-396-914-886-155; 143-455-353-681-603,2,true,cc-by,gold
055-449-224-986-40X,Table 7: Yearly performance and decomposition of transaction cost-adjusted long-short portfolios based on LLM sentiment (2014–2023).,,,component,,,PeerJ,,,,,,,,,,,,,,http://dx.doi.org/10.7717/peerj-cs.3349/table-7,,10.7717/peerj-cs.3349/table-7,,,0,,0,false,,
055-475-641-850-887,Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs,2025-01-01,2025,preprint,,,Elsevier BV,,Theo Koraag; Niklas Wagner; Felix Dobslaw; Lucas Gren,,,,,,Domain (mathematical analysis); Software; Open source; Business; Computer science; Engineering; Software engineering; Mathematics; Mathematical analysis; Programming language,,,,,,http://dx.doi.org/10.2139/ssrn.5489581,,10.2139/ssrn.5489581,,,0,006-626-884-287-564; 008-708-960-845-751; 010-081-393-133-162; 016-531-084-325-607; 018-209-392-602-415; 038-258-320-042-590; 038-287-375-579-320; 047-706-358-211-647; 053-094-537-530-97X; 053-591-547-250-936; 070-504-120-279-017; 083-703-764-116-241; 093-322-014-375-686; 096-219-192-521-74X; 110-454-093-753-903; 110-510-842-185-282; 115-392-352-185-391; 117-524-296-887-122; 132-507-555-191-410; 136-766-311-960-457; 154-042-138-562-576,0,false,,
055-546-364-636-772,How To Unlock Financial Institutions Digital Resources Latent Value with APIs Store & Virtual Assistant,,2016,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Mohamed E Ait Hassoune,,,,,,Finance; Fixed income; Digital transformation; Business; Natural language; Revenue; Mergers and acquisitions; Service-oriented architecture; Analytics; Application programming interface,,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2820498,http://dx.doi.org/10.2139/ssrn.2820498,,10.2139/ssrn.2820498,2508145780,,0,,0,false,,
055-794-758-962-071,Digital transformation in financial services provision: A Nigerian perspective to the adoption of chatbot,2021-05-19,2021,journal article,Journal of Enterprising Communities: People and Places in the Global Economy,17506204,Emerald,United Kingdom,Abdulazeez Abdulquadri; Emmanuel Mogaji; Tai Anh Kieu; Nguyen Phong Nguyen,"Purpose:; Recognising the high numbers of unbanked and financially excluded adults in Nigeria, this study positions chatbot as a digital transformation tool to radically change business model, improve customer experience, and enhance financial inclusion in emerging markets.; ; Methodology: The Search-Access-Test (S-A-T) model was adopted to understand how Nigerian banks are adopting chatbots.; ; Findings:; Majority of Nigerian banks now have chatbots which enhance customer engagement and financial inclusion. WhatsApp was the most frequently used platform. Chatbots were often branded and presented with female gender identification. The chatbots were less responsive beyond their pre-defined path. While Nigeria is a multilingual country with English being the original language, none of the chatbots used any of the Nigerian’s local languages.; ; Originality:; While many theoretically based model for investigating the adoption of digital technologies has often placed focus on users’ ability to engage, this study takes an alternative perspective; by using the Search-Access-Test (S-A-T) model, it lays the responsibilities on the banks and chatbot developer to ensure that their chatbots are secure, responsive and able to meet the needs of the customers.; ; Practical implications:; Brands needs to reevaluate their chatbots with regards to responsiveness, pre-defined questions, verification and privacy. There are also possibilities of branding the chatbot and developing content creation strategies for proper engagement. Beyond English, the integration of African languages into chatbot is essential for digital transformation. Digital literacy and skills, particularly in the field of science, technology, engineering and mathematics (STEM) should be supported to equip future developers and create more jobs.",15,2,258,281,Digital transformation; Financial services; Content creation; Customer engagement; Digital literacy; Chatbot; Financial inclusion; Public relations; Business model,,,,,https://www.emerald.com/insight/content/doi/10.1108/JEC-06-2020-0126/full/html http://gala.gre.ac.uk/id/eprint/30005/,http://dx.doi.org/10.1108/jec-06-2020-0126,,10.1108/jec-06-2020-0126,3119553947,,0,002-972-732-307-34X; 003-118-998-063-278; 005-885-274-117-373; 006-494-449-801-037; 006-665-614-730-871; 011-176-387-724-212; 011-526-846-145-963; 012-698-714-818-481; 013-046-753-798-658; 023-351-076-897-766; 023-742-217-441-622; 027-087-800-065-713; 027-870-773-115-918; 032-652-158-672-929; 038-780-787-340-729; 052-764-472-537-743; 060-247-840-417-302; 060-467-260-279-219; 062-082-962-614-929; 066-559-168-501-768; 069-647-253-101-776; 088-557-268-966-764; 093-375-309-026-212; 093-947-181-193-492; 096-923-029-679-618; 103-094-384-406-08X; 110-772-133-997-202; 111-278-622-296-434; 112-256-911-197-001; 112-885-767-712-794; 123-233-504-661-353; 125-812-694-825-484; 154-069-822-888-951; 156-219-242-807-530; 172-394-434-510-385,189,true,,green
056-286-620-183-812,Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios,2025-07-24,2025,preprint,arXiv (Cornell University),,,,Zhuang Qiang Bok; Watson Wei Khong Chua,"Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step logic. In finance, however, professionals must not only converge on optimal decisions but also generate creative, plausible futures under uncertainty. We introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent thinking in LLMs for financial tasks. ConDiFi features 607 macro-financial prompts for divergent reasoning and 990 multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we evaluated 14 leading models and uncovered striking differences. Despite high fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models like DeepSeek-R1 and Cohere Command R+ rank among the top for generating actionable, insights suitable for investment decisions. ConDiFi provides a new perspective to assess reasoning capabilities essential to safe and strategic deployment of LLMs in finance.",,,,,Economics; Finance,,,,,https://arxiv.org/abs/2507.18368,http://dx.doi.org/10.48550/arxiv.2507.18368,,10.48550/arxiv.2507.18368,,,0,,0,true,,green
056-512-143-098-406,Combining Domain and Alignment Vectors Provides Better Knowledge-Safety Trade-offs in LLMs,,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),,Association for Computational Linguistics,,Megh Thakkar; Quentin Fournier; Matthew Riemer; Pin-Yu Chen; Amal Zouaq; Payel Das; Sarath Chandar,,,,268,277,Domain (mathematical analysis); Computer science; Business; Risk analysis (engineering); Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.18653/v1/2025.acl-short.22,,10.18653/v1/2025.acl-short.22,,,0,,0,false,,
056-536-978-626-429,"Evolving trends, limitations, and ethical considerations in AI-driven conversational interfaces: assessing ChatGPT's impact on healthcare, financial services, and educational sectors",2024-11-04,2024,journal article,Technology Analysis and Strategic Management,09537325; 14653990,Routledge,United Kingdom,Mohd Afjal,"This study presents a comprehensive bibliometric analysis of ChatGPT's impact on three critical sectors: healthcare, financial services, and education, covering the period from 2022 to 2024. The research aimed to explore the evolving trends, limitations, and ethical considerations associated with the deployment of ChatGPT and similar AI-driven conversational interfaces. The findings reveal that while ChatGPT offers significant potential in enhancing efficiency and decision-making processes, it also faces substantial challenges in data privacy, integration with existing systems, and the need for improved emotional intelligence. The study highlights the inherent biases in AI models, raising concerns about the accuracy and fairness of AI-driven outputs. Ethical considerations, including data privacy, transparency, accountability, and equity in AI applications, are identified as crucial areas requiring urgent attention and development. Additionally, the research underscores the importance of developing robust regulatory frameworks and ethical guidelines to govern AI deployment in these sectors. Successful integration of ChatGPT and similar technologies depends on addressing these challenges and ethical considerations through collaborative efforts among AI developers, industry professionals, policymakers, and ethicists. Future research should focus on improving AI's data security, emotional intelligence, bias reduction, human-AI collaboration, and assessing long-term impacts across diverse settings.",,,1,20,Health care; Financial services; Business; Engineering ethics; Psychology; Knowledge management; Computer science; Finance; Economics; Engineering; Economic growth,,,,,,,,,,,0,,0,false,,
056-688-915-155-538,"Managerial sentiment, life cycle and corporate investment: a large language model approach",2024-07-02,2024,journal article,International Journal of Managerial Finance,17439132; 17586569,Emerald,United Kingdom,Anamika Rana; Asis Kumar Sahu; Byomakesh Debata,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>This paper investigates the relationship between managerial sentiment and corporate investment in emerging capital markets. Further, we begin with the assertion that the positive impact of managerial sentiment on corporate investment varies according to the corporate life cycle. Lastly, we investigate whether the relationship between managerial sentiment and corporate investment can be moderated by factors like (1) economic policy uncertainty/geo-political risk, (2) size of the firm, (3) financial constraint, (4) industrial competition, and (5) Environmental Social and Governance (ESG) rating.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>This study has considered Indian listed companies (465 firms) for the period spanning from 2003–2004 to 2022–2023. This study constructs the managerial sentiment using a novel large language model-financial bidirectional encoder representation from the Transformers (FinBERT), as well as on management discussion and analysis reports. Then, we employ fixed effect regression to investigate the relationship between managerial sentiment and corporate investment. Additionally, we use propensity score matching, two-stage least squares instrumental variables, and a two-step system generalized method of moments approach for robustness tests.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>The findings show a positive and significant relationship between managerial sentiment and corporate investment. Additionally, our results demonstrate that this relationship is evident only during the growth and maturity phase of the corporate life cycle. Moreover, uncertainty pertaining to the economy and geopolitical issues, firm size, financial health, industry dynamics, and ESG disclosure also play a crucial role in shaping the investment-sentiment relationship.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>The study is unique because it determines the relationship between managerial sentiment and corporate investment by using the novel FinBERT model. In addition, we have introduced a corporate life cycle, which is an essential aspect of our study. Additionally, this research was conducted in an emerging market with more information asymmetry and weaker disclosure rules. Thus, other emerging markets can benchmark the outcomes.</jats:p></jats:sec>",21,1,87,110,Corporate governance; Economics; Robustness (evolution); Market sentiment; Investment (military); Investment decisions; Business; Econometrics; Financial economics; Microeconomics; Finance; Politics; Biochemistry; Chemistry; Production (economics); Political science; Law; Gene,,,,,,http://dx.doi.org/10.1108/ijmf-12-2023-0617,,10.1108/ijmf-12-2023-0617,,,0,001-354-922-931-683; 001-751-100-594-234; 002-006-438-937-237; 003-953-647-647-22X; 004-082-018-727-361; 004-259-290-832-014; 004-699-534-320-672; 005-042-771-654-262; 005-673-089-571-131; 005-989-438-190-750; 006-399-176-500-152; 009-022-170-719-443; 012-190-685-561-287; 013-042-681-322-977; 013-943-268-559-979; 014-134-762-445-615; 014-343-045-734-116; 015-718-494-537-948; 017-091-424-807-11X; 017-621-990-271-933; 022-168-904-707-330; 026-200-753-913-905; 028-382-484-238-798; 029-364-455-451-36X; 032-888-193-060-347; 034-210-549-560-961; 035-235-742-777-246; 036-979-852-259-747; 042-335-426-642-552; 043-892-211-907-224; 046-373-079-491-042; 047-073-906-609-326; 049-569-312-606-875; 049-879-929-171-975; 052-189-048-818-521; 052-862-594-353-562; 055-318-598-916-211; 055-485-045-935-306; 056-467-153-344-631; 056-491-806-605-596; 057-085-980-691-031; 060-555-637-807-553; 063-695-863-579-900; 066-021-443-046-671; 069-646-781-007-826; 070-250-583-618-145; 073-854-087-596-828; 077-542-059-860-903; 079-167-079-371-964; 086-513-165-274-063; 086-730-647-813-593; 086-730-721-632-046; 089-823-287-593-970; 090-589-468-914-713; 092-727-226-172-556; 093-769-286-760-343; 100-202-053-769-173; 106-772-837-932-154; 108-659-204-010-764; 114-111-686-658-531; 115-746-855-129-75X; 117-506-044-370-080; 120-356-451-279-986; 127-379-617-155-518; 132-558-492-977-554; 142-265-139-854-661; 146-301-466-127-630; 149-539-855-833-87X; 154-042-138-562-576; 154-658-228-826-534; 161-554-674-528-075; 163-999-779-073-016; 167-999-949-731-567; 171-529-937-327-864; 179-426-196-483-972; 189-197-165-376-265,4,false,,
056-919-779-684-210,Reducing Hallucinations and Trade-Offs in Responses in Generative AI Chatbots for Cancer Information: Development and Evaluation Study.,2025-09-11,2025,journal article,JMIR cancer,23691999,JMIR Publications Inc.,Canada,Sota Nishisako; Takahiro Higashi; Fumihiko Wakao,"<AbstractText Label=""Background"" NlmCategory=""UNASSIGNED"">Generative artificial intelligence (AI) is increasingly used to find information. Providing accurate information is essential to support patients with cancer and their families; however, information returned by generative AIs is sometimes wrong. Returning wrong information is called hallucination. Retrieval-augmented generation (RAG), which supplements large language model (LLM) outputs with relevant external sources, has the potential to reduce hallucinations. Although RAG has been proposed as a promising technique, its real-world performance in public health communication remains underexplored.</AbstractText>;           <AbstractText Label=""Objective"" NlmCategory=""UNASSIGNED"">This study aimed to examine cancer information returned by generative AIs with RAG using cancer-specific information sources and general internet searches to determine whether using RAG with reliable information sources reduces the hallucination rates of generative AI chatbots.</AbstractText>;           <AbstractText Label=""Methods"" NlmCategory=""UNASSIGNED"">We developed 6 types of chatbots by combining 3 patterns of reference information with 2 versions of LLMs. Thus, GPT-4 and GPT-3.5 chatbots that use cancer information service (CIS) information, Google information, and no reference information (conventional chatbots) were developed. A total of 62 cancer-related questions in Japanese were compiled from public sources. All responses were generated automatically and independently reviewed by 2 experienced clinicians. The reviewers assessed the presence of hallucinations, defined as medically harmful or misinformation. We compared hallucination rates across chatbot types and calculated odds ratios (OR) using generalized linear mixed-effects models. Subgroup analyses were also performed based on whether questions were covered by CIS content.</AbstractText>;           <AbstractText Label=""Results"" NlmCategory=""UNASSIGNED"">For the chatbots that used information from CIS, the hallucination rates were 0% for GPT-4 and 6% for GPT-3.5, whereas those for chatbots that used information from Google were 6% and 10% for GPT-4 and GPT-3.5, respectively. For questions on information that is not issued by CIS, the hallucination rates for Google-based chatbots were 19% for GPT-4 and 35% for GPT-3.5. The hallucination rates for conventional chatbots were approximately 40%. Using reference data from Google searches generated more hallucinations than using CIS data, with an OR of 9.4 (95% CI 1.2-17.5, P&lt;.01); the OR for the conventional chatbot was 16.1 (95% CI 3.7-50.0, P&lt;.001). While conventional chatbots always generated a response, the RAG-based chatbots sometimes declined to answer when information was lacking. The conventional chatbots responded to all questions, but the response rate decreased (36% to 81%) for RAG-based chatbots. For questions on information not covered by CIS, the CIS chatbots did not respond, while the Google chatbots generated responses in 52% of the cases for GPT-4 and 71% for GPT-3.5.</AbstractText>;           <AbstractText Label=""Conclusions"" NlmCategory=""UNASSIGNED"">Using RAG with reliable information sources significantly reduces the hallucination rate of generative AI chatbots and increases the ability to admit lack of information, making them more suitable for general use, where users need to be provided with accurate information.</AbstractText>;           <CopyrightInformation>© Sota Nishisako, Takahiro Higashi, Fumihiko Wakao. Originally published in JMIR Cancer (https://cancer.jmir.org).</CopyrightInformation>",11,,e70176,e70176,Misinformation; Generative model; The Internet; Odds; Psychology; Computer science; Information retrieval; Medicine; Generative grammar; Artificial intelligence; Machine learning; World Wide Web; Computer security; Logistic regression,AI; GPT; RAG; artificial intelligence; cancer information service; generative AI chatbot; generative pretrained transformer; hallucination; medical information provision; retrieval-augmented generation,Humans; Hallucinations/prevention & control; Neoplasms/psychology; Artificial Intelligence; Internet; Generative Artificial Intelligence,,,,http://dx.doi.org/10.2196/70176,40934488,10.2196/70176,,PMC12425422,0,008-884-104-207-426; 009-661-190-177-945; 010-629-902-827-682; 021-843-323-999-136; 023-735-226-782-503; 024-694-662-305-04X; 025-429-632-617-412; 025-594-124-474-813; 025-784-698-758-396; 026-246-460-413-628; 029-930-188-233-926; 031-226-967-413-179; 041-197-617-177-631; 044-282-127-394-719; 046-382-298-171-588; 052-407-175-838-608; 056-839-106-535-202; 057-888-544-402-263; 059-640-149-464-572; 063-104-480-070-392; 064-277-151-369-344; 067-433-106-934-552; 070-044-085-070-677; 073-120-773-515-28X; 075-046-239-817-210; 078-525-848-451-071; 081-530-179-859-786; 086-000-208-286-870; 090-578-755-021-682; 091-859-351-368-150; 112-116-905-098-192; 112-499-398-720-135; 120-327-042-569-023; 122-614-126-107-429; 128-110-503-104-995; 130-950-282-801-474; 137-120-754-193-596; 140-987-845-509-381; 144-471-236-462-002; 145-609-086-683-373; 158-831-874-520-048; 173-571-798-938-244; 190-874-884-638-090,0,true,cc-by,gold
057-315-573-154-299,FinGPT: Democratizing Internet-scale Data for Financial Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Xiao-Yang Liu; Guoxuan Wang; Hongyang Yang; Daochen Zha,"Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available, and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, Financial Generative Pre-trained Transformer (FinGPT), that automates the collection and curation of real-time financial data from 34 diverse sources on the Internet, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. Additionally, we propose a simple yet effective strategy for fine-tuning FinLLM using the inherent feedback from the market, dubbed Reinforcement Learning with Stock Prices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method that enables users to customize their own FinLLMs from general-purpose LLMs at a low cost. Finally, we showcase several FinGPT applications, including robo-advisor, sentiment analysis for algorithmic trading, and low-code development. FinGPT aims to democratize FinLLMs, stimulate innovation, and unlock new opportunities in open finance. The codes have been open-sourced.",,,,,The Internet; Computer science; Finance; Financial market; Financial modeling; Stock market; Data science; World Wide Web; Business; Geography; Context (archaeology); Archaeology,,,,,https://arxiv.org/abs/2307.10485,http://dx.doi.org/10.48550/arxiv.2307.10485,,10.48550/arxiv.2307.10485,,,0,,3,true,other-oa,green
057-644-725-051-22X,"Leveraging Large Language Models for Institutional Portfolio Management:
  Persona-Based Ensembles",2024-11-29,2024,preprint,arXiv (Cornell University),,,,Yoshia Abe; Shuhei Matsuo; Ryoma Kondo; Ryohei Hisano,"Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially when combined with the mode ensemble, outperform the buy-and-hold strategy in terms of Sharpe ratio during periods of rising consumer price index (CPI). However, traditional strategies are more effective during declining CPI trends or sharp market downturns. These results suggest that while LLMs can enhance portfolio management, they may require complementary strategies to optimize performance across varying market conditions.",,,,,Persona; Portfolio; Computer science; Business; Data science; Linguistics; Knowledge management; Human–computer interaction; Finance; Philosophy,,,,,https://arxiv.org/abs/2411.19515,http://dx.doi.org/10.48550/arxiv.2411.19515,,10.48550/arxiv.2411.19515,,,0,,0,true,,green
058-321-860-984-479,Utilising AI-based Chatbot for Maximising Value and Profitability of Islamic Financial Institutions,2024-04-01,2024,journal article,مجلة بيت المشورة,24106836; 24090867,Bait Al-Mashura Finance Consultations,,Zaki Ahmad; Ikhlas al-Amatullah; Mohamed Nafeel Mahboob,"<jats:p>global Islamic finance industry has experienced significant growth in recent years, establishing Islamic financial institutions (IFIs) as prominent players in the international financial landscape. To maintain competitiveness and sustainability, IFIs must embrace innovation and explore avenues for maximising their performance. This research aims to investigate the effect of AI-based Chatbots on the value and profitability of IFIs in Organisation of Islamic Cooperation (OIC) countries. The study has employed quantitative methodology, and the data was collected from the Bloomberg database, Thomson Reuters DataStream, World Development Indicators and annual reports of 104 IFIs across 44 OIC countries from 2020 to 2022. Employing the Generalised Method of Moments (GMM), this study investigates the impact of AI-based Chatbots on the value and profitability of IFIs. The findings show a positive impact of Chatbot on Return on Assets (ROA), Return on Equity (ROE), and Tobin’s Q (TBQ). Furthermore, the analysis reveals that variables such as Gross Domestic Product (GDP), institutional size (SIZE), and age (AGE) exhibit positive associations with the value and profitability of IFIs. Conversely, the study identifies Consumer Price Index (CPI) as exerting a negative influence on the value and profitability of IFIs, indicating potential challenges posed by inflationary pressures. IFIs may consider investing in the development and deployment of advanced AI technologies to enhance overall profitability. Keywords: AI-based Chatbot; Profitability; Islamic financial institutions (IFIs); OIC Countries; Dynamic GMM</jats:p>",,21,243,275,Chatbot; Profitability index; Islam; Business; Value (mathematics); Finance; Computer science; World Wide Web; History; Archaeology; Machine learning,,,,,,http://dx.doi.org/10.33001/m0104202421/125,,10.33001/m0104202421/125,,,0,,1,true,cc-by,gold
058-473-097-766-525,Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Agam Shah; Sudheer Chava,"Recently large language models (LLMs) like ChatGPT have shown impressive performance on many natural language processing tasks with zero-shot. In this paper, we investigate the effectiveness of zero-shot LLMs in the financial domain. We compare the performance of ChatGPT along with some open-source generative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We address three inter-related research questions on data annotation, performance gaps, and the feasibility of employing generative models in the finance domain. Our findings demonstrate that ChatGPT performs well even without labeled data but fine-tuned models generally outperform it. Our research also highlights how annotating with generative models can be time-intensive. Our codebase is publicly available on GitHub under CC BY-NC 4.0 license.",,,,,Zero (linguistics); Shot (pellet); Computer science; Generative grammar; Generative model; Domain (mathematical analysis); Finance; Artificial intelligence; Business; Linguistics; Philosophy; Chemistry; Organic chemistry; Mathematical analysis; Mathematics,,,,,https://arxiv.org/abs/2305.16633,http://dx.doi.org/10.48550/arxiv.2305.16633,,10.48550/arxiv.2305.16633,,,0,,1,true,cc-by,green
058-493-784-538-426,An End-To-End LLM Enhanced Trading System,2025-11-20,2025,journal article,Journal of Applied Business and Economics,1499691x,North American Business Press,,Ziyao Zhou; Ronitt Mehra,"<jats:p>This project introduces an end-to-end trading system that leverages Large Language Models (LLMs) for real-time market sentiment analysis and trading signal generation. By synthesizing financial news and social-media streams, the system integrates sentiment-driven insights with technical indicators to support actionable investment decisions. FinGPT serves as the core sentiment model, ensuring domain-specific accuracy, while additional models validate performance across financial contexts. The framework combines live multi-source data processing, LLM-based summarization, and real-world strategy testing, deployed on Kubernetes for scalable operation. This comprehensive approach demonstrates how advanced LLM techniques can enhance trading strategies through timely and nuanced sentiment understanding.</jats:p>",27,6,,,,,,,,,http://dx.doi.org/10.33423/jabe.v27i6.7961,,10.33423/jabe.v27i6.7961,,,0,,0,false,,
059-023-773-463-975,Large Language Models and Financial Market Sentiment,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Shaun Alexander Bond; Hayden Klok; Min Zhu,,,,,,Financial market; Sentiment analysis; Business; Financial economics; Finance; Economics; Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.2139/ssrn.4584928,,10.2139/ssrn.4584928,,,0,001-738-152-412-643; 005-915-144-846-281; 006-399-176-500-152; 011-898-372-474-85X; 019-051-684-028-071; 022-486-017-682-206; 024-431-940-748-541; 024-771-846-569-660; 040-672-185-519-039; 047-871-035-706-003; 053-452-330-947-361; 057-982-454-558-150; 067-192-891-291-569; 077-228-874-021-967; 078-106-229-576-047; 078-981-577-025-706; 087-548-501-519-629; 089-844-745-151-655; 100-906-449-585-640; 104-972-636-546-082; 106-055-773-412-560; 113-256-775-452-798; 137-925-447-012-933; 154-513-179-142-971; 168-485-640-067-096,3,false,,
059-346-222-934-693,"Narratives from GPT-derived Networks of News, and a link to Financial Markets Dislocations",,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Deborah Miori; Constantin Petrov,"Starting from a corpus of economic articles from The Wall Street Journal, we present a novel systematic way to analyse news content that evolves over time. We leverage on state-of-the-art natural language processing techniques (i.e. GPT3.5) to extract the most important entities of each article available, and aggregate co-occurrence of entities in a related graph at the weekly level. Network analysis techniques and fuzzy community detection are tested on the proposed set of graphs, and a framework is introduced that allows systematic but interpretable detection of topics and narratives. In parallel, we propose to consider the sentiment around main entities of an article as a more accurate proxy for the overall sentiment of such piece of text, and describe a case-study to motivate this choice. Finally, we design features that characterise the type and structure of news within each week, and map them to moments of financial markets dislocations. The latter are identified as dates with unusually high volatility across asset classes, and we find quantitative evidence that they relate to instances of high entropy in the high-dimensional space of interconnected news. This result further motivates the pursued efforts to provide a novel framework for the systematic analysis of narratives within news.",,,,,Link (geometry); Narrative; Financial market; Business; Financial system; Political science; Finance; Art; Literature; Computer science; Computer network,,,,,https://arxiv.org/pdf/2311.14419 https://arxiv.org/abs/2311.14419,http://dx.doi.org/10.2139/ssrn.4628533,,10.2139/ssrn.4628533,,,0,000-524-338-757-738; 000-810-519-562-283; 012-669-946-569-944; 019-706-336-089-489; 021-852-924-092-024; 025-352-207-023-08X; 070-758-582-977-063; 124-090-804-896-251,1,true,,green
059-370-557-937-204,Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models,2025-10-07,2025,preprint,arXiv (Cornell University),,,,Fabrizio Dimino; Krati Saxena; Bhaskarjit Sarmah; Stefano Pasquali,"Large Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.",,,,,Representation (politics); Investment (military); Computer science; Linguistics; Econometrics; Natural language processing; Economics; Political science; Philosophy; Politics; Law,,,,,https://arxiv.org/abs/2510.05702,http://dx.doi.org/10.48550/arxiv.2510.05702,,10.48550/arxiv.2510.05702,,,0,,0,true,,green
059-569-926-479-295,Hybrid Reasoning Developer Assistants Leveraging LLMs for Enterprise Grade Java and Node Financial Systems,2025-09-07,2025,journal article,AVE Trends in Intelligent Management Letters,30691117,AVE Trends Publishing Company,,Jaya Ram Menda,"<jats:p>Developers working on complex Java and Node financial applications continue to face challenges in maintaining reliability, compliance, and development speed, creating demand for hybrid reasoning assistants powered by large language models. This study examines the capabilities of such assistants in supporting enterprise-grade financial engineering by analysing how statistical language understanding and structured reasoning can augment end-to-end software development workflows. The purpose of this research is to evaluate whether LLM-enhanced assistants can meaningfully improve code quality, interpretive accuracy, and architectural consistency across demanding financial workloads. Using a mixed methodological approach that combines qualitative assessment of reasoning outputs, quantitative measurement of productivity and error reduction, and controlled experiments simulating real financial services environments, the study investigates the practical value of hybrid assistants within Java microservice pipelines and Node-based event-driven systems. Findings show that these assistants significantly reduce coding defects, accelerate iterative development, and enhance the coherence of financial logic during design and implementation. The integration of symbolic reasoning pathways further supports compliance-aligned coding practices and more dependable architectural decisions. Strategically, the study provides a structured framework for implementing hybrid reasoning developer assistants in enterprise financial settings, contributing to both academic understanding and industry practice. The results highlight the growing significance of LLM-enabled development methodologies in modernising financial engineering and improving the resilience, efficiency, and adaptability of critical software systems.</jats:p>",1,3,150,164,,,,,,,http://dx.doi.org/10.64091/atiml.2025.000164,,10.64091/atiml.2025.000164,,,0,,0,false,,
059-592-450-070-325,Portfolio Performance Based on LLM News Scores and Related Economical Analysis,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Ruoxu Wu,,,,,,Portfolio; News analytics; Econometrics; Business; Actuarial science; Computer science; Economics; Financial economics; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.4709617,,10.2139/ssrn.4709617,,,0,,3,false,,
059-755-413-774-01X,"The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM
  Recommender Systems Using Multi-Head Early Exit",2025-01-03,2025,preprint,arXiv (Cornell University),,,,Huixue Zhou; Hengrui Gu; Xi Liu; Kaixiong Zhou; Mingfu Liang; Yongkang Xiao; Srinivas Govindan; Piyush Chawla; Jiyan Yang; Xiangfei Meng; Huayu Li; Buyun Zhang; Liang Luo; Wen-Yen Chen; Yiping Han; Bo Long; Rui Zhang; Tianlong Chen,"The deployment of Large Language Models (LLMs) in recommender systems for predicting Click-Through Rates (CTR) necessitates a delicate balance between computational efficiency and predictive accuracy. This paper presents an optimization framework that combines Retrieval-Augmented Generation (RAG) with an innovative multi-head early exit architecture to concurrently enhance both aspects. By integrating Graph Convolutional Networks (GCNs) as efficient retrieval mechanisms, we are able to significantly reduce data retrieval times while maintaining high model performance. The early exit strategy employed allows for dynamic termination of model inference, utilizing real-time predictive confidence assessments across multiple heads. This not only quickens the responsiveness of LLMs but also upholds or improves their accuracy, making it ideal for real-time application scenarios. Our experiments demonstrate how this architecture effectively decreases computation time without sacrificing the accuracy needed for reliable recommendation delivery, establishing a new standard for efficient, real-time LLM deployment in commercial systems.",,,,,Head (geology); Recommender system; Computer science; Real-time computing; Information retrieval; Biology; Paleontology,,,,,https://arxiv.org/abs/2501.02173,http://dx.doi.org/10.48550/arxiv.2501.02173,,10.48550/arxiv.2501.02173,,,0,,0,true,,green
059-805-455-184-774,Financial Inclusion and Large Language Models,2025-07-09,2025,book chapter,Advances in Computational Intelligence and Robotics,23270411; 2327042x,IGI Global Scientific Publishing,,Peterson K. Ozili; Kingsley Obiora; Chinwendu Onuzo,"<jats:p>Large language models have gained popularity, and it is important to understand their applications in the financial inclusion domain. This study identifies the benefits and risks of using large language models (LLMs) in the financial inclusion domain. We show that LLMs can be used to (i) summarize the key themes in financial inclusion communications, (ii) gain insights from the tone of financial inclusion communications, (iii) bring discipline to financial inclusion communications, (iv) improve financial inclusion decision making, and (v) enhance context-sensitive text analysis and evaluation. However, the use of large language models in the financial inclusion domain poses risks relating to biased interpretations of LLM-generated responses, data privacy risk, misinformation and falsehood risks. We emphasize that LLMs can be used safely in the financial inclusion domain to summarise financial inclusion speeches and communication, but they should not be used in situations where finding the truth is important to make decisions that promote financial inclusion.</jats:p>",,,267,284,Financial inclusion; Inclusion (mineral); Business; Linguistics; Finance; Sociology; Financial services; Philosophy; Social science,,,,,,http://dx.doi.org/10.4018/979-8-3373-1250-7.ch010,,10.4018/979-8-3373-1250-7.ch010,,,0,005-441-888-225-179; 011-322-379-629-55X; 021-887-316-511-133; 022-877-953-239-088; 029-644-246-929-239; 035-190-809-124-818; 035-268-660-446-137; 035-876-619-354-109; 035-911-721-299-418; 045-556-664-246-529; 059-533-229-398-25X; 061-960-297-209-594; 063-883-846-915-886; 069-272-524-621-08X; 081-961-426-543-311; 086-861-071-925-444; 096-318-725-614-767; 109-974-901-877-562; 110-000-902-892-836; 123-086-416-056-770; 125-812-396-894-254; 126-891-953-162-545; 129-667-561-878-440; 139-069-233-849-176; 142-511-418-752-212; 145-350-056-844-957; 150-383-567-157-587; 150-808-124-602-873; 151-390-736-859-080; 152-751-015-281-925; 153-939-232-752-691; 158-729-730-798-226; 164-670-314-521-352; 167-276-016-058-463; 167-854-143-799-231; 169-909-727-725-947; 171-228-001-252-94X; 171-321-665-870-544; 177-619-872-912-487; 180-033-874-961-16X; 188-406-476-808-215; 189-523-252-991-996; 189-782-278-861-205; 190-893-969-002-837,0,false,,
060-414-843-051-946,Application of a Model Life Cycle Concept to Investments in Artificial Intelligence Evaluation on the Example of Large Language Models,2024-07-12,2024,journal article,Finance: Theory and Practice,25877089; 25875671,Financial University under the Government of the Russian Federation,,N. A. Nikitin,"<jats:p>The life cycle of an artificial intelligence model is the <jats:bold>object</jats:bold> of research. The <jats:bold>purpose</jats:bold> of the study is to develop a model life-cycle methodology that describes the economic content of the investment process in artificial intelligence technology. During the study, both general scientific <jats:bold>methods</jats:bold> such as analysis, synthesis, comparison, abstraction, induction and deduction were used, as well as project methodologies of the life-cycle, employed as the basis for the value creation life-cycle of the model. The analysis was based on identifying the necessary stages of model development in terms of the CRISP-DM methodology and determining the features of each of them in terms of cash flows. Modified versions of the model life-cycle containing risk assessment, including model risk, were also taken into account. In the process of research, the proposed generalized model life-cycle methodology was specified for a specific AI technology — large language models. As a result of the study, the author proposed a three-stage model. The possible optionality between the stages and the characteristics of cash flows are described. It was<jats:bold> concluded</jats:bold> that an investment project for the development of AI contains several real options — abandonment, reduction, expansion and replacement. For large language models, the life cycle structure and possible optionalities are preserved. The peculiarity is that the value creation process involves cash flows from different areas of application of the model in business processes. The results of the study are of practical importance for medium and large businesses engaged in the independent development of AI models and/or applying them to their business processes. The proposed concept of the model life-cycle can also be used to develop a methodology for evaluating investments in AI using real options.</jats:p>",28,3,206,217,Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.26794/2587-5671-2024-28-3-206-217,,10.26794/2587-5671-2024-28-3-206-217,,,0,006-668-955-712-27X; 040-618-307-001-282; 044-034-131-226-299; 060-910-785-236-311; 079-599-647-606-768; 128-671-364-070-384; 132-800-157-067-548; 145-062-409-897-151; 148-699-012-400-714; 160-023-262-626-403; 166-596-369-456-850; 167-252-793-890-442; 185-976-346-713-846; 187-921-603-756-535,0,true,cc-by,gold
060-420-234-700-219,Exploring the Factors Influencing the Adoption and Continuous Engagement in Unlocking the Potential of Technology Driven Chatbots in Banking and Financial Institutions,,2023,journal article,Engineered Science,2576988x; 25769898,Engineered Science Publisher,,Pankaj Yadav; Priya Gupta; Pratibha Rai; Nithesh Naik; Kasirajan Kasipandian,"This paper explores key factors driving chatbot adoption in the banking sector and emphasizes the importance of aligning with users’ lifestyles for successful integration, offering valuable insights for industry leaders in the digital transformation journey.",,,,,Chatbot; Business; Key (lock); Financial sector; Financial services; Marketing; Knowledge management; Finance; Computer science; World Wide Web; Computer security,,,,,https://www.espublisher.com/uploads/article_pdf/es1054_JustAccepted.pdf https://doi.org/10.30919/es1054,http://dx.doi.org/10.30919/es1054,,10.30919/es1054,,,0,,1,true,,bronze
060-538-407-847-045,EU trade agreements and external differentiation: a large language model approach,2024-11-21,2024,journal article,West European Politics,01402382; 17439655,Informa UK Limited,United Kingdom,Andreas Dür,,48,5,1059,1085,,,,,,,http://dx.doi.org/10.1080/01402382.2024.2425549,,10.1080/01402382.2024.2425549,,,0,006-806-860-822-137; 009-930-052-972-900; 013-422-251-409-807; 015-755-037-676-056; 020-485-159-177-451; 023-232-614-979-565; 024-725-538-945-858; 025-242-284-792-635; 033-206-217-942-287; 041-252-676-705-595; 046-416-242-417-22X; 052-201-386-466-679; 052-709-319-788-966; 055-570-273-898-216; 069-882-178-345-18X; 072-727-756-321-138; 076-551-640-852-905; 077-168-490-480-353; 088-210-573-084-634; 090-245-377-782-621; 092-910-959-183-215; 093-886-363-365-682; 095-455-192-718-233; 103-871-182-359-282; 110-065-903-666-715; 119-363-996-689-627; 121-854-249-348-659; 125-142-778-146-93X; 131-835-228-984-145; 143-466-955-752-353; 163-743-616-003-778; 165-672-959-750-73X; 174-245-216-978-442,3,false,,
060-975-152-229-020,Optimized Financial Planning: Integrating Individual and Cooperative Budgeting Models with LLM Recommendations,2023-12-25,2023,journal article,AI,26732688,MDPI AG,,I. de Zarzà; J. de Curtò; Gemma Roig; Carlos T. Calafate,"<jats:p>In today’s complex economic environment, individuals and households alike grapple with the challenge of financial planning. This paper introduces novel methodologies for both individual and cooperative (household) financial budgeting. We firstly propose an optimization framework for individual budget allocation, aiming to maximize savings by efficiently distributing monthly income among various expense categories. We then extend this model to households, wherein the complexity of handling multiple incomes and shared expenses is addressed. The cooperative model prioritizes not only maximized savings but also the preferences and needs of each member, fostering a harmonious financial environment, whether they are short-term needs or long-term aspirations. A notable innovation in our approach is the integration of recommendations from a large language model (LLM). Given its vast training data and potent inferential capabilities, the LLM provides initial feasible solutions to our optimization problems, acting as a guiding beacon for individuals and households unfamiliar with the nuances of financial planning. Our preliminary results indicate that the LLM-recommended solutions result in budget plans that are both economically sound, meaning that they are consistent with established financial management principles and promote fiscal resilience and stability, and aligned with the financial goals and preferences of the concerned parties. This integration of AI-driven recommendations with econometric models, as an instantiation of an extended coevolutionary (EC) theory, paves the way for a new era in financial planning, making it more accessible and effective for a wider audience, as we propose an example of a new theory in economics where human behavior can be greatly influenced by AI agents.</jats:p>",5,1,91,114,Financial plan; Financial modeling; Finance; Psychological resilience; Retirement planning; Financial management; Economics; Computer science; Business; Management science; Psychotherapist; Psychology,financial planning; household finance; LLMs; budgeting; extended coevolutionary,,,GOETHE-University Frankfurt am Main; R&amp;D project; MCIN/AEI/10.13039/501100011033; ERDF,https://www.mdpi.com/2673-2688/5/1/6/pdf?version=1703579413 https://doi.org/10.3390/ai5010006 https://riunet.upv.es/bitstream/10251/205022/1/dedeRoig%20-%20Optimized%20Financial%20Planning%20Integrating%20Individual%20and%20Cooperative%20Budgeting%20Models%20w....pdf https://hdl.handle.net/10251/205022,http://dx.doi.org/10.3390/ai5010006,,10.3390/ai5010006,,,0,008-287-802-029-297; 024-920-582-562-188; 025-171-683-465-265; 036-444-444-848-066; 045-313-662-900-287; 045-380-384-679-147; 046-019-020-887-681; 055-581-611-680-978; 057-418-765-315-693; 057-622-719-929-940; 079-314-132-199-778; 079-666-169-485-582; 080-337-384-382-20X; 084-986-793-263-151; 095-207-857-591-933; 101-274-641-142-35X; 112-471-034-541-019; 113-267-285-721-100; 147-664-215-115-204; 167-887-600-154-254; 172-300-578-524-794; 185-583-432-618-274; 191-424-188-274-666,19,true,cc-by,gold
061-510-559-135-09X,The Impact of AI and Cross-Border Data Regulation on International Trade in Digital Services: A Large Language Model,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Ruiqi Sun; Daniel Trefler,,,,,,International trade; Computer science; International economics; Business; Economics,,,,,,http://dx.doi.org/10.2139/ssrn.4652398,,10.2139/ssrn.4652398,,,0,006-549-972-600-905; 009-364-042-450-523; 011-282-074-541-369; 017-358-296-871-204; 019-287-642-772-493; 019-760-142-834-521; 025-493-849-887-460; 032-112-944-716-514; 032-248-615-558-503; 033-177-960-804-999; 038-177-605-239-018; 040-111-828-598-288; 046-928-573-140-180; 047-221-542-563-890; 047-483-088-068-653; 049-492-291-494-655; 052-452-116-511-608; 052-925-790-275-202; 068-864-486-586-772; 073-656-712-996-863; 141-365-325-283-980; 157-600-120-947-85X; 169-814-618-760-194,2,false,,
061-828-752-873-57X,Improvement of Chatbot in Trading System for SMEs by Using Deep Neural Network,,2019,conference proceedings article,2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),,IEEE,,Sathit Prasomphan,"This research presents a method for developing chatbots to serve their users. In general, these chatbots are used for answering questions in many businesses, providing customer information, providing train schedules, helping customer reservations, virtual assistants; serve as call centers to serve ten million customers automatically. A deep learning based conversational artificial intelligence technique was used as tools for learning conversation between machine and customer. In addition, the steps required are the technique used in conjunction with the convolution neural network technique by using Tensorflow training to improve the accuracy of these chatbots. From the experimental results, using deep learning for chatbots learning, the accuracy is better than the traditional model.",,,517,522,Deep learning; Machine learning; Artificial intelligence; Conversation; Conjunction (grammar); Chatbot; Computer science; Artificial neural network; Convolutional neural network,,,,,http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=8725745 http://xplorestaging.ieee.org/ielx7/8717973/8725611/08725745.pdf?arnumber=8725745,http://dx.doi.org/10.1109/icccbda.2019.8725745,,10.1109/icccbda.2019.8725745,2947821605,,1,016-647-542-440-122; 021-092-397-907-180; 024-276-714-908-263; 025-661-113-833-136; 033-695-243-724-371; 068-610-397-791-272; 077-899-984-222-177; 156-872-845-749-191,12,false,,
061-854-031-056-667,CFGPT: Chinese Financial Assistant with Large Language Model,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Jiangtong Li; Yuxuan Bian; Guoxuan Wang; Yang Lei; Dawei Cheng; Zhijun Ding; Changjun Jiang,"Large language models (LLMs) have demonstrated great potential in natural language processing tasks within the financial domain. In this work, we present a Chinese Financial Generative Pre-trained Transformer framework, named CFGPT, which includes a dataset~(CFData) for pre-training and supervised fine-tuning, a financial LLM~(CFLLM) to adeptly manage financial texts, and a deployment framework~(CFAPP) designed to navigate real-world financial applications. The CFData comprising both a pre-training dataset and a supervised fine-tuning dataset, where the pre-training dataset collates Chinese financial data and analytics, alongside a smaller subset of general-purpose text with 584M documents and 141B tokens in total, and the supervised fine-tuning dataset is tailored for six distinct financial tasks, embodying various facets of financial analysis and decision-making with 1.5M instruction pairs and 1.5B tokens in total. The CFLLM, which is based on InternLM-7B to balance the model capability and size, is trained on CFData in two stage, continued pre-training and supervised fine-tuning. The CFAPP is centered on large language models (LLMs) and augmented with additional modules to ensure multifaceted functionality in real-world application. Our codes are released at https://github.com/TongjiFinLab/CFGPT.",,,,,Computer science; Transformer; Finance; Financial services; Machine learning; Artificial intelligence; Natural language processing; Business; Engineering; Voltage; Electrical engineering,,,,,https://arxiv.org/abs/2309.10654,http://dx.doi.org/10.48550/arxiv.2309.10654,,10.48550/arxiv.2309.10654,,,0,,0,true,other-oa,green
062-282-710-348-990,Chatbots Vs Human Agents: A Comparative Study of Customer Satisfaction in the Financial Services Industry in Sri Lanka,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,M.S. Ishar Ali,,,,,,Sri lanka; Customer satisfaction; Business; Financial services; Marketing; Finance; Economics; Socioeconomics; Tanzania,,,,,,http://dx.doi.org/10.2139/ssrn.5292006,,10.2139/ssrn.5292006,,,0,,0,false,,
062-521-072-656-061,The financial benefits of digital technology adoption: Evidence from large language model,,2025,journal article,Research in International Business and Finance,02755319; 18783384,Elsevier BV,Netherlands,Cheng Li; Tao Li; Shengkun Zhao; Xuankai Zhao,,80,,103136,103136,Business; Economics; Finance,,,,Central University of Finance and Economics; National Social Science Fund of China; National Social Science Fund of China; Major Program of National Fund of Philosophy and Social Science of China,,http://dx.doi.org/10.1016/j.ribaf.2025.103136,,10.1016/j.ribaf.2025.103136,,,0,000-317-314-210-058; 001-095-144-401-665; 004-623-891-601-332; 004-638-352-444-122; 006-654-812-851-865; 017-457-587-896-536; 023-938-984-489-752; 025-335-907-210-260; 025-740-966-098-490; 026-216-316-110-799; 029-009-280-336-32X; 029-183-984-628-842; 029-929-924-812-435; 031-410-877-423-189; 032-248-615-558-503; 035-695-565-316-883; 036-160-861-488-036; 038-602-216-116-211; 038-863-170-046-961; 039-084-366-336-390; 039-632-224-190-503; 041-649-340-724-560; 043-952-224-261-145; 050-130-499-776-720; 052-558-268-233-614; 054-660-903-967-662; 055-464-638-548-309; 057-386-561-697-323; 062-609-516-509-65X; 063-637-909-999-199; 066-595-209-667-130; 067-084-251-208-095; 071-966-749-464-239; 072-971-003-248-807; 081-510-469-716-89X; 085-860-667-275-912; 090-235-537-312-09X; 094-611-827-820-65X; 097-831-471-117-528; 098-355-670-090-198; 100-736-422-180-505; 101-100-647-851-755; 104-628-547-166-421; 110-033-170-535-569; 110-194-117-396-683; 115-646-434-404-129; 118-769-671-898-707; 119-676-252-068-34X; 124-692-620-647-363; 128-727-413-419-889; 132-135-802-217-768; 132-314-137-802-214; 137-071-813-654-631; 137-211-774-200-005; 155-611-422-957-886; 158-185-717-893-736; 158-500-449-887-053; 164-944-611-106-425; 178-643-297-115-591; 188-730-715-206-321,0,false,,
062-788-333-335-50X,Chinese fine-grained financial sentiment analysis with large language models,2024-12-07,2024,journal article,Neural Computing and Applications,09410643; 14333058,Springer Science and Business Media LLC,Germany,Yinyu Lan; Yanru Wu; Wang Xu; Weiqiang Feng; Youhao Zhang,,37,30,24883,24892,Computational Science and Engineering; Computer science; Sentiment analysis; Natural language processing; Artificial intelligence; Machine learning,,,,,,http://dx.doi.org/10.1007/s00521-024-10603-6,,10.1007/s00521-024-10603-6,,,0,000-321-618-347-08X; 004-142-287-080-115; 004-853-485-857-424; 014-711-631-488-436; 018-866-913-622-957; 031-289-121-988-030; 035-876-619-354-109; 043-457-340-202-531; 049-019-515-186-480; 053-613-341-919-467; 064-435-031-799-666; 066-044-762-294-041; 067-946-624-100-658; 070-360-035-700-487; 088-672-515-974-141; 096-680-693-934-53X; 119-818-171-606-098; 137-548-243-845-043; 152-228-352-649-480; 181-432-462-344-434,5,false,,
063-511-635-521-505,Training LLMs for querying financial data with natural language prompts,2025-04-30,2025,journal article,World Journal of Advanced Engineering Technology and Sciences,25828266,GSC Online Press,,null Sasibhushan Rao Chanthati,"<jats:p>The abstract introduces the motivation behind this research: the rapid expansion of financial investment data and the need for Artificial Intelligence-driven solutions for accurate and real-time analysis. The research presents an open-source Large Language Model (LLM) fine-tuned on financial datasets to provide precise investment insights.; ; The LLM integrates MongoDB Atlas Vector Search to store and retrieve vector embeddings efficiently. This allows natural language querying of financial data.; ; The implementation leverages Hugging Face Transformers, Sentence-Transformers, and FastAPI, enabling a scalable framework for real-time financial queries and automated decision-making.; ; The system is deployed as an API, allowing seamless integration into financial platforms.; ; The research contributes to the open-source community by providing a robust AI-powered financial tool for analysts, traders, and researchers.</jats:p>",15,1,25,32,Training (meteorology); Natural (archaeology); Finance; Computer science; Business; Geography; Archaeology; Meteorology,,,,,,http://dx.doi.org/10.30574/wjaets.2025.15.1.0190,,10.30574/wjaets.2025.15.1.0190,,,0,,0,false,,
063-514-609-318-785,FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Yangyang Yu; Haohang Li; Zhi Chen; Yuechen Jiang; Yang Li; Denghui Zhang; Rong Liu; Jordan W. Suchow; Khaldoun Khashanah,"Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce \textsc{FinMem}, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, \textsc{FinMem}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare \textsc{FinMem} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, \textsc{FinMem} presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.",,,,,Computer science; Interpretability; Financial market; Trading strategy; Scalability; Artificial intelligence; Finance; Business; Database,,,,,https://arxiv.org/abs/2311.13743,http://dx.doi.org/10.48550/arxiv.2311.13743,,10.48550/arxiv.2311.13743,,,0,,3,true,cc-by,green
063-708-889-225-713,Parameter Efficient Instruction Tuning of LLMs for Financial Applications,2024-07-26,2024,journal article,,,,,Subhendu Khatuya,"XBRL tagging in financial texts involves categorizing entities into numerous labels, presenting challenges for state-of-the-art models. Financial reports like 10-Q and 10-K, which must be tagged with XBRL according to a taxonomy with thousands of labels. The FNXL dataset exemplifies this with 2,794 labels. Manual tagging is neither scalable nor cost-effective, necessitating automatic annotation methods. Additionally, summarizing long Earnings Call Transcripts (ECTs) is crucial for financial decision-making. The ECTSum dataset highlights challenges in automatic summarization, including a high compression ratio and documents exceeding typical LLM token limits. This study proposes novel methods for both XBRL tagging and ECT summarization.",,,8494,8495,Computer science,,,,,,,,,,,0,,0,false,,
064-236-814-735-667,Stock Price Trend Prediction using Emotion Analysis of Financial Headlines with Distilled LLM Model,2024-06-26,2024,conference proceedings article,Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,,ACM,,Rithesh Bhat; Bhanu Jain,"Capturing the volatility of stock prices helps individual traders, stock analysts, and institutions alike increase their returns in the stock market. Financial news headlines have been shown to have a significant effect on stock price mobility. Lately, many financial portals have restricted web scraping of stock prices and other related financial data of companies from their websites. In this study we demonstrate that emotion analysis of financial news headlines alone can be sufficient in predicting stock price movement, even in the absence of any financial data.",,,67,73,Stock (firearms); Stock price; Econometrics; Computer science; Economics; Engineering; Geology; Mechanical engineering; Paleontology; Series (stratigraphy),,,,,https://dl.acm.org/doi/pdf/10.1145/3652037.3652076 https://doi.org/10.1145/3652037.3652076,http://dx.doi.org/10.1145/3652037.3652076,,10.1145/3652037.3652076,,,1,057-040-097-643-496; 059-300-969-491-735; 081-081-928-966-538; 127-752-209-564-390,11,true,other-oa,hybrid
064-496-108-612-548,"Ploutos: Towards interpretable stock movement prediction with financial
  large language model",2024-02-18,2024,preprint,arXiv (Cornell University),,,,Hanshuang Tong; Jun Li; Ning Wu; Ming Gong; Dongmei Zhang; Qi Zhang,"Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates interpretable rationales. To generate accurate and faithful rationales, the training strategy of PloutosGPT leverage rearview-mirror prompting mechanism to guide GPT-4 to generate rationales, and a dynamic token weighting mechanism to finetune LLM by increasing key tokens weight. Extensive experiments show our framework outperforms the state-of-the-art methods on both prediction accuracy and interpretability.",,,,,Movement (music); Stock (firearms); Economics; Finance; Econometrics; Computer science; Geography; Art; Archaeology; Aesthetics,,,,,https://arxiv.org/abs/2403.00782,http://dx.doi.org/10.48550/arxiv.2403.00782,,10.48550/arxiv.2403.00782,,,0,,0,true,,green
064-529-523-086-838,Complex Forecasting and Investment Strategy Optimization Via Chain-of-Thought of Large Language Models,2025-01-01,2025,preprint,,,Elsevier BV,,Mengzhu Guo; Meiqun Yin,,,,,,Investment (military); Chain (unit); Computer science; Economics; Political science; Physics; Astronomy; Politics; Law,,,,,,http://dx.doi.org/10.2139/ssrn.5186607,,10.2139/ssrn.5186607,,,0,,0,false,,
064-539-304-558-478,"WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off",2024-03-06,2024,preprint,arXiv (Cornell University),,,,Eva Giboulot; Teddy Furon,"Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.",,,,,Watermark; Robustness (evolution); Computer science; Digital watermarking; Artificial intelligence; Biology; Image (mathematics); Biochemistry; Gene,,,,,https://arxiv.org/abs/2403.04808,http://dx.doi.org/10.48550/arxiv.2403.04808,,10.48550/arxiv.2403.04808,,,0,,0,true,cc-by,green
064-583-832-683-158,FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Xin Guo; Haotian Xia; Zhaowei Liu; Hanyang Cao; Zhi Yang; Zhiqiang Liu; Sizhe Wang; Jinyi Niu; Chuqi Wang; Yanhui Wang; Xiaolong Liang; Xiaoming Huang; Bing Zhu; Zhongyu Wei; Yun Chen; Weining Shen; Liwen Zhang,"Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial knowledge evaluation benchmark, utilizing data of mock exams and covering a wide range of evaluated LLMs.",,,,,Benchmark (surveying); Certificate; Finance; Domain (mathematical analysis); Computer science; Business; Geography; Theoretical computer science; Mathematical analysis; Mathematics; Geodesy,,,,,https://arxiv.org/abs/2308.09975,http://dx.doi.org/10.48550/arxiv.2308.09975,,10.48550/arxiv.2308.09975,,,0,,0,true,other-oa,green
064-653-811-375-608,The State of the Art of Large Language Models on Chartered Financial Analyst Exams,,2024,conference proceedings article,Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track,,Association for Computational Linguistics,,Mahmoud Mahfouz; Ethan Callanan; Mathieu Sibue; Antony Papadimitriou; Zhiqiang Ma; Xiaomo Liu; Xiaodan Zhu,,,,1068,1082,State (computer science); Computer science; Programming language; Accounting; Business,,,,,,http://dx.doi.org/10.18653/v1/2024.emnlp-industry.80,,10.18653/v1/2024.emnlp-industry.80,,,0,,2,false,,
064-694-189-637-399,Financial text analysis and credit risk assessment using a GPT-4 and improved BERT fusion model.,2025-11-18,2025,journal article,PloS one,19326203,Public Library of Science (PLoS),United States,Huirong Tan; Yanruixue Xie,"This study aims to improve the identification of potential credit risks in unstructured financial texts. It addresses the core problem of financial text analysis and credit risk assessment by proposing a hybrid model that combines the generative semantic understanding of Generative Pre-trained Transformer-4 (GPT-4) with the enhanced feature extraction of Bidirectional Encoder Representations from Transformers (BERT). To overcome the limitations of traditional methods-such as weak contextual reasoning in long texts, insufficient recognition of industry-specific terminology, and implicit credit risk expressions-the model incorporates a financial dictionary enhancement module and a named entity recognition (NER) component. GPT-4 is leveraged for prompt-based generation to extract latent risk information from complex texts, including annual reports. A dual-model semantic fusion mechanism with attention weighting constructs a multi-level risk assessment system that integrates contextual understanding, industry adaptability, and interpretability. Experiments on multiple publicly available financial datasets and real-world annual reports demonstrate the model's effectiveness. Results show that the proposed approach outperforms representative baseline models in accuracy, adaptability, and interpretability. This work carries both theoretical and practical significance for research at the intersection of financial technology and natural language processing.",20,11,e0336217,,,,,,,,http://dx.doi.org/10.1371/journal.pone.0336217,41252425,10.1371/journal.pone.0336217,,PMC12626313,0,021-427-583-925-691; 022-533-926-007-717; 038-999-746-929-322; 041-327-058-753-366; 043-104-736-690-961; 051-745-531-135-577; 053-385-392-118-297; 060-133-356-807-477; 079-426-446-189-931; 085-577-890-801-459; 090-581-738-186-140; 095-322-282-397-382; 095-487-201-672-019; 099-575-972-149-82X; 115-398-550-843-591; 115-733-547-304-20X; 131-030-459-473-790; 137-946-747-237-598; 142-659-165-343-154; 147-426-793-848-609; 155-740-683-873-630; 158-835-653-290-62X; 193-222-672-874-089; 194-741-935-657-475,1,true,cc-by,gold
064-938-897-897-085,Cost–effective use of chatbot in the activities of a trading company,2022-03-06,2022,journal article,Entrepreneur's Guide,2687136x; 20739885,Publishing Agency Science and Education,,Y. V. Frolov; A. T. Nevolko; O. I. Yazykova,"The article discusses the processes of tender and procurement activities that are promising for automation in a trading company. The types of activities of a specialist in tender activities are analyzed. Using the visual editor of the ROBIN platform, an algorithm for the operation of an automated system is proposed. Calculations showed that by automating the main processes associated with the participation of the company in tenders, it is possible to free up the working time of specialists and gain an economic effect of about 250 thousand rubles.",15,1,91,96,Call for bids; Procurement; Automation; Business; Tender offer; Chatbot; Computer science; Industrial organization; Operations research; Operations management; Commerce; Marketing; World Wide Web; Finance; Engineering; Mechanical engineering; Corporate governance; Shareholder,trading company; tender activity; ROBIN platform; economic effect,,,,https://www.pp-mag.ru/jour/article/download/1609/1576 https://doi.org/10.24182/2073-9885-2022-15-1-91-96,http://dx.doi.org/10.24182/2073-9885-2022-15-1-91-96,,10.24182/2073-9885-2022-15-1-91-96,,,0,,0,true,cc-by,gold
065-428-026-687-302,A Study on the Financial Chatbot Interaction Design based on the Companionship & Ritual Elements to Prevent Emotional Investment of Individuals in the 2030 Generation,2021-09-30,2021,journal article,Journal of the HCI Society of Korea,19760671; 26717611,The HCI Society of Korea,,Seung-Heon Lee; Chaieun Park; Hyeon-Min Kang; Jong-Wha Lee; Kyung-Won Baek; Jin-Woo Kim,,16,3,5,17,Interaction design; Business; Marketing; Investment (macroeconomics); Chatbot; Interpersonal relationship,,,,,http://dx.doi.org/10.17210/jhsk.2021.09.16.3.5,http://dx.doi.org/10.17210/jhsk.2021.09.16.3.5,,10.17210/jhsk.2021.09.16.3.5,3207960328,,0,,0,false,,
065-451-896-008-284,Potential application of large language models in the financial field,2025-11-20,2025,journal article,Advances in Engineering Technology Research,27901688,Madison Academic Press,,Shuai Zhang,"<jats:p> Recent advancements in artificial intelligence have led to the research, development, and implementation of various new technologies. Large language models (LLMs) have emerged as a prominent research focus across multiple disciplines due to their remarkable capabilities in data collection, analysis, scenario simulation, and programming. These capabilities hold significant potential for applications in the financial sector. By leveraging these strengths, economic researchers, financial institutions, and individual investors can more effectively strike a balance between risk and return. The financial market contains vast amounts of information, ranging from essential data to peripheral materials, which can be challenging for humans to process comprehensively. Large language models can help financial professionals extract key insights from financial markets and make informed decisions.</jats:p>",15,1,1778,,,,,,,,http://dx.doi.org/10.56028/aetr.15.1.1778.2025,,10.56028/aetr.15.1.1778.2025,,,0,,0,false,,
065-623-586-221-205,A Multi-agent System Based On LLM For Trading Financial Assets,2025-02-20,2025,journal article,Ovidius University Annals. Economic Sciences Series,23933119; 23933127,Universitatea Ovidius din Constanta,,Simona-Vasilica Oprea; Adela Bara,,24,2,613,623,Business; Finance; Financial system,,,,,,http://dx.doi.org/10.61801/ouaess.2024.2.80,,10.61801/ouaess.2024.2.80,,,0,,0,true,cc-by-nc-sa,gold
065-717-737-616-069,"Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in
  Large Language Models",2025-02-17,2025,preprint,arXiv (Cornell University),,,,Yingshui Tan; Yilei Jiang; Yanshi Li; Jiaheng Liu; Xingyuan Bu; Wenbo Su; Xiangyu Yue; Xiaoyong Zhu; Bo Zheng,"Fine-tuning large language models (LLMs) based on human preferences, commonly achieved through reinforcement learning from human feedback (RLHF), has been effective in improving their performance. However, maintaining LLM safety throughout the fine-tuning process remains a significant challenge, as resolving conflicts between safety and helpfulness can be non-trivial. Typically, the safety alignment of LLM is trained on data with safety-related categories. However, our experiments find that naively increasing the scale of safety training data usually leads the LLMs to an ``overly safe'' state rather than a ``truly safe'' state, boosting the refusal rate through extensive safety-aligned data without genuinely understanding the requirements for safe responses. Such an approach can inadvertently diminish the models' helpfulness. To understand the phenomenon, we first investigate the role of safety data by categorizing them into three different groups, and observe that each group behaves differently as training data scales up. To boost the balance between safety and helpfulness, we propose an Equilibrate RLHF framework including a Fine-grained Data-centric (FDC) approach that achieves better safety alignment even with fewer training data, and an Adaptive Message-wise Alignment (AMA) approach, which selectively highlight the key segments through a gradient masking strategy. Extensive experimental results demonstrate that our approach significantly enhances the safety alignment of LLMs while balancing safety and helpfulness.",,,,,Helpfulness; Computer science; Business; Psychology; Social psychology,,,,,https://arxiv.org/abs/2502.11555,http://dx.doi.org/10.48550/arxiv.2502.11555,,10.48550/arxiv.2502.11555,,,0,,0,true,,green
065-797-642-249-211,Large Language Models in Accounting and Financial Research: A Review of Applications in Accounting-related Text Analysis,2025-06-13,2025,journal article,"Advances in Economics, Management and Political Sciences",27541169; 27541177,EWA Publishing,,Yan Hao,"<jats:p>With the rapid advancement of large language models (LLM) technologies, new technical support and research paradigms have emerged for traditional accounting and finance studies. Despite these opportunities, scholars in the fields of accounting and finance often encounter challenges when navigating the extensive and complex domain knowledge of large language models, as well as the ever-evolving academic literature. To address this gap, this paper conducts a qualitative survey on the applications of large language models in early accounting and finance researches. This paper is structured into three main sections. First, it delves into the definition, underlying principles, and developmental trajectory of large language models. Second, it synthesizes the latest research on large language models applications in accounting and finance, categorizing these studies into three major domains, including sentiment analysis, report analysis and practical works. Finally, the paper highlights emerging trends and potential research directions, aiming to provide a comprehensive guide for future scholarly exploration in this dynamic field. Although it still has many shortcomings such as high cost and black box of process, the application of large language modeling provides a new technical support for text analysis in accounting and financial research, and also improves the actual efficiency of accounting and auditing.</jats:p>",189,1,55,60,Accounting; Accounting research; Financial accounting; Accounting information system; Computer science; Business,,,,,,http://dx.doi.org/10.54254/2754-1169/2025.bl24036,,10.54254/2754-1169/2025.bl24036,,,0,,0,false,,
065-808-703-821-850,Predicting Portfolio Price Crashes in the Vietnamese Stock Market Using Large Language Models and Deep Learning Models with Temporal Relational Reasoning,2025-11-06,2025,conference proceedings article,2025 17th International Conference on Knowledge and System Engineering (KSE),,IEEE,,Minh-Huong Nguyen; Thanh-Tam Do Thi; Hong-Viet Tran; Van-Tan Bui; Phuong-Thai Nguyen,,,,1,6,,,,,,,http://dx.doi.org/10.1109/kse68178.2025.11309645,,10.1109/kse68178.2025.11309645,,,0,012-393-409-984-54X; 019-729-680-880-124; 024-481-362-055-895; 037-429-954-122-389; 060-890-267-325-909; 061-039-512-735-504; 062-529-238-466-118; 086-143-324-444-942; 124-158-980-599-57X; 135-881-420-791-717; 189-780-492-464-500,0,false,,
065-974-635-780-500,FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making,,2024,conference proceedings article,Advances in Neural Information Processing Systems 37,,"Neural Information Processing Systems Foundation, Inc. (NeurIPS)",,Yupeng Cao; Zhi Chen; Zhenyu Cui; Zhiyang Deng; Yueru He; Jimin Huang; Yuechen Jiang; Dong Li; Haohang Li; Rong Liu; Koduvayur Subbalakshmi; Jordan Suchow; Qianqian Xie; Guojun Xiong; Zhaozhuo Xu; Zhiyuan Yao; Yangyang Yu; Denghui Zhang,,,,137010,137045,,,,,,,http://dx.doi.org/10.52202/079017-4354,,10.52202/079017-4354,,,0,,4,false,,
066-337-964-894-415,EU trade agreements and external differentiation: a large language model approach,2024-11-21,2024,journal article,West European Politics,01402382; 17439655,Routledge,United Kingdom,Andreas Dür,"The EU's trade agreements with third countries exhibit varying degrees of alignment with intra-EU rules. What factors explain this external differentiation? This article answers this question by relying on a large language model to measure the alignment of 62 EU trade agreements with intra-EU rules at the level of individual topics, such as trade in goods, intellectual property rights, or sustainable development. The results indicate that this alignment is higher for third countries (1) with large trade flows with the EU; (2) with an EU membership perspective; and (3) with a medium level of regulatory capacity. These associations are particularly pronounced for provisions tackling behind-the-border trade barriers. These findings contribute to the literature on external differentiation and advance our understanding of EU trade policy and preferential trade agreements.",,,1,27,International trade; External trade; International economics; Economics; Product differentiation; Business; Microeconomics; Cournot competition,,,,,,,,,,,0,,0,true,cc-by-nc-nd,hybrid
066-657-731-963-814,¿Habla Español? Do Financial Aid Offices Use Chatbots…and Can They Speak Spanish?,,2021,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,ZW Taylor; Linda Eguiluz; Paige Erin Wheeler,,,,,,Finance; Higher education; Political science; Converse; Chatbot; FAFSA; Internet users,,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3861837 https://autopapers.ssrn.com/sol3/papers.cfm?abstract_id=3861837,http://dx.doi.org/10.2139/ssrn.3861837,,10.2139/ssrn.3861837,3211829679,,0,,0,false,,
066-691-860-031-967,GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Udit Gupta,"Annual Reports of publicly listed companies contain vital information about their financial health which can help assess the potential impact on Stock price of the firm. These reports are comprehensive in nature, going up to, and sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for a single firm, let alone the whole universe of firms that exist. Over the years, financial experts have become proficient in extracting valuable information from these documents relatively quickly. However, this requires years of practice and experience. This paper aims to simplify the process of assessing Annual Reports of all the firms by leveraging the capabilities of Large Language Models (LLMs). The insights generated by the LLM are compiled in a Quant styled dataset and augmented by historical stock price data. A Machine Learning model is then trained with LLM outputs as features. The walkforward test results show promising outperformance wrt S&P500 returns. This paper intends to provide a framework for future work in this direction. To facilitate this, the code has been released as open source.",,,,,Stock (firearms); Computer science; Annual report; Stock price; Open source; Data science; Finance; Business; Software; Engineering; Programming language; Mechanical engineering; Paleontology; Series (stratigraphy); Biology,,,,,https://arxiv.org/abs/2309.03079,http://dx.doi.org/10.48550/arxiv.2309.03079,,10.48550/arxiv.2309.03079,,,0,,2,true,cc-by,green
066-752-119-925-251,FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis,2024-01-01,2024,preprint,arXiv (Cornell University),,,,Chao Zhang; Yuren Mao; Yijiang Fan; Yu Mi; Yunjun Gao; Lu Chen; Dongfang Lou; Jinshu Lin,"Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because, financial professionals may not well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Extensive experimental results on BULL demonstrate that FinSQL achieves the state-of-the-art Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to 36.64% performance improvement in scenarios requiring few-shot cross-database model transfer.",,,,,SQL; Computer science; Database; Benchmark (surveying); Macro; Finance; Programming language; Business; Geography; Geodesy,,,,,https://arxiv.org/abs/2401.10506,http://dx.doi.org/10.48550/arxiv.2401.10506,,10.48550/arxiv.2401.10506,,,0,,1,true,other-oa,green
067-191-067-911-905,Narratives from GPT-derived networks of news and a link to financial markets dislocations.,2024-03-17,2024,news,International journal of data science and analytics,2364415x; 23644168,Springer Science and Business Media LLC,Switzerland,Deborah Miori; Constantin Petrov,"We introduce a novel framework to study the dynamics of news narratives, by leveraging GPT3.5 advanced text analysis capabilities and graph theory. In particular, we focus on a corpus of economic articles from The Wall Street Journal and dynamically extract the main topics of discussion over time, in a completely systematic and scalable fashion. As a simple application of the suggested approach, we show how the structure of such topics of discussion has a statistically significant relationship with the contemporaneous state of financial markets, which can be used to construct an investment strategy or monitor financial risks. Our work is based on the intrinsic ability of GPT models to track the context of sentences within a document, thanks to which we can accurately extract a ranking of the most important entities discussed within each article, and evaluate their entity-specific sentiments. Then, we create a graph for each week of data, in which nodes are the entities retrieved and edges are built from the co-occurrence of such entities within articles. Graph centrality measures are computed over time to track the most representative keywords of topics of discussion, which result in an accurate summary view of the evolution of economic narratives. Fuzzy community detection is finally used to cluster linked entities into a more detailed representation of topics. Such groups of entities are mapped to the related journal articles, which are in turn summarised to reach a highly nuanced and interpretable view of the topics discussed within each week. Linking the features of these topics to the relevant financial market time series, we find that high fragmentation within our networks' communities relates to moments of financial markets dislocations (i.e. dates with unusually high volatility across asset classes). This result should thus motivate stronger effort within financial research to move beyond ubiquitous sentiment analysis of news and delve deeper into broader and more holistic studies of textual data.",20,2,1105,1129,Computer science; Centrality; Construct (python library); Narrative; Data science; Context (archaeology); Scalability; Graph; Representation (politics); Information retrieval; Artificial intelligence; Data mining; Finance; Theoretical computer science; Political science; Business; Linguistics; History; Philosophy; Mathematics; Archaeology; Combinatorics; Database; Politics; Law; Programming language,Large Language Models; Market dislocations; Network analysis,,,Engineering and Physical Sciences Research Council,https://link.springer.com/content/pdf/10.1007/s41060-024-00516-x.pdf https://doi.org/10.1007/s41060-024-00516-x,http://dx.doi.org/10.1007/s41060-024-00516-x,40852117,10.1007/s41060-024-00516-x,,PMC12367935,0,000-524-338-757-738; 000-810-519-562-283; 006-781-789-967-625; 006-932-740-174-70X; 008-256-017-635-425; 008-930-280-522-163; 011-598-948-848-961; 012-669-946-569-944; 013-866-087-413-94X; 015-146-519-088-544; 015-692-170-714-052; 016-467-739-536-920; 018-708-439-679-636; 019-706-336-089-489; 021-852-924-092-024; 025-352-207-023-08X; 026-294-977-463-130; 039-255-997-298-072; 042-152-947-297-801; 043-626-084-101-663; 044-758-206-909-304; 048-296-067-882-050; 070-758-582-977-063; 092-722-776-849-098; 098-428-380-981-106; 099-556-209-818-563; 101-562-225-779-462; 114-624-351-788-784; 116-956-385-925-402; 124-090-804-896-251; 156-853-163-550-018; 168-485-640-067-096; 168-868-974-520-805; 178-669-202-372-33X,0,true,cc-by,hybrid
067-247-358-060-856,FinDKG: Dynamic Knowledge Graphs with Large Language Models for Detecting Global Trends in Financial Markets,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Xiaohui Victor Li; Francesco Sanna Passino,"Dynamic knowledge graphs (DKGs) are popular structures to express different types of connections between objects over time. They can also serve as an efficient mathematical tool to represent information extracted from complex unstructured data sources, such as text or images. Within financial applications, DKGs could be used to detect trends for strategic thematic investing, based on information obtained from financial news articles. In this work, we explore the properties of large language models (LLMs) as dynamic knowledge graph generators, proposing a novel open-source fine-tuned LLM for this purpose, called the Integrated Contextual Knowledge Graph Generator (ICKG). We use ICKG to produce a novel open-source DKG from a corpus of financial news articles, called FinDKG, and we propose an attention-based GNN architecture for analysing it, called KGTransformer. We test the performance of the proposed model on benchmark datasets and FinDKG, demonstrating superior performance on link prediction tasks. Additionally, we evaluate the performance of the KGTransformer on FinDKG for thematic investing, showing it can outperform existing thematic ETFs.",,,573,581,Financial market; Computer science; Knowledge graph; Artificial intelligence; Business; Finance,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698603 https://doi.org/10.1145/3677052.3698603 http://arxiv.org/pdf/2407.10909 http://arxiv.org/abs/2407.10909,http://dx.doi.org/10.1145/3677052.3698603,,10.1145/3677052.3698603,,,0,000-608-383-851-000; 005-088-921-681-092; 010-761-556-060-295; 012-393-409-984-54X; 016-489-127-449-970; 019-588-146-751-334; 019-729-680-880-124; 027-135-917-068-508; 035-147-833-064-444; 036-979-852-259-747; 042-152-947-297-801; 044-737-522-085-105; 045-900-098-666-069; 059-238-652-395-142; 069-619-442-481-917; 084-208-779-084-455; 085-310-285-569-652; 093-335-333-853-916; 124-232-191-551-576; 126-417-835-942-913,6,true,,bronze
067-327-794-320-511,Research on Financial Big Data Collection and Intelligent Decision-Making System Based on Multimodal Large Language Model,2024-08-10,2024,conference proceedings article,2024 International Conference on Fuzzy Theory and Its Applications (iFUZZY),,IEEE,,Yiping Jiang; Zhijing Li; C. L. Philip Chen,,,,1,6,Computer science; Data collection; Big data; Data modeling; Data science; Artificial intelligence; Software engineering; Data mining; Statistics; Mathematics,,,,,,http://dx.doi.org/10.1109/ifuzzy63051.2024.10662884,,10.1109/ifuzzy63051.2024.10662884,,,0,017-880-552-274-954; 018-707-270-316-551; 019-282-248-583-819; 049-671-541-564-147; 063-881-861-185-296; 089-435-153-785-024; 192-806-908-794-847,2,false,,
067-529-793-860-239,"LLM Trading: Analysis of LLM Agent Behavior in Experimental Asset
  Markets",2025-02-18,2025,preprint,arXiv (Cornell University),,,,Thomas Henning; Siddhartha M. Ojha; Ross Spoon; Jiatong Han; Colin F. Camerer,"This paper explores how Large Language Models (LLMs) behave in a classic experimental finance paradigm widely known for eliciting bubbles and crashes in human participants. We adapt an established trading design, where traders buy and sell a risky asset with a known fundamental value, and introduce several LLM-based agents, both in single-model markets (all traders are instances of the same LLM) and in mixed-model ""battle royale"" settings (multiple LLMs competing in the same market). Our findings reveal that LLMs generally exhibit a ""textbook-rational"" approach, pricing the asset near its fundamental value, and show only a muted tendency toward bubble formation. Further analyses indicate that LLM-based agents display less trading strategy variance in contrast to humans. Taken together, these results highlight the risk of relying on LLM-only data to replicate human-driven market phenomena, as key behavioral features, such as large emergent bubbles, were not robustly reproduced. While LLMs clearly possess the capacity for strategic decision-making, their relative consistency and rationality suggest that they do not accurately mimic human market dynamics.",,,,,Asset (computer security); Financial economics; Business; Monetary economics; Economics; Computer science; Computer security,,,,,https://arxiv.org/abs/2502.15800,http://dx.doi.org/10.48550/arxiv.2502.15800,,10.48550/arxiv.2502.15800,,,0,,0,true,,green
067-926-704-440-152,Profitable Trade-Off Between Memory and Performance In Multi-Domain Chatbot Architectures,2021-01-01,2021,preprint,arXiv (Cornell University),,,,D. Emre Taşar; Şükrü Ozan; M. Fatih Akca; Oğuzhan Ölmez; Semih Gülüm; Seçilay Kutal; Ceren Belhan,"Text classification problem is a very broad field of study in the field of natural language processing. In short, the text classification problem is to determine which of the previously determined classes the given text belongs to. Successful studies have been carried out in this field in the past studies. In the study, Bidirectional Encoder Representations for Transformers (BERT), which is a frequently preferred method for solving the classification problem in the field of natural language processing, is used. By solving classification problems through a single model to be used in a chatbot architecture, it is aimed to alleviate the load on the server that will be created by more than one model used for solving more than one classification problem. At this point, with the masking method applied during the estimation of a single BERT model, which was created for classification in more than one subject, the estimation of the model was provided on a problem-based basis. Three separate data sets covering different fields from each other are divided by various methods in order to complicate the problem, and classification problems that are very close to each other in terms of field are also included in this way. The dataset used in this way consists of five classification problems with 154 classes. A BERT model containing all classification problems and other BERT models trained specifically for the problems were compared with each other in terms of performance and the space they occupied on the server.",,,,,Computer science; Chatbot; Field (mathematics); Artificial intelligence; Encoder; Architecture; Natural language processing; Point (geometry); Data mining; Machine learning; Mathematics; Art; Geometry; Visual arts; Operating system; Pure mathematics,,,,,https://arxiv.org/abs/2111.03963,http://dx.doi.org/10.48550/arxiv.2111.03963,,10.48550/arxiv.2111.03963,,,0,,0,true,other-oa,green
068-170-373-123-839,GenAI and LLM for Financial Institutions: A Corporate Strategic Survey,2024-01-01,2024,preprint,,,Elsevier BV,,Jun Xu,,,,,,Business; Finance; Accounting; Financial system,,,,,,http://dx.doi.org/10.2139/ssrn.4988118,,10.2139/ssrn.4988118,,,0,004-771-690-834-863; 032-614-010-344-147; 054-401-065-511-159; 099-231-777-860-115; 138-902-537-965-660,2,false,,
068-239-839-905-123,Designing intelligent virtual assistants and artificial intelligence chatbots for enhanced financial service delivery and customer retention,2025-06-10,2025,book chapter,The Future of Financial IT: Agentic Artificial Intelligence and Intelligent Infrastructure in Modern Banking,,Deep Science Publishing,,Bharath Somu,"<jats:p>Financial markets are one of the strongest and most diversified aspects and a necessary part of the world economy, ranging from providing simple products like bank accounts to very complex products where small changes in the conditions may risk the wellbeing of the company and the economy. The ties of financial organizations with big corporations and with the political circuit make them necessary actors in the economy; hence their services have to be reliable, transparent, and affordable. At the same time, these services move huge amounts of money, in which individuals have invested their entire life, and should therefore be safe from hackers or incompetent personnel. It is a well-known fact that the consumers do not want to have any reply in the recovery of the damage, and that they require the services to run flawlessly.</jats:p>",,,107,122,Customer service; Chatbot; Business; Service (business); Service delivery framework; Computer science; Engineering management; Knowledge management; Engineering; Artificial intelligence; Marketing,,,,,,http://dx.doi.org/10.70593/978-93-49910-62-1_9,,10.70593/978-93-49910-62-1_9,,,0,,0,false,,
068-307-522-655-367,Using Large Language Models for Financial Advice,2024-01-01,2024,preprint,,,Elsevier BV,,Christian Fieberg; Lars Hornuf; David Streich,,,,,,Advice (programming); Finance; Business; Computer science; Programming language,,,,,,http://dx.doi.org/10.2139/ssrn.4850039,,10.2139/ssrn.4850039,,,0,007-570-784-407-502; 008-250-861-392-767; 009-122-864-785-531; 018-508-064-179-38X; 021-212-598-219-644; 023-379-578-217-307; 023-510-281-484-827; 023-853-922-616-880; 027-163-358-030-949; 033-799-091-679-810; 040-660-876-513-914; 042-492-265-651-624; 044-961-379-196-14X; 049-014-021-030-838; 051-201-561-722-62X; 052-027-687-931-983; 061-056-306-117-590; 061-173-128-168-696; 066-508-279-094-544; 072-878-583-167-977; 077-444-269-577-533; 077-882-683-245-032; 078-934-110-822-38X; 082-600-317-526-224; 083-989-154-270-555; 086-260-434-815-196; 087-044-222-911-370; 100-446-896-445-856; 100-707-306-967-308; 116-639-705-671-271; 116-653-450-604-253; 117-649-902-526-853; 121-323-404-465-87X; 121-570-640-268-403; 122-479-959-461-132; 128-079-345-767-474; 130-185-490-312-282; 132-412-858-453-658; 137-204-486-519-26X; 140-187-514-139-093; 147-185-050-695-707; 147-719-403-835-896; 153-731-994-523-956; 153-939-232-752-691; 154-513-179-142-971; 155-228-766-925-763; 157-787-315-927-958; 158-926-995-241-258; 171-389-867-596-062; 174-752-787-115-639; 181-432-462-344-434; 183-770-364-175-803; 196-395-945-746-897,2,false,,
068-463-522-546-552,Opportunities vs. Risks: Exploring Automatic Annotation of Financial Polarity Biases via Large Language Models,2025-09-29,2025,conference proceedings article,"Anais do XIII Symposium on Knowledge Discovery, Mining and Learning (KDMiLe 2025)",,Sociedade Brasileira de Computação - SBC,,Viviane Romero; Gabriel Assis; Jonnathan Carvalho; Paulo Mann; Aline Paes,"<jats:p>The financial market encompasses investors with distinct risk profiles - conservative, moderate and aggressive — each reflecting different attitudes toward gains and losses. These profiles influence the interpretation of financial news, particularly regarding the comparison between the labels ""positive""/""negative"" and the categories ""opportunity""/""risk"". Although these pairs of terms may initially appear equivalent, their practical application reveals notable inconsistencies. This paper employs large language models (LLMs) to annotate financial news, investigating whether such models capture the biases associated with each investor profile. We analyze the correlation between ""opportunity"" and ""positive"" and between ""risk"" and ""negative"" in the labels generated by the models. Furthermore, we examine whether, in the absence of explicit instructions regarding risk preference, the LLMs implicitly adopt a default bias when performing sentiment analysis on financial texts. Our findings provide insights into how risk profiles influence model behavior and suggest directions for improving both the personalization and accuracy of polarity detection in financial news analysis.</jats:p>",,,1,8,,,,,,,http://dx.doi.org/10.5753/kdmile.2025.247742,,10.5753/kdmile.2025.247742,,,0,002-608-750-650-394; 006-399-176-500-152; 016-774-344-390-234; 028-030-994-875-077; 034-313-875-821-065; 039-110-388-312-10X; 068-426-647-064-116; 080-401-913-719-722; 092-777-404-685-748; 101-997-359-718-799; 113-256-775-452-798; 131-043-585-684-057; 137-334-909-376-199; 155-124-422-705-728; 157-307-409-184-600; 163-920-021-517-416; 163-979-407-953-145; 199-870-113-581-216,0,false,,
069-082-428-499-080,"The Structure of Financial Equity Research Reports -- Identification of
  the Most Frequently Asked Questions in Financial Analyst Reports to Automate
  Equity Research Using Llama 3 and GPT-4",2024-07-04,2024,preprint,arXiv (Cornell University),,,,Adria Pop; Jan Spörer,"This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled ""text-extractable"" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.",,,,,Equity (law); Identification (biology); Finance; Business; Actuarial science; Accounting; Political science; Botany; Law; Biology,,,,,https://arxiv.org/abs/2407.18327,http://dx.doi.org/10.48550/arxiv.2407.18327,,10.48550/arxiv.2407.18327,,,0,,0,true,,green
069-204-884-170-45X,Uncovering Inconsistencies and Contradictions in Financial Reports using Large Language Models,2023-12-15,2023,conference proceedings article,2023 IEEE International Conference on Big Data (BigData),,IEEE,,Tobias Deußer; David Leonhard; Lars Hillebrand; Armin Berger; Mohamed Khaled; Sarah Heiden; Tim Dilmaghani; Bernd Kliem; Rüdiger Loitz; Christian Bauckhage; Rafet Sifa,"Correct identification and correction of contradictions and inconsistencies within financial reports constitute a fundamental component of the audit process. To streamline and automate this critical task, we introduce a novel approach leveraging large language models and an embedding-based paragraph clustering methodology. This paper assesses our approach across three distinct datasets, including two annotated datasets and one unannotated dataset, all within a zero-shot framework. Our findings reveal highly promising results that significantly enhance the effectiveness and efficiency of the auditing process, ultimately reducing the time required for a thorough and reliable financial report audit.",,,2814,2822,Computer science; Audit; Paragraph; Process (computing); Task (project management); Identification (biology); Embedding; Cluster analysis; Component (thermodynamics); Data mining; Accounting; Artificial intelligence; Programming language; Engineering; Botany; Physics; Systems engineering; World Wide Web; Business; Biology; Thermodynamics,,,,Ministry of Education,http://publica.fraunhofer.de/bitstreams/ff5b4b84-0943-40ab-9548-7c4e9e71330f/download https://publica.fraunhofer.de/handle/publica/459340,http://dx.doi.org/10.1109/bigdata59044.2023.10386673,,10.1109/bigdata59044.2023.10386673,,,0,007-087-814-979-131; 009-429-532-265-827; 016-750-756-337-724; 028-177-414-077-965; 037-297-734-603-234; 044-269-083-119-080; 056-585-364-780-633; 061-880-191-179-008; 062-528-338-395-471; 065-289-123-278-060; 066-264-056-530-646; 067-789-398-367-12X; 068-235-843-619-56X; 087-804-522-078-017; 088-157-603-923-110; 090-090-375-031-572; 098-328-166-773-244; 112-817-312-216-36X; 113-235-191-398-422; 113-961-214-877-335; 118-295-561-505-827; 125-007-830-851-395; 133-845-012-843-96X; 139-695-524-263-341; 145-631-585-677-091; 154-572-670-408-768; 162-788-294-201-095; 177-161-644-026-465; 183-909-034-957-216; 189-973-804-567-981; 198-366-138-874-075,5,true,,green
069-451-893-305-334,Large Language Model - Based Conversational Recommender System for Personalizing Long-term Cryptocurrency Portfolio Recommendation,2025-07-30,2025,conference proceedings article,2025 International Conference on Information and Communication Technology (ICoICT),,IEEE,,Alfara Nafi Dinara; Z. K. A. Baizal,,,,1,7,Recommender system; Computer science; Cryptocurrency; Term (time); Portfolio; Language model; Information retrieval; World Wide Web; Artificial intelligence; Finance; Business; Physics; Quantum mechanics,,,,,,http://dx.doi.org/10.1109/icoict66265.2025.11193014,,10.1109/icoict66265.2025.11193014,,,0,009-567-850-304-698; 015-319-479-154-297; 021-264-544-972-600; 023-263-156-298-755; 035-725-842-109-36X; 041-890-828-763-820; 051-044-730-730-49X; 063-693-254-034-631; 065-429-882-286-813; 074-706-398-942-208; 080-456-402-575-454; 087-287-720-923-768; 119-477-524-413-45X; 196-523-323-110-29X,0,false,,
069-806-883-352-09X,"Enhancing TinyBERT for Financial Sentiment Analysis Using GPT-Augmented
  FinBERT Distillation",2024-09-19,2024,preprint,arXiv (Cornell University),,,,Graison Jos Thomas,"In the rapidly evolving field of financial sentiment analysis, the efficiency and accuracy of predictive models are critical due to their significant impact on financial markets. Transformer based models like BERT and large language models (LLMs) like GPT-4, have advanced NLP tasks considerably. Despite their advantages, BERT-based models face challenges with computational intensity in edge computing environments, and the substantial size and compute requirements of LLMs limit their practical deployment. This study proposes leveraging the generative capabilities of LLMs, such as GPT-4 Omni, to create synthetic, domain-specific training data. This approach addresses the challenge of data scarcity and enhances the performance of smaller models by making them competitive with their larger counterparts. The research specifically aims to enhance FinBERT, a BERT model fine-tuned for financial sentiment analysis, and develop TinyFinBERT, a compact transformer model, through a structured, two-tiered knowledge distillation strategy. Using data augmented by GPT-4 Omni, which involves generating new training examples and transforming existing data, we significantly improved the accuracy of FinBERT, preparing it to serve as a teacher model. This enhanced FinBERT then distilled knowledge to TinyFinBERT, employing both GPT-4 Omni and GPT-3.5 Turbo augmented data. The distillation strategy incorporated both logit and intermediate layer distillation. The training and evaluation of TinyFinBERT utilized the PhraseBank dataset and the FiQA 2018 Task1 dataset, achieving performance comparable to FinBERT while being substantially smaller and more efficient. This research demonstrates how LLMs can effectively contribute to the advancement of financial sentiment analysis by enhancing the capabilities of smaller, more efficient models through innovative data augmentation and distillation techniques.",,,,,Distillation; Business; Finance; Computer science; Chromatography; Chemistry,,,,,https://arxiv.org/abs/2409.18999,http://dx.doi.org/10.48550/arxiv.2409.18999,,10.48550/arxiv.2409.18999,,,0,,0,true,,green
069-894-014-355-363,Smart Trader: Towards an Advisory Chatbot Service for Saudi Stock Market Investment,2024-03-03,2024,conference proceedings article,2024 Seventh International Women in Data Science Conference at Prince Sultan University (WiDS PSU),,IEEE,,Thanaa Alhydary; Layan Qashqary; Salma Alotaibi; Rana Jaha; Shorouq Alansari,,,,32,35,Chatbot; Business; Investment (military); Stock market; Finance; Service (business); Computer science; Stock (firearms); Computer security; World Wide Web; Marketing; Engineering; Context (archaeology); Mechanical engineering; Paleontology; Politics; Political science; Law; Biology,,,,,,http://dx.doi.org/10.1109/wids-psu61003.2024.00021,,10.1109/wids-psu61003.2024.00021,,,0,017-009-949-927-506; 032-700-400-584-155; 035-952-314-754-138; 044-972-364-805-241; 136-501-897-618-224; 147-044-512-540-381; 160-629-415-152-883,1,false,,
070-051-879-066-86X,AI-Powered Financial Insights: Using Large Language Models to Improve Government Decision-Making and Policy Execution,2025-10-02,2025,journal article,Journal of Industrial Engineering and Applied Science,30056071; 3005608x,Southern United Academy of Sciences Limited,,Luqing Ren,"<jats:p>Given the complexity of fiscal data types and the lengthy policy execution chain, this study constructs an application framework for language models supporting government decision-making. It systematically investigates task modules including decision-making question-answering identification, expenditure forecasting modeling, executive summary extraction, semantic matching, and conflict reasoning. The framework elucidates model architecture design methodologies and semantic fusion mechanisms, while introducing response capability simulation testing and performance evaluation systems. Using heterogeneous fiscal corpora and multi-task experimental data, demonstrates that the model exhibits strong performance in accuracy, generative consistency, and generalization capabilities, supporting intelligent applications across diverse fiscal scenarios.</jats:p>",3,5,21,26,Government (linguistics); Computer science; Finance; Business; Linguistics; Philosophy,,,,,,http://dx.doi.org/10.70393/6a69656173.333139,,10.70393/6a69656173.333139,,,0,,0,false,,
070-143-754-513-986,Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs,2025-01-16,2025,preprint,,,Qeios Ltd,,Joao Fonseca; Andrew Bell; Julia Stoyanovich,"<jats:p>Large Language Models (LLMs) have been shown to be susceptible to _jailbreak attacks_, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs “self-reflect”, may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict “normal” model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SAFENUDGE, that combines Controlled Text Generation with “nudging,” or using text interventions to change the behavior of a model. SAFENUDGE _triggers during text-generation while a jailbreak attack is being executed_, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SAFENUDGE is open-source and available through https://pypi.org/, and is compatible with models loaded with the Hugging Face transformerslibrary.</jats:p>",,,,,Safeguarding; Computer science; Business; Medicine; Nursing,,,,,http://arxiv.org/pdf/2501.02018 http://arxiv.org/abs/2501.02018,http://dx.doi.org/10.32388/zcxbc8,,10.32388/zcxbc8,,,0,014-286-233-108-712; 028-526-694-794-16X; 046-539-494-254-463; 047-082-704-948-48X; 050-735-345-132-596; 054-573-458-676-983; 068-864-486-586-772; 099-308-300-141-757; 111-769-133-477-716; 112-910-696-699-626; 125-820-182-759-399; 129-391-933-593-24X; 133-021-072-238-444; 158-669-937-793-877,0,true,cc-by,green
070-285-668-639-869,Leveraging LLMs for Financial News Analysis and Macroeconomic Indicator Nowcasting,,2024,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Lívia Réka Ónozó; Frigyes Viktor Arthur; Bálint Gyires-Tóth,,12,,160529,160547,Nowcasting; Economic indicator; Economic forecasting; Computer science; Financial system; Finance; Business; Economics; Macroeconomics; Econometrics; Meteorology; Geography,,,,Central Bank of Hungary; Nemzeti Kutat?si Fejleszt?si ?s Innov?ci?s Hivatal; Mesters?ges Intelligencia Nemzeti Laborat?rium,,http://dx.doi.org/10.1109/access.2024.3488363,,10.1109/access.2024.3488363,,,0,003-845-144-162-046; 006-399-176-500-152; 016-494-238-445-856; 020-506-148-516-294; 022-486-017-682-206; 023-312-248-006-551; 024-534-747-490-43X; 028-418-996-940-216; 035-034-660-623-20X; 047-871-035-706-003; 052-170-807-894-470; 059-561-225-158-126; 086-693-726-867-346; 131-548-187-091-743; 143-826-435-341-290; 152-777-711-371-559; 169-214-367-113-89X,4,true,"CC BY, CC BY-NC-ND",gold
070-415-303-523-428,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,2022-01-01,2022,preprint,arXiv (Cornell University),,,,Xiang Deng; Vasilisa Bashlovkina; Feng Han; Simon Baumgartner; Michael Bendersky,"Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.",,,,,Pipeline (software); Computer science; Task (project management); Quality (philosophy); Social media; Jargon; Sentiment analysis; Artificial intelligence; Machine learning; Financial market; Supervised learning; Natural language processing; Data science; Finance; Business; Economics; World Wide Web; Linguistics; Philosophy; Management; Epistemology; Programming language; Artificial neural network,,,,,https://arxiv.org/abs/2212.11311,http://dx.doi.org/10.48550/arxiv.2212.11311,,10.48550/arxiv.2212.11311,,,0,,0,true,other-oa,green
070-467-851-257-671,"Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly
  Improves Retrieval Augmented Generation With LLMs",2025-02-16,2025,preprint,arXiv (Cornell University),,,,Mohammad Reza Rezaei; Adji Bousso Dieng,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.",,,,,Diversity (politics); Quality (philosophy); Business; Economics; Political science; Physics; Law; Quantum mechanics,,,,,https://arxiv.org/abs/2502.11228,http://dx.doi.org/10.48550/arxiv.2502.11228,,10.48550/arxiv.2502.11228,,,0,,1,true,,green
070-532-488-848-570,Virtual assistant for Fidelity Investments,,2004,,,,,,Adam Edward Fuller; Anna Y. Tatashina,,,,,,Human–computer interaction; Fidelity; Remote assistance; Computer science,,,,,https://digital.wpi.edu/concern/student_works/c534fs21b?locale=en,https://digital.wpi.edu/concern/student_works/c534fs21b?locale=en,,,3203080265,,0,,0,false,,
071-280-613-312-248,Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Marcio Fonseca; Shay B. Cohen,"Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.",,,,,Leverage (statistics); Counterfactual thinking; Sentence; Computer science; Context (archaeology); Language model; Task (project management); Natural language processing; Data science; Cognitive psychology; Artificial intelligence; Psychology; Social psychology; Engineering; Paleontology; Biology; Systems engineering,,,,,https://arxiv.org/abs/2311.08704,http://dx.doi.org/10.48550/arxiv.2311.08704,,10.48550/arxiv.2311.08704,,,0,,0,true,other-oa,green
071-389-387-086-973,Multi-Document Financial Question Answering using LLMs,2024-11-08,2024,preprint,arXiv (Cornell University),,,,Shalin Shah; Srikanth Ryali; Ramasubbu Venkatesh,"We propose two new methods for multi-document financial question answering. First, a method that uses semantic tagging, and then, queries the index to get the context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that uses semantic tagging, and, retrieves knowledge graph triples from a graph database, as context. KG_RAG uses knowledge graphs constructed using a small model that is fine-tuned using knowledge distillation using a large teacher model. The data consists of 18 10K reports of Apple, Microsoft, Alphabet, NVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of questions in the data consists of 111 complex questions including many esoteric questions that are difficult to answer and the answers are not completely obvious. As evaluation metrics, we use overall scores as well as segmented scores for measurement including the faithfulness, relevance, correctness, similarity, an LLM based overall score and the rouge scores as well as a similarity of embeddings. We find that both methods outperform plain RAG significantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.",,,,,Question answering; Business; Finance; Computer science; Information retrieval,,,,,https://arxiv.org/abs/2411.07264,http://dx.doi.org/10.48550/arxiv.2411.07264,,10.48550/arxiv.2411.07264,,,0,,1,true,,green
071-429-676-769-002,Chatbot as a Trading Tool in the Cryptocurrency Market,,2021,journal article,Business Inform,22224459; 2311116x,Research Centre of Industrial Problems of Development of NAS of Ukraine,,Y. K. Plakhotna; Central Ukrainian National Technical University Zahreba,"<jats:p>Today, cryptocurrencies and topics related to information technology are attracting more attention not only on the part of traders, but also scientists. More research is being carried out aimed at the thorough study of cryptocurrencies, as well as the search for ways to facilitate interaction with blockchain. The topic of data analysis for cryptocurrencies is becoming increasingly important as the number of companies dependent on cryptocurrencies is growing rapidly. There are problems related to the cryptocurrency trading process, such as forecasting prices and trends, forecasting volatility, building a portfolio, detecting fraud, analyzing indicators for various cryptocurrencies. To solve these problems, trading bots are used. Trading bots are software products or websites that offer so-called «algorithmic trading», as they automatically analyze the actions and indicators of the market, offer strategies to maximize the trader’s profits and increase his satisfaction. They can aggregate historical market data, calculate indicators, model the order fulfillment and can even be set up to execute strategies while the customer is asleep. When analyzing the needs of the market, it turned out that there was a lack of a chat bot that would help traders or simply persons interested in the topic of cryptocurrencies to receive fresh information about the latest changes in the market. The article considers the functions and examples of performance of the chat bot CryptoAlert, created by one of the authors, which helps users to always be aware of the latest changes in the cryptocurrency market. The main function of the bot is to receive notifications about significant changes in the price of the selected coin. The use of CryptoAlert facilitates the trader’s work and significantly increases the likelihood of successful trading in the market.</jats:p>",11,526,388,394,Cryptocurrency; Trading strategy; Portfolio; Computer science; Order (exchange); Process (computing); Algorithmic trading; Volatility (finance); Business; Computer security; Finance; Operating system,cryptocurrencies; trading; chat bot; volatility; price change schedule. Fig.: 4. Bibl.: 14,,,,https://www.business-inform.net/export_pdf/business-inform-2021-11_0-pages-388_394.pdf https://doi.org/10.32983/2222-4459-2021-11-388-394,http://dx.doi.org/10.32983/2222-4459-2021-11-388-394,,10.32983/2222-4459-2021-11-388-394,,,0,,0,true,cc-by-sa,gold
071-493-553-890-730,"Can Large Language Models Effectively Process and Execute Financial
  Trading Instructions?",2024-12-06,2024,preprint,arXiv (Cornell University),,,,Yu Kang; Ge Wang; Xin Yang; Yuda Wang; Mingwen Liu,"The development of Large Language Models (LLMs) has created transformative opportunities for the financial industry, especially in the area of financial trading. However, how to integrate LLMs with trading systems has become a challenge. To address this problem, we propose an intelligent trade order recognition pipeline that enables the conversion of trade orders into a standard format in trade execution. The system improves the ability of human traders to interact with trading platforms while addressing the problem of misinformation acquisition in trade execution. In addition, we have created a trade order dataset of 500 pieces of data to simulate real-world trading scenarios. Moreover, we designed several metrics to provide a comprehensive assessment of dataset reliability and the generative power of big models in finance by experimenting with five state-of-the-art LLMs on our dataset. The results indicate that while LLMs demonstrate high generation rates (87.50% to 98.33%) and perfect follow-up rates, they face significant challenges in accuracy (5% to 10%) and completeness, with high missing rates (14.29% to 67.29%). In addition, LLMs tend to over-interrogate, suggesting that large models tend to collect more information, carrying certain challenges for information security.",,,,,Process (computing); Computer science; Finance; Programming language; Business,,,,,https://arxiv.org/abs/2412.04856,http://dx.doi.org/10.48550/arxiv.2412.04856,,10.48550/arxiv.2412.04856,,,0,,0,true,,green
071-524-112-570-212,Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Zihao Wu; Lu Zhang; Chao Cao; Xiaowei Yu; Haixing Dai; Chong Ma; Zhengliang Liu; Lin Zhao; Gang Li; Wei Liu; Quanzheng Li; Dinggang Shen; Xiang Li; Dajiang Zhu; Tianming Liu,"Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) GPT-4 outperforms ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model that is capable of solving various tasks across different domains is feasible.",,,,,Task (project management); Computer science; Domain (mathematical analysis); Inference; Domain-specific language; Artificial intelligence; Machine learning; Software engineering; Systems engineering; Mathematics; Mathematical analysis; Engineering,,,,,https://arxiv.org/abs/2304.09138,http://dx.doi.org/10.48550/arxiv.2304.09138,,10.48550/arxiv.2304.09138,,,0,,0,true,cc-by,green
071-659-755-199-208,Firm‐Level Trade Policy Uncertainty and Cost Stickiness: Evidence From a Large Language Model Approach,2025-05-20,2025,journal article,Accounting & Finance,08105391; 1467629x,Wiley,United States,Shuyang Jia; Peng Liang; Nan Hu,"<jats:title>ABSTRACT</jats:title><jats:p>This study investigates how managers' perceived trade policy uncertainty (TPU) affects firms' cost management decisions. Adopting a firm‐level TPU measure developed by a large language model, our analysis reveals that TPU reduces cost stickiness. This effect is more pronounced in firms managed by non‐overconfident, risk‐averse, or high‐ability executives. Our results hold when we employ a series of endogeneity tests, including a coarsened exact matching method, a difference‐in‐difference approach, and an instrumental variable approach. Finally, we find that firms' cost management decisions resulting from pessimistic expectations about future sales have significant implications for firm performance.</jats:p>",65,3,3058,3080,Economics; Econometrics,,,,National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities,,http://dx.doi.org/10.1111/acfi.70032,,10.1111/acfi.70032,,,0,006-929-811-507-325; 007-109-528-679-001; 010-679-347-509-372; 013-484-544-181-336; 017-042-776-084-188; 018-722-421-791-372; 018-973-133-682-91X; 020-002-785-753-752; 027-107-844-546-610; 028-095-620-024-822; 036-979-852-259-747; 038-013-503-792-663; 038-242-929-429-895; 038-849-931-554-776; 040-556-924-679-266; 041-783-304-053-672; 045-100-985-676-161; 049-357-849-712-120; 058-642-666-213-520; 061-978-873-940-410; 062-211-669-804-852; 065-666-404-377-357; 067-843-533-191-759; 073-854-087-596-828; 076-861-401-430-610; 079-200-702-740-71X; 079-639-674-831-26X; 083-955-905-534-924; 084-866-125-470-626; 086-936-635-444-034; 091-928-087-936-10X; 092-022-643-764-122; 093-929-980-366-688; 096-696-449-401-906; 103-247-865-872-131; 104-373-107-404-359; 104-684-853-775-918; 107-742-120-222-031; 111-255-013-433-673; 111-459-997-118-741; 118-318-679-990-070; 125-247-294-429-870; 127-379-617-155-518; 128-021-308-035-029; 129-338-804-219-304; 129-412-812-740-960; 129-767-632-180-962; 130-162-244-489-451; 130-533-182-179-596; 136-519-458-264-706; 140-459-735-291-941; 154-042-138-562-576; 157-912-489-332-268; 158-481-374-319-455; 159-614-024-183-351; 160-828-754-627-734; 166-692-120-050-337; 167-999-949-731-567; 179-754-167-090-486; 190-132-778-687-689,0,false,,
071-688-133-656-26X,Zero-Shot LLM Agents for Explainable Bridge Portfolio Maintenance Scheduling Under Uncertainty,,2025,preprint,,,Elsevier BV,,Giulio Mariniello; Tommaso Pastore; Domenico Asprone,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5946036,,10.2139/ssrn.5946036,,,0,,0,false,,
072-658-255-705-794,Financial Intelligent Customer Service Multimodal Dialogue System Design and Risk Control Collaborative Application,,2025,journal article,Computer Science and Application,21618801; 2161881x,Hans Publishers,,冲 刘,,15,10,351,357,Computer science; Control (management); Risk Control; Process management; Finance; Business; Artificial intelligence; Risk management,,,,,,http://dx.doi.org/10.12677/csa.2025.1510274,,10.12677/csa.2025.1510274,,,0,104-189-499-658-912; 123-046-690-818-161,0,false,,
073-018-198-431-408,Parameter-Efficient Fine-Tuning of Pre-trained Large Language Models for Financial Text Analysis,2024-11-26,2024,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Switzerland,Germany,Kelly Langa; Hairong Wang; Olaperi Okuboyejo,,,,3,20,Computer science; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1007/978-3-031-78255-8_1,,10.1007/978-3-031-78255-8_1,,,0,007-962-566-589-953; 023-650-321-831-526; 037-051-987-842-778; 041-341-332-394-580; 057-982-454-558-150; 076-795-863-495-475; 078-791-946-251-745; 104-956-882-948-669; 117-929-228-389-733; 137-408-709-935-186; 144-516-287-769-326; 153-939-232-752-691; 174-061-172-895-947; 181-432-462-344-434,2,false,,
073-303-412-668-303,Artificial Intelligence in Financial Services: Customer Chatbot Advisor Adoption,2019-11-30,2019,journal article,International Journal of Innovative Technology and Exploring Engineering,22783075,Blue Eyes Intelligence Engineering and Sciences Engineering and Sciences Publication - BEIESP,,Dr.Kanchan Patil*; Dr.Mugdha S Kulkarni,"<jats:p>The growing sophistication technology has helped us exchange Information at our fingertips, eliminating the need for human support.” A platform designed to understand, learn and converse like a human and answer ad-hoc queries in real time is commonly referred to as a Chabot”. Chabot advisor is Artificial intelligence (AI) computer program that impersonates human communication in its natural format including text or spoken language using a technique such as NLP, image processing or video processing along with the end task completion as instructed by the user [1]. The purpose of the paper was to examine what are the drivers for Chabot advisor services adoption (CBA), focusing on financial services. This study presents the explanatory Chabot advisor services factors by extending the Technology Acceptance Model (TAM). The construct in the research are like perceived privacy, perceived security, enjoyment and social influence. This empirical study was conducted in Pune city in India by collecting primary data from 310 online financial services customers. Data collected was analyzed using structural equation modeling using PLS-SEM.The outcome of this study is vital to financial companies like banks, policymakers, technology services adoption literature and provide customer-centric financial services.</jats:p>",9,1,4296,4303,Chatbot; Sophistication; Financial services; Computer science; Knowledge management; Empirical research; Technology acceptance model; Usability; Artificial intelligence; Business; Finance; Human–computer interaction; Social science; Philosophy; Epistemology; Sociology,Chatbot advisors; Artificial Intelligence; Financial Services; Technology adoption; PLS SEM,,,,,http://dx.doi.org/10.35940/ijitee.a4928.119119,,10.35940/ijitee.a4928.119119,,,0,,35,true,,gold
073-425-021-386-101,Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,2025-01-01,2025,journal article,IEEE Transactions on Big Data,23327790; 23722096,,,Zihao Wu; Lu Zhang; Chao Cao; Xiaowei Yu; Zhengliang Liu; Lin Zhao; Yiwei Li; Haixing Dai; Chong Ma; Gang Li; Wei Liu; Quanzheng Li; Dinggang Shen; Li Xiang; Dajiang Zhu; Tianming Liu,,,,1,14,Computer science; Task (project management); Artificial intelligence; Management; Economics,,,,,,,,,,,0,,0,false,,
073-631-398-097-091,CONVERSATIONAL AI FOR PERSONALIZED WEALTH MANAGEMENT IN CLOUD-BASED CRM SOLUTIONS,2025-06-20,2025,journal article,international journal of advanced research in computer science,09765697,IJARCS International Journal of Advanced Research in Computer Science,,Mahesh Yadlapati,"<jats:p>Conversational AI has emerged as a transformative tool for personalized wealth management within cloud-based CRM solutions [1]. With the increasing complexity of financial services, clients demand personalized interactions and real-time insights tailored to their investment goals [2]. This paper explores the design and implementation of AI-driven virtual assistants to enhance client engagement, automate advisory services, and improve operational efficiency [3]. The proposed approach leverages Natural Language Processing (NLP), Machine Learning (ML), and cloud computing technologies to provide predictive financial insights and personalized recommendations [4]. A hybrid AI-human collaboration model is introduced to optimize decision-making processes, ensuring a balance between automation and human expertise [5]. The evaluation framework considers response accuracy, client satisfaction, operational efficiency, and regulatory compliance [6]. Our experimental results demonstrate that the proposed system enhances customer retention by 35%, improves response accuracy by 20%, and reduces operational costs by 25% [7]. This study also discusses potential challenges, including data privacy concerns, ethical implications, and scalability considerations [8]. Future directions include integrating reinforcement learning techniques and expanding multi-language capabilities to cater to a global audience.</jats:p>",16,3,22,27,Computer science; Cloud computing; World Wide Web; Database; Operating system,,,,,,http://dx.doi.org/10.26483/ijarcs.v16i3.7263,,10.26483/ijarcs.v16i3.7263,,,0,,0,false,,
073-747-382-635-005,"Trading Degrees of Freedom: User Control in Pens, GPT, and Automated Vehicles",2023-10-19,2023,journal article,Proceedings of the Human Factors and Ergonomics Society Annual Meeting,10711813; 21695067; 15419312,SAGE Publications,,Vianney Renata; John D. Lee; Amudha V. Kamaraj,"<jats:p> Technology development often has profound consequences for human agency. Considering the evolution of seemingly mundane tools, such as pens, can help us anticipate the consequences of emerging technology. We consider how the evolution of pens might guide automation and artificial intelligence (AI) design. Automation and AI change activities by trading degrees of freedom—expanding, eliminating, or resolving—to transform intent into action. The evolution of pens shows three approaches to trading degrees of freedom. A technology-based approach eliminates degrees of freedom to improve efficiency A human-centered approach trades degrees of freedom to enhance human agency. A system-centered approach trades degrees of freedom to address domain entropy and promote resilience. We link trading degrees of freedom to concrete design considerations with examples from pens, large language models, and vehicle automation. We hope that analyzing a technology far removed from AI and vehicle automation will provoke new discussions of humanautomation interaction. </jats:p>",67,1,2472,2477,Degrees of freedom (physics and chemistry); Automation; Agency (philosophy); Computer science; Control (management); Freedom of choice; Artificial intelligence; Engineering; Sociology; Law; Political science; Quantum mechanics; Mechanical engineering; Social science; Physics,,,,,,http://dx.doi.org/10.1177/21695067231192270,,10.1177/21695067231192270,,,0,008-710-972-478-55X; 013-079-966-565-066; 027-299-299-899-996; 029-170-093-688-304; 034-993-006-353-011; 042-536-761-052-980; 043-232-889-879-518; 066-274-060-631-166; 067-426-451-375-880; 068-236-693-566-697; 080-511-738-866-254; 087-394-536-396-164; 102-076-145-265-317; 107-578-458-532-182; 163-420-631-813-617; 176-296-448-201-555; 178-439-482-544-402,0,false,,
074-042-452-083-012,"FinTagging: An LLM-ready Benchmark for Extracting and Structuring
  Financial Information",2025-05-26,2025,preprint,arXiv (Cornell University),,,,Yan Wang; Yang Ren; Lingfei Qian; Xueqing Peng; Keyi Wang; Yi Han; Dongji Feng; Fengran Mo; Shengyuan Lin; Qinchuan Zhang; Kaiwen He; Chenri Luo; Jianxing Chen; Junwei Wu; Jimin Huang; Guojun Xiong; Xiao-Yang Liu; Qianqian Xie; Jian-Yun Nie,"We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.",,,,,Structuring; Benchmark (surveying); Computer science; Finance; Business; Data mining; Artificial intelligence; Geography; Cartography,,,,,https://arxiv.org/abs/2505.20650,http://dx.doi.org/10.48550/arxiv.2505.20650,,10.48550/arxiv.2505.20650,,,0,,1,true,,green
074-433-575-957-726,"Text2TimeSeries: Enhancing Financial Forecasting through Time Series
  Prediction Updates with Event-Driven Insights from Large Language Models",2024-07-04,2024,preprint,arXiv (Cornell University),,,,Litton Jose Kurisinkel; Pruthwik Mishra; Yue Zhang,"Time series models, typically trained on numerical data, are designed to forecast future values. These models often rely on weighted averaging techniques over time intervals. However, real-world time series data is seldom isolated and is frequently influenced by non-numeric factors. For instance, stock price fluctuations are impacted by daily random events in the broader world, with each event exerting a unique influence on price signals. Previously, forecasts in financial markets have been approached in two main ways: either as time-series problems over price sequence or sentiment analysis tasks. The sentiment analysis tasks aim to determine whether news events will have a positive or negative impact on stock prices, often categorizing them into discrete labels. Recognizing the need for a more comprehensive approach to accurately model time series prediction, we propose a collaborative modeling framework that incorporates textual information about relevant events for predictions. Specifically, we leverage the intuition of large language models about future changes to update real number time series predictions. We evaluated the effectiveness of our approach on financial market data.",,,,,Series (stratigraphy); Event (particle physics); Time series; Computer science; Finance; Econometrics; Machine learning; Economics; Geology; Paleontology; Physics; Quantum mechanics,,,,,https://arxiv.org/abs/2407.03689,http://dx.doi.org/10.48550/arxiv.2407.03689,,10.48550/arxiv.2407.03689,,,0,,0,true,,green
074-438-217-755-377,"Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of
  Bilateral Financial Exchanges, A bond market study",2025-02-28,2025,preprint,arXiv (Cornell University),,,,Alicia Vidler; Toby Walsh,"Bilateral markets, such as those for government bonds, involve decentralized and opaque transactions between market makers (MMs) and clients, posing significant challenges for traditional modeling approaches. To address these complexities, we introduce TRIBE an agent-based model augmented with a large language model (LLM) to simulate human-like decision-making in trading environments. TRIBE leverages publicly available data and stylized facts to capture realistic trading dynamics, integrating human biases like risk aversion and ambiguity sensitivity into the decision-making processes of agents. Our research yields three key contributions: first, we demonstrate that integrating LLMs into agent-based models to enhance client agency is feasible and enriches the simulation of agent behaviors in complex markets; second, we find that even slight trade aversion encoded within the LLM leads to a complete cessation of trading activity, highlighting the sensitivity of market dynamics to agents' risk profiles; third, we show that incorporating human-like variability shifts power dynamics towards clients and can disproportionately affect the entire system, often resulting in systemic agent collapse across simulations. These findings underscore the emergent properties that arise when introducing stochastic, human-like decision processes, revealing new system behaviors that enhance the realism and complexity of artificial societies.",,,,,Risk aversion (psychology); Bond; Financial market; Power (physics); Business; Finance; Economics; Financial economics; Expected utility hypothesis; Physics; Quantum mechanics,,,,,https://arxiv.org/abs/2503.00320,http://dx.doi.org/10.48550/arxiv.2503.00320,,10.48550/arxiv.2503.00320,,,0,,0,true,,green
074-561-499-082-142,A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Sorouralsadat Fatemi; Yuheng Hu,"Financial sentiment analysis plays a crucial role in uncovering latent patterns and detecting emerging trends, enabling individuals to make well-informed decisions that may yield substantial advantages within the constantly changing realm of finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless, their potential and applicability in the context of financial sentiment analysis have not been thoroughly explored yet. To bridge this gap, we employ two approaches: in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs on a finance-domain dataset. Given the computational costs associated with fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from 250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art results to evaluate their effectiveness in the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can achieve comparable performance to state-of-the-art fine-tuned LLMs, even with models having fewer parameters and a smaller training dataset. Additionally, the zero-shot and one-shot performance of LLMs produces comparable results with fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is no observed enhancement in performance for finance-domain sentiment analysis when the number of shots for in-context learning is increased.",,,,,Context (archaeology); Computer science; Finance; Artificial intelligence; Economics; Geography; Archaeology,,,,,https://arxiv.org/abs/2312.08725,http://dx.doi.org/10.48550/arxiv.2312.08725,,10.48550/arxiv.2312.08725,,,0,,0,true,other-oa,green
074-644-083-914-666,Evaluating Sustainability Trade-Offs in Engineering Design as System of Systems Using LLMs,2025-06-08,2025,conference proceedings article,2025 20th Annual System of Systems Engineering Conference (SoSE),,IEEE,,Tugba Karabiyik; Akif Ozer; Umit Karabiyik,,,,1,7,Sustainability; Computer science; Environmental economics; Risk analysis (engineering); Business; Economics; Ecology; Biology,,,,,,http://dx.doi.org/10.1109/sose66311.2025.11083823,,10.1109/sose66311.2025.11083823,,,0,,0,false,,
075-356-529-898-550,FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models,,2024,conference proceedings article,Findings of the Association for Computational Linguistics ACL 2024,,Association for Computational Linguistics,,Gagan Bhatia; El Moatez Billah Nagoudi; Hasan Cavusoglu; Muhammad Abdul-Mageed,,,,13064,13087,Computer science,,,,,https://arxiv.org/pdf/2402.10986 https://arxiv.org/abs/2402.10986,http://dx.doi.org/10.18653/v1/2024.findings-acl.774,,10.18653/v1/2024.findings-acl.774,,,0,,21,true,,green
075-515-218-575-821,"WHICH AI MODEL LEADS IN SUMMARIZING FINANCIAL ARTICLES? A COMPARATIVE ANALYSIS OF GPT, MISTRAL, AND LLAMA",,2025,conference proceedings article,27th Annual International Conference ECONOMIC COMPETITIVENESS AND SUSTAINABILITY 2025 PROCEEDINGS,,Mendel University in Brno,,Jana Dannhoferová; Jan Přichystal,,,,39,50,,,,,,,http://dx.doi.org/10.11118/978-80-7701-047-4-0039,,10.11118/978-80-7701-047-4-0039,,,0,019-770-927-454-050; 048-140-013-310-216; 048-829-725-764-697; 062-509-137-138-977; 074-971-964-648-503; 079-761-354-207-424; 083-092-756-933-41X; 086-785-967-015-839; 089-315-150-581-679; 113-808-433-444-942; 119-698-758-279-379; 126-872-184-526-952; 134-328-642-083-126; 167-491-133-613-958,0,false,,
075-718-431-880-366,Exploring the Impact of Chatbots on Customer Satisfaction and Business Efficiency in New Zealand's Financial Technology Industry,,2023,journal article,Rere Āwhio – Journal of Applied Research and Practice,28158598; 27447626,Te Pukenga,,Kashmira More; Indrapriya Kularatne,"The emergence of Artificial Intelligence has disrupted the conventional ways of dealing with customers.Artificial Intelligence-powered chatbots are one of the outputs of this technological disruption.Although the capabilities of a chatbot have been widely recognised, its adoption in financial service institutions has caused arguments over data protection.Customers tend to become comfortable with the human-like chatbot due to it being non-judgmental.The research method used was secondary qualitative research techniques based on literature reviews.The objective of this investigation is to evaluate the rise of chatbot adoption in customer services in the financial service institutions of New Zealand.The findings of the research indicate a rise in chatbot adoption has also led to data privacy issues in customer services.The research intended to objectively evaluate chatbots, to determine their merit at the expense of data protection and recommend solutions to address data privacy problems with chatbots.",,3,26,34,Customer satisfaction; Business; Marketing,,,,,https://online.op.ac.nz/assets/Uploads/EXPLORING-THE-IMPACT.pdf https://doi.org/10.34074/rere.00304,http://dx.doi.org/10.34074/rere.00304,,10.34074/rere.00304,,,0,,1,true,,bronze
076-112-090-315-608,"SilverSight: A Multi-Task Chinese Financial Large Language Model Based
  on Adaptive Semantic Space Learning",2024-04-07,2024,preprint,arXiv (Cornell University),,,,Yuhang Zhou; Zeping Li; Siyu Tian; Yuchen Ni; Sen Liu; Guangnan Ye; Hongfeng Chai,"Large language models (LLMs) are increasingly being applied across various specialized fields, leveraging their extensive knowledge to empower a multitude of scenarios within these domains. However, each field encompasses a variety of specific tasks that require learning, and the diverse, heterogeneous data across these domains can lead to conflicts during model task transfer. In response to this challenge, our study introduces an Adaptive Semantic Space Learning (ASSL) framework, which utilizes the adaptive reorganization of data distributions within the semantic space to enhance the performance and selection efficacy of multi-expert models. Utilizing this framework, we trained a financial multi-task LLM named ""SilverSight"". Our research findings demonstrate that our framework can achieve results close to those obtained with full data training using only 10% of the data, while also exhibiting strong generalization capabilities.",,,,,Task (project management); Space (punctuation); Computer science; Artificial intelligence; Natural language processing; Economics; Management; Operating system,,,,,https://arxiv.org/abs/2404.04949,http://dx.doi.org/10.48550/arxiv.2404.04949,,10.48550/arxiv.2404.04949,,,0,,0,true,,green
076-310-685-074-054,"LLM4FTS: Enhancing Large Language Models for Financial Time Series
  Prediction",2025-05-05,2025,preprint,arXiv (Cornell University),,,,Zian Liu; Renjun Jia,"Predicting financial time series presents significant challenges due to inherent low signal-to-noise ratios and intricate temporal patterns. Traditional machine learning models exhibit limitations in this forecasting task constrained by their restricted model capacity. Recent advances in large language models (LLMs), with their greatly expanded parameter spaces, demonstrate promising potential for modeling complex dependencies in temporal sequences. However, existing LLM-based approaches typically focus on fixed-length patch analysis due to the Transformer architecture, ignoring market data's multi-scale pattern characteristics. In this study, we propose $LLM4FTS$, a novel framework that enhances LLM capabilities for temporal sequence modeling through learnable patch segmentation and dynamic wavelet convolution modules. Specifically,we first employ K-means++ clustering based on DTW distance to identify scale-invariant patterns in market data. Building upon pattern recognition results, we introduce adaptive patch segmentation that partitions temporal sequences while preserving maximal pattern integrity. To accommodate time-varying frequency characteristics, we devise a dynamic wavelet convolution module that emulates discrete wavelet transformation with enhanced flexibility in capturing time-frequency features. These three modules work together to improve large language model's ability to handle scale-invariant patterns in financial time series. Extensive experiments on real-world financial datasets substantiate the framework's efficacy, demonstrating superior performance in capturing complex market patterns and achieving state-of-the-art results in stock return prediction. The successful deployment in practical trading systems confirms its real-world applicability, representing a significant advancement in LLM applications for financial forecasting.",,,,,Series (stratigraphy); Computer science; Time series; Finance; Artificial intelligence; Economics; Machine learning; Geology; Paleontology,,,,,https://arxiv.org/abs/2505.02880,http://dx.doi.org/10.48550/arxiv.2505.02880,,10.48550/arxiv.2505.02880,,,0,,0,true,,green
076-335-242-585-219,AI Financial Advisor Chatbot - Scalable & Personalized Financial Guidance as a Service,,2025,journal article,International Journal of Enhanced Research In Science Technology & Engineering,23197463,Enhanced Research Publications,,Saikat Goswami; Harshwardhan Singh; Shivam Shriwastava; Sonali Bhowmik; Dr. Dipankar Misra; Prof. (Dr.) Sumit Nandi,"<jats:p>In today’s society, proper money management is very important. Many individuals earn good salaries but still find it difficult to save, invest, or pay EMIs because they do not plan their finances properly. This issue inspired us to develop a website called AI Advisor, which helps users manage their money in a smart and easy way. After a user enters their monthly salary, the tool calculates how much should be set aside for savings, how much be invested, and how much EMI can be afforded if they plan to make a purchase. The tool follows basic financial principles and provides sound advice, along with financial tips and tricks to improve the user’s financial health. AI Advisor is especially helpful for students, fresh graduates, early-career professionals, and families who want to better understand their income and learn how to manage it effectively. The primary goal of this project is to reduce financial stress and help people manage their finances easily with the support of artificial intelligence.</jats:p>",14,6,206,211,Chatbot; Service (business); Financial services; Finance; Business; Scalability; Computer science; World Wide Web; Marketing; Database,,,,,,http://dx.doi.org/10.55948/ijerste.2025.0631,,10.55948/ijerste.2025.0631,,,0,,0,false,,
076-527-892-348-625,"LLM Inference Scheduling: A Survey of Techniques, Frameworks, and Trade-offs",2025-11-05,2025,preprint,,,Institute of Electrical and Electronics Engineers (IEEE),,Morgan Heisler; Zahra Yousefijamarani; Xinglu Wang; Qian Wang; Ge Shi; Hanieh Sadri; Timothy Yu; Yifei Li; Haley Li; Gursimran Singh; Linzi Xing; Daesik Jang; Hong Chang; Qintao Zhang; Kan Chen; Shuming Jing; Yizhou Shan; Yi Li; Xiaolong Bai; Yu Zhang; Tianzheng Wang; Jiannan Wang; Ivan Beschastinikh; Ying Xiong; Yong Zhang; Zhenan Fan,,,,,,,,,,,,http://dx.doi.org/10.36227/techrxiv.176238087.79673350/v1,,10.36227/techrxiv.176238087.79673350/v1,,,0,,0,false,,
076-546-850-528-242,Evaluating the Accuracy of Chatbots in Financial Literature,2024-11-11,2024,preprint,arXiv (Cornell University),,,,Orhan Erdem; Kristi Hassett; Feyzullah Egriboyun,"We evaluate the reliability of two chatbots, ChatGPT (4o and o1-preview versions), and Gemini Advanced, in providing references on financial literature and employing novel methodologies. Alongside the conventional binary approach commonly used in the literature, we developed a nonbinary approach and a recency measure to assess how hallucination rates vary with how recent a topic is. After analyzing 150 citations, ChatGPT-4o had a hallucination rate of 20.0% (95% CI, 13.6%-26.4%), while the o1-preview had a hallucination rate of 21.3% (95% CI, 14.8%-27.9%). In contrast, Gemini Advanced exhibited higher hallucination rates: 76.7% (95% CI, 69.9%-83.4%). While hallucination rates increased for more recent topics, this trend was not statistically significant for Gemini Advanced. These findings emphasize the importance of verifying chatbot-provided references, particularly in rapidly evolving fields.",,,,,Business; Computer science,,,,,https://arxiv.org/abs/2411.07031,http://dx.doi.org/10.48550/arxiv.2411.07031,,10.48550/arxiv.2411.07031,,,0,,0,true,,green
077-082-413-651-14X,WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off,,2024,conference proceedings article,Advances in Neural Information Processing Systems 37,,"Neural Information Processing Systems Foundation, Inc. (NeurIPS)",,Teddy Furon; Eva Giboulot,,,,18848,18881,,,,,,,http://dx.doi.org/10.52202/079017-0597,,10.52202/079017-0597,,,0,,1,false,,
077-426-916-648-213,"Wealth of Nations, Wealth of Data: How GDP Shapes Diverse Large Language Models like ChatGPT : Interviewing Assorted Open Source Generative AI Models",2023-12-15,2023,conference proceedings article,2023 IEEE International Conference on Big Data (BigData),,IEEE,,Alex Kaplunovich,"Generative large language models (such as ChatGPT) are increasingly influencing various aspects of our lives, partly due to their training on vast datasets that encompassing big data paradigms and range of topics. ""Intervista,"" an award-winning Italian film by Federico Fellini, focuses on his interview with a Japanese TV crew. Inspired by this, we conducted interviews with a diverse set of open-source and OpenAI models to explore various political, economic, and cultural aspects of life, evaluating LLM performance. We also examined whether a correlation exists between a country's GDP per capita and the quality of the model's answers. To this end, we utilized a Huggingface model leaderboard to select appropriate models and deployed them in an AWS SageMaker GPU environment. The identical questions were posed about nearly 200 countries, and the responses were analyzed to verify their accuracy and correlation with Gross Domestic Product (GDP). We were amazed by the diversity, quantity, and quality of existing pretrained open-source LLMs. Our journey provided insights into model selection, inference pipeline automation, GPU configuration, generated texts benchmarking, and systematic evaluation of model quality. Overall, leading LLMs performed well, providing reasonable responses for many countries. However, we discovered that the depth and detail of the answers were influenced by a country's GDP per capita, with higher-income nations receiving more accurate responses.",,,4654,4663,Generative grammar; Computer science; Interview; Generative model; Open source; Artificial intelligence; Natural language processing; Sociology; Programming language; Software; Anthropology,,,,,,http://dx.doi.org/10.1109/bigdata59044.2023.10386329,,10.1109/bigdata59044.2023.10386329,,,0,007-962-566-589-953; 068-864-486-586-772; 154-917-645-855-591; 194-870-254-181-302,1,false,,
077-581-534-134-575,Multi-Agent LLM Framework for Stock Recommendation via Financial Feature Summarization,2025-07-04,2025,conference proceedings article,"2025 International Conference on Information, Implementation, and Innovation in Technology (I2ITCON)",,IEEE,,Siddhika Joshi; Anupkumar Bongale; Deepak Dharrao,,,,1,6,,,,,,,http://dx.doi.org/10.1109/i2itcon65200.2025.11210609,,10.1109/i2itcon65200.2025.11210609,,,0,015-303-838-904-074; 017-296-092-624-358; 026-049-561-382-987; 029-266-520-167-711; 035-876-619-354-109; 038-241-735-129-443; 065-974-635-780-500; 116-795-748-063-30X; 119-422-873-153-993; 126-416-704-188-100; 133-414-421-649-207; 190-209-715-673-59X; 193-136-826-957-223,0,false,,
078-516-732-049-916,Alpha-GPT 2.0: Human-in-the-Loop AI for Quantitative Investment,2024-02-15,2024,preprint,arXiv (Cornell University),,,,Hang Yuan; Saizhuo Wang; Jian Guo,"Recently, we introduced a new paradigm for alpha mining in the realm of quantitative investment, developing a new interactive alpha mining system framework, Alpha-GPT. This system is centered on iterative Human-AI interaction based on large language models, introducing a Human-in-the-Loop approach to alpha discovery. In this paper, we present the next-generation Alpha-GPT 2.0 \footnote{Draft. Work in progress}, a quantitative investment framework that further encompasses crucial modeling and analysis phases in quantitative investment. This framework emphasizes the iterative, interactive research between humans and AI, embodying a Human-in-the-Loop strategy throughout the entire quantitative investment pipeline. By assimilating the insights of human researchers into the systematic alpha research process, we effectively leverage the Human-in-the-Loop approach, enhancing the efficiency and precision of quantitative investment research.",,,,,Investment (military); Loop (graph theory); Alpha (finance); Human-in-the-loop; Business; Economics; Computer science; Political science; Mathematics; Economy; Artificial intelligence; Combinatorics; Law; Cronbach's alpha; Politics; Service (business),,,,,https://arxiv.org/abs/2402.09746,http://dx.doi.org/10.48550/arxiv.2402.09746,,10.48550/arxiv.2402.09746,,,0,,1,true,,green
078-595-752-639-989,Developing Responsible Chatbots for Financial Services: A Pattern-Oriented Responsible Artificial Intelligence Engineering Approach,,2023,journal article,IEEE Intelligent Systems,15411672; 19411294,Institute of Electrical and Electronics Engineers (IEEE),United States,Qinghua Lu; Yuxiu Luo; Liming Zhu; Mingjian Tang; Xiwei Xu; Jon Whittle,"The recent release of ChatGPT has gained huge attention and discussion worldwide, with responsible AI being a crucial topic of discussion. One key question is how we can ensure that AI systems, like ChatGPT, are developed and adopted in a responsible way? To tackle the responsible AI challenges, various ethical principles have been released by governments, organisations, and companies. However, those principles are very abstract and not practical enough. Further, significant efforts have been put on algorithm-level solutions that only address a narrow set of principles, such as fairness and privacy. To fill the gap, we adopt a pattern-oriented responsible AI engineering approach and build a Responsible AI Pattern Catalogue to operationalise responsible AI from a system perspective. In this article, we first summarise the major challenges in operationalising responsible AI at scale and introduce how we use the Responsible AI Pattern Catalogue to address those challenges. We then examine the risks at each stage of the chatbot development process and recommend pattern-driven mitigations to evaluate the the usefulness of the Responsible AI Pattern Catalogue in a real-world setting.",38,6,42,51,Computer science; Chatbot; Process (computing); Set (abstract data type); Scale (ratio); Key (lock); Perspective (graphical); Applications of artificial intelligence; Artificial intelligence; Data science; Computer security; Physics; Quantum mechanics; Programming language; Operating system,,,,,https://ieeexplore.ieee.org/ielx7/9670/5196652/10268394.pdf https://doi.org/10.1109/mis.2023.3320437,http://dx.doi.org/10.1109/mis.2023.3320437,,10.1109/mis.2023.3320437,,,0,002-343-687-377-434; 011-707-791-258-831; 017-688-528-040-883; 023-260-540-417-516; 026-060-975-476-032; 026-980-570-150-300; 036-998-104-612-799; 046-273-255-819-505; 057-004-732-282-733; 065-314-583-834-517; 065-327-345-711-805; 069-180-806-649-058; 116-057-853-146-778; 130-046-176-122-683; 188-038-507-525-327,13,true,cc-by,hybrid
078-646-059-819-057,Assessing Large Language Models Used for Extracting Table Information from Annual Financial Reports,2024-10-09,2024,journal article,Computers,2073431x,MDPI AG,,David Balsiger; Hans-Rudolf Dimmler; Samuel Egger-Horstmann; Thomas Hanne,"<jats:p>The extraction of data from tables in PDF documents has been a longstanding challenge in the field of data processing and analysis. While traditional methods have been explored in depth, the rise of Large Language Models (LLMs) offers new possibilities. This article addresses the knowledge gaps regarding LLMs, specifically ChatGPT-4 and BARD, for extracting and interpreting data from financial tables in PDF format. This research is motivated by the real-world need to efficiently gather and analyze corporate financial information. The hypothesis is that LLMs—in this case, ChatGPT-4 and BARD—can accurately extract key financial data, such as balance sheets and income statements. The methodology involves selecting representative pages from 46 annual reports of large Swiss corporations listed in the SMI Expanded Index from 2022 and copy–pasting text from these into LLMs. Eight analytical questions were posed to the LLMs, and their responses were assessed for accuracy and for identifying potential error sources in data extraction. The findings revealed significant variance in the performance of ChatGPT-4 and another LLM, BARD, with ChatGPT-4 generally exhibiting superior accuracy. This research contributes to understanding the capabilities and limitations of LLMs in processing and interpreting complex financial data from corporate documents.</jats:p>",13,10,257,257,Table (database); Computer science; Natural language processing; Artificial intelligence; Econometrics; Data mining; Economics,,,,,,http://dx.doi.org/10.3390/computers13100257,,10.3390/computers13100257,,,0,040-108-161-394-81X; 071-791-128-920-412; 077-236-611-019-515; 100-696-031-346-949; 181-432-462-344-434; 186-424-154-408-215,9,true,cc-by,gold
078-683-830-180-297,HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization,2025-01-01,2025,preprint,,,Elsevier BV,,Benjamin Coriat; Eric Benhamou,,,,,,Reinforcement learning; Portfolio; Reinforcement; Portfolio optimization; Computer science; Finance; Business; Artificial intelligence; Engineering; Structural engineering,,,,,,http://dx.doi.org/10.2139/ssrn.5365047,,10.2139/ssrn.5365047,,,0,,0,false,,
078-768-236-680-11X,"Enhancing Anomaly Detection in Financial Markets with an LLM-based
  Multi-Agent Framework",2024-03-28,2024,preprint,arXiv (Cornell University),,,,Taejin Park,"This paper introduces a Large Language Model (LLM)-based multi-agent framework designed to enhance anomaly detection within financial market data, tackling the longstanding challenge of manually verifying system-generated anomaly alerts. The framework harnesses a collaborative network of AI agents, each specialised in distinct functions including data conversion, expert analysis via web research, institutional knowledge utilization or cross-checking and report consolidation and management roles. By coordinating these agents towards a common objective, the framework provides a comprehensive and automated approach for validating and interpreting financial data anomalies. I analyse the S&P 500 index to demonstrate the framework's proficiency in enhancing the efficiency, accuracy and reduction of human intervention in financial market monitoring. The integration of AI's autonomous functionalities with established analytical methods not only underscores the framework's effectiveness in anomaly detection but also signals its broader applicability in supporting financial market monitoring.",,,,,Anomaly detection; Anomaly (physics); Business; Financial market; Finance; Computer science; Data mining; Physics; Condensed matter physics,,,,,https://arxiv.org/abs/2403.19735,http://dx.doi.org/10.48550/arxiv.2403.19735,,10.48550/arxiv.2403.19735,,,0,,1,true,,green
078-928-546-086-124,"The Evolution of Machine Learning Applications: Voice, Virtual Assistants, and Financial Decision-Making",2025-01-01,2025,preprint,,,Elsevier BV,,Linda F Harris,,,,,,Computer science; Finance; Business,,,,,,http://dx.doi.org/10.2139/ssrn.5400561,,10.2139/ssrn.5400561,,,0,,0,false,,
079-030-505-370-925,"Retrieval-augmented Large Language Models for Financial Time Series
  Forecasting",2025-02-09,2025,preprint,arXiv (Cornell University),,,,Mengxi Xiao; Zihao Jiang; Lingfei Qian; Zhengyu Chen; Yueru He; Yijing Xu; Yuecheng Jiang; Dong Li; Ruey-Ling Weng; Min Peng; Jimin Huang; Sophia Ananiadou; Qianqian Xie,"Stock movement prediction, a fundamental task in financial time-series forecasting, requires identifying and retrieving critical influencing factors from vast amounts of time-series data. However, existing text-trained or numeric similarity-based retrieval methods fall short in handling complex financial analysis. To address this, we propose the first retrieval-augmented generation (RAG) framework for financial time-series forecasting, featuring three key innovations: a fine-tuned 1B parameter large language model (StockLLM) as the backbone, a novel candidate selection method leveraging LLM feedback, and a training objective that maximizes similarity between queries and historically significant sequences. This enables our retriever, FinSeer, to uncover meaningful patterns while minimizing noise in complex financial data. We also construct new datasets integrating financial indicators and historical stock prices to train FinSeer and ensure robust evaluation. Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while FinSeer surpasses existing retrieval methods, achieving an 8\% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work underscores the importance of tailored retrieval models in financial forecasting and provides a novel framework for future research.",,,,,Series (stratigraphy); Computer science; Time series; Finance; Econometrics; Economics; Machine learning; Geology; Paleontology,,,,,https://arxiv.org/abs/2502.05878,http://dx.doi.org/10.48550/arxiv.2502.05878,,10.48550/arxiv.2502.05878,,,0,,0,true,,green
079-388-354-945-985,A Hybrid Lightweight LLM Chatbot for Sustainable Cryptocurrency Investment Decisions: Optimizing Small Models for Domain-Specific Performance,2025-07-08,2025,conference proceedings article,"2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC)",,IEEE,,Ali Shiri; Mikaeil Mayeli Feridani; Samira Keivanpour,,,,1740,1745,Chatbot; Cryptocurrency; Computer science; Domain (mathematical analysis); Investment (military); Computer security; World Wide Web; Mathematical analysis; Mathematics; Politics; Political science; Law,,,,Mitacs,,http://dx.doi.org/10.1109/compsac65507.2025.00236,,10.1109/compsac65507.2025.00236,,,0,009-790-728-439-136; 060-300-769-233-251; 143-510-770-212-129; 172-796-058-445-349; 182-709-129-451-768,0,false,,
079-641-743-487-750,Synergizing Investment Portfolios: A Technologically Advanced Approach with Integrated Chatbot Capabilities,2024-04-25,2024,conference proceedings article,"2024 MIT Art, Design and Technology School of Computing International Conference (MITADTSoCiCon)",,IEEE,,Priya. M. Shelke; Rajendra G. Pawar; Parth Pawar; Rupali Rathod; Tanishq Kawoor,,,,1,6,Chatbot; Computer science; Investment (military); World Wide Web; Politics; Political science; Law,,,,,,http://dx.doi.org/10.1109/mitadtsocicon60330.2024.10575311,,10.1109/mitadtsocicon60330.2024.10575311,,,0,014-288-820-659-77X; 036-318-384-104-745; 048-873-526-995-351; 050-549-770-216-476; 051-340-949-168-004; 062-006-364-404-011; 067-380-143-434-657; 136-267-115-280-693; 141-023-105-391-71X; 147-726-988-217-14X; 155-796-823-183-731; 158-888-011-720-252; 166-913-111-315-440,0,false,,
079-701-405-403-325,Interactive Financial Dashboard With Predictive Analytics And Chatbot Assistance,2025-11-02,2025,book chapter,Advances in Computer Science Research,29481961; 2352538x,Atlantis Press International BV,,A. Karthisha; G. Chetan Kumar; Ch. Guna Venkat Chowdary; S. Ojeshwar; S. Ram Prasad Reddy,,,,913,928,Dashboard; Chatbot; Analytics; Predictive analytics; Computer science; Data science; Finance; Business; World Wide Web,,,,,,http://dx.doi.org/10.2991/978-94-6463-858-5_77,,10.2991/978-94-6463-858-5_77,,,0,,0,true,cc-by-nc,hybrid
079-784-925-855-452,Using Large Language Models for Financial Advice,2025-01-01,2025,preprint,,,Elsevier BV,,Christian Fieberg; Lars Hornuf; Maximilian Meiler; David Streich,,,,,,Advice (programming); Finance; Computer science; Business; Programming language,,,,,,http://dx.doi.org/10.2139/ssrn.5133294,,10.2139/ssrn.5133294,,,0,007-570-784-407-502; 008-250-861-392-767; 009-122-864-785-531; 018-508-064-179-38X; 021-212-598-219-644; 023-379-578-217-307; 023-510-281-484-827; 023-853-922-616-880; 027-163-358-030-949; 033-799-091-679-810; 040-660-876-513-914; 042-492-265-651-624; 044-961-379-196-14X; 049-014-021-030-838; 051-201-561-722-62X; 052-027-687-931-983; 061-056-306-117-590; 061-173-128-168-696; 066-508-279-094-544; 072-878-583-167-977; 077-444-269-577-533; 077-882-683-245-032; 078-934-110-822-38X; 082-600-317-526-224; 083-989-154-270-555; 086-260-434-815-196; 087-044-222-911-370; 100-446-896-445-856; 100-707-306-967-308; 116-639-705-671-271; 116-653-450-604-253; 117-649-902-526-853; 121-323-404-465-87X; 121-570-640-268-403; 122-479-959-461-132; 128-079-345-767-474; 130-185-490-312-282; 132-412-858-453-658; 137-204-486-519-26X; 140-187-514-139-093; 147-185-050-695-707; 147-719-403-835-896; 153-731-994-523-956; 153-939-232-752-691; 154-513-179-142-971; 155-228-766-925-763; 157-787-315-927-958; 158-926-995-241-258; 171-389-867-596-062; 174-752-787-115-639; 181-432-462-344-434; 183-770-364-175-803; 196-395-945-746-897,1,false,,
079-909-772-141-614,A Dutch Financial Large Language Model,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Sander Noels; Jorne De Blaere; Tijl De Bie,"This paper presents FinGEITje, the first Dutch financial Large Language Model (LLM) specifically designed and optimized for various financial tasks. Together with the model, we release a specialized Dutch financial instruction tuning dataset with over 140,000 samples, constructed employing an automated translation and data processing method. The open-source data construction method is provided, facilitating the creation of financial instruction datasets in different languages. To evaluate model performance, the study introduces the first Dutch financial evaluation benchmark, along with an automated evaluation method that utilizes an LLM as an independent evaluator, reducing manual intervention in performance evaluation. The experimental results highlight the superior performance of FinGEITje across five critical Dutch and English financial tasks.",,,283,291,Computer science; Benchmark (surveying); Language model; Finance; Financial modeling; Artificial intelligence; Machine learning; Natural language processing; Economics; Geodesy; Geography,,,,Vlaamse Overheid; Agentschap Innoveren en Ondernemen,https://dl.acm.org/doi/pdf/10.1145/3677052.3698628 https://doi.org/10.1145/3677052.3698628 http://arxiv.org/pdf/2410.12835 http://arxiv.org/abs/2410.12835,http://dx.doi.org/10.1145/3677052.3698628,,10.1145/3677052.3698628,,,0,004-853-485-857-424; 057-982-454-558-150; 089-312-271-916-486; 090-417-304-783-146; 092-770-866-733-36X; 146-335-085-869-417,0,true,cc-by,hybrid
080-044-820-111-424,Using Chatbot Technologies to Help Individuals Make Sound Personalized Financial Decisions,2021-12-02,2021,conference proceedings article,2021 IEEE International Humanitarian Technology Conference (IHTC),,IEEE,,Reshawn Ramjattan; Patrick Hosein; Nigel Henry,"Those that struggle with debt and poor financial management can find many sources of online content to increase their financial literacy. However, consumption of educational content often does not translate to behavioural change. Software-based on just-in-time education can make use of nudge theory and gamification to help increase financial literacy while also influencing positive financial behaviour. Using chatbot technology as a conversational interface to this educational software increases convenience and usability, especially among those that struggle with learning to use technology. We present the design for a financial education chatbot that defines core situations for providing suggestions and information to users. We also used a survey to evaluate its usefulness and found that 82 per cent of 68 participants consider the chatbot strongly beneficial to their financial education and behaviour.",,,1,4,Chatbot; Financial literacy; Usability; Debt; Computer science; Finance; World Wide Web; Business; Human–computer interaction,,,,,,http://dx.doi.org/10.1109/ihtc53077.2021.9698928,,10.1109/ihtc53077.2021.9698928,,,2,000-260-463-025-177; 001-888-580-574-220; 008-868-747-452-214; 009-395-554-394-627; 019-905-091-856-150; 024-632-304-490-333; 027-155-565-602-022; 033-146-917-697-68X; 051-607-610-902-126; 058-150-899-658-693; 091-066-370-334-597; 108-007-135-053-82X; 117-203-880-254-001; 139-385-434-966-824,11,false,,
080-118-574-462-681,"Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language
  Models",2025-03-24,2025,preprint,arXiv (Cornell University),,,,Pin-Yu Chen; Han Shen; Payel Das; Tianyi Chen,"Fine-tuning Large Language Models (LLMs) on some task-specific datasets has been a primary use of LLMs. However, it has been empirically observed that this approach to enhancing capability inevitably compromises safety, a phenomenon also known as the safety-capability trade-off in LLM fine-tuning. This paper presents a theoretical framework for understanding the interplay between safety and capability in two primary safety-aware LLM fine-tuning strategies, providing new insights into the effects of data similarity, context overlap, and alignment loss landscape. Our theoretical results characterize the fundamental limits of the safety-capability trade-off in LLM fine-tuning, which are also validated by numerical experiments.",,,,,Fine-tuning; Business; Computer science; Physics; Particle physics,,,,,https://arxiv.org/abs/2503.20807,http://dx.doi.org/10.48550/arxiv.2503.20807,,10.48550/arxiv.2503.20807,,,0,,0,true,,green
080-671-300-636-633,"Decision-informed Neural Networks with Large Language Model Integration
  for Portfolio Optimization",2025-02-02,2025,preprint,arXiv (Cornell University),,,,Yoontae Hwang; Yaxuan Kong; Stefan Zohren; Yongjae Lee,"This paper addresses the critical disconnect between prediction and decision quality in portfolio optimization by integrating Large Language Models (LLMs) with decision-focused learning. We demonstrate both theoretically and empirically that minimizing the prediction error alone leads to suboptimal portfolio decisions. We aim to exploit the representational power of LLMs for investment decisions. An attention mechanism processes asset relationships, temporal dependencies, and macro variables, which are then directly integrated into a portfolio optimization layer. This enables the model to capture complex market dynamics and align predictions with the decision objectives. Extensive experiments on S\&P100 and DOW30 datasets show that our model consistently outperforms state-of-the-art deep learning models. In addition, gradient-based analyses show that our model prioritizes the assets most crucial to decision making, thus mitigating the effects of prediction errors on portfolio performance. These findings underscore the value of integrating decision objectives into predictions for more robust and context-aware portfolio management.",,,,,Portfolio; Computer science; Artificial neural network; Artificial intelligence; Machine learning; Business; Finance,,,,,https://arxiv.org/abs/2502.00828,http://dx.doi.org/10.48550/arxiv.2502.00828,,10.48550/arxiv.2502.00828,,,0,,0,true,,green
080-828-384-981-970,FinLlama: LLM-Based Financial Sentiment Analysis for Algorithmic Trading,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Giorgos Iacovides; Thanos Konstantinidis; Mingxue Xu; Danilo Mandic,"Online sources of financial news have a profound influence on both market movements and trading decisions. Standard sentiment analysis employs a lexicon-based approach to aid financial decisions, but struggles with context sensitivity and word ordering. On the other hand, Large Language Models (LLMs) are powerful, but are not finance-specific and require significant computational resources. To this end, we introduce a finance specific LLM framework, based on the Llama 2 7B foundational model, in order to benefit from its generative nature and comprehensive language manipulation. Such a generator-discriminator scheme, referred to as FinLlama, both classifies sentiment valence and quantifies its strength, offering a nuanced insight into financial news. The FinLlama model is fine-tuned on supervised financial sentiment analysis data, to make it handle the complexities of financial lexicon and context, and is equipped with a neural network-based decision mechanism. The subsequent parameter-efficient fine-tuning optimises trainable parameters, thus minimising computational and memory requirements without sacrificing accuracy. Simulation results demonstrate the ability of FinLlama to increase market returns in portfolio management scenarios, yielding high-return and resilient portfolios, even during volatile periods.",,,134,141,Computer science; Sentiment analysis; Finance; Artificial intelligence; Business,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698696 https://doi.org/10.1145/3677052.3698696,http://dx.doi.org/10.1145/3677052.3698696,,10.1145/3677052.3698696,,,0,004-853-485-857-424; 006-399-176-500-152; 044-019-077-336-343; 057-982-454-558-150; 058-918-036-879-234; 080-600-379-598-503; 090-829-135-229-996; 168-081-247-605-520; 184-487-387-969-895; 190-624-303-228-420,18,true,cc-by,hybrid
080-915-956-275-066,Enhancing Financial Forecasting with Large Language Models: Capabilities and Challenges,,2025,journal article,Procedia Computer Science,18770509,Elsevier BV,,Achraf Wali; Isaac Lera; Lamia Hadrich Belguith; Antoni Jaume-i-Capó,,270,,1978,1985,,,,,,,http://dx.doi.org/10.1016/j.procs.2025.09.318,,10.1016/j.procs.2025.09.318,,,0,003-751-429-300-847; 016-469-035-664-480; 018-866-913-622-957; 075-259-264-765-001; 177-575-126-909-348,0,false,,
081-253-420-497-353,Web-based chatbot for basic financial and mortgage services,2023-05-05,2023,conference proceedings article,2023 2nd International Conference on Vision Towards Emerging Trends in Communication and Networking Technologies (ViTECoN),,IEEE,,Atharva Thodge; Harsh M; C. Pretty Diana Cyril,"A Chatbot often also called as conversational interfaces, enable users a new method to converse with the computers. Traditionally, any mundane query would require a human to resolve them and therefore there were limitations to it, like time and knowledge of that particular human. But with the onset of the chatbot system the experience is like getting your queries resolved by a human but there are no common limitations to the same. Many businesses have developed their own chatbots to help their customers with their needs. Our chatbot is a financial services chatbot which specializes in lending and mortgage services.",,,1,5,Chatbot; Converse; Computer science; World Wide Web; Web application; Mathematics; Geometry,,,,,,http://dx.doi.org/10.1109/vitecon58111.2023.10157424,,10.1109/vitecon58111.2023.10157424,,,0,001-536-892-769-300; 007-581-025-024-301; 024-884-677-827-470; 089-414-800-314-825; 134-136-252-227-167; 158-154-417-527-126,1,false,,
081-457-444-249-180,From text to trade: harnessing the potential of generative AI for investor sentiment analysis in financial markets through large language models,2025-07-01,2025,journal article,International Journal of Information Technology,25112104; 25112112,Springer Science and Business Media LLC,,Nouri Hicham; Nassera Habbat,,,,,,Computer science; Generative grammar; Sentiment analysis; Financial market; Natural language processing; Artificial intelligence; Finance; Business,,,,,,http://dx.doi.org/10.1007/s41870-025-02622-w,,10.1007/s41870-025-02622-w,,,0,000-229-653-453-046; 009-044-278-158-331; 023-277-201-000-119; 025-056-996-352-460; 031-280-767-518-002; 032-077-250-652-21X; 038-409-574-648-694; 051-192-593-639-714; 051-625-957-657-974; 052-533-333-893-926; 053-105-124-479-927; 058-189-010-098-424; 070-360-035-700-487; 081-027-703-496-494; 096-108-678-238-715; 098-716-987-193-081; 105-956-835-654-883; 106-507-415-390-116; 110-396-493-163-969; 114-729-682-190-42X; 121-894-845-078-76X; 125-861-047-716-434; 126-524-448-163-286; 156-043-365-777-179; 166-416-707-027-119; 175-316-486-626-713; 176-007-843-971-791; 179-904-590-926-026; 195-452-262-700-285; 197-549-332-090-915,1,false,,
081-516-075-265-026,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Research Management Tools",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Research management; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905471,https://guides.library.harvard.edu/c.php?g=1082940&p=7905471,,,3090810995,,0,,0,false,,
081-575-396-119-484,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Pawan Kumar Rajpoot; Ankur Parikh,"Relation extraction (RE) is a crucial task in natural language processing (NLP) that aims to identify and classify relationships between entities mentioned in text. In the financial domain, relation extraction plays a vital role in extracting valuable information from financial documents, such as news articles, earnings reports, and company filings. This paper describes our solution to relation extraction on one such dataset REFinD. The dataset was released along with shared task as a part of the Fourth Workshop on Knowledge Discovery from Unstructured Data in Financial Services, co-located with SIGIR 2023. In this paper, we employed OpenAI models under the framework of in-context learning (ICL). We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example. The first retrieval mechanism, we employed, is a learning-free dense retriever and the other system is a learning-based retriever. We were able to achieve 3rd rank overall. Our best F1-score is 0.718.",,,,,Relationship extraction; Computer science; Relation (database); Context (archaeology); Task (project management); Named-entity recognition; Domain (mathematical analysis); Rank (graph theory); Artificial intelligence; Natural language processing; Information extraction; Machine learning; Information retrieval; Finance; Data mining; Business; Management; Paleontology; Mathematical analysis; Mathematics; Combinatorics; Biology; Economics,,,,,https://arxiv.org/abs/2306.17519,http://dx.doi.org/10.48550/arxiv.2306.17519,,10.48550/arxiv.2306.17519,,,0,,0,true,cc-by,green
081-653-838-702-361,Can GPT Help Improve Robo-advisory? The Construction of Robo-advisor for Users with Low Investment Experience Based on LLM,2024-07-31,2024,journal article,"Advances in Economics, Management and Political Sciences",27541169; 27541177,EWA Publishing,,Zeyu Feng,"<jats:p>Robo-advisors have emerged as a significant innovation in investment management, offering automated financial advice to investors. However, user acceptance remains a challenge, particularly among those with limited investment experience. This paper explores the potential of large language models (LLMs) to enhance the interaction attributes of robo-advisor products and increase acceptance among novice investors. The study contributes to the existing literature by exploring the application of LLMs in robo-advisors, supplementing the exploration of interaction design, and systematically reviewing the service processes of current robo-advisor products. Findings suggest that existing robo-advisor products have room for improvement in interaction attributes and algorithmic mechanisms. Through theoretical exploration, this paper proposes methods for optimizing robo-advisor products by integrating LLMs. In conclusion, this research lays the groundwork for designing robo-advisor products with integrated LLM functionality, offering theoretical references for practitioners and researchers in financial technology. Future research directions include exploring user expectations and conducting controlled experiments to analyze the impact of LLM integration on user decisions.</jats:p>",90,1,26,41,Investment (military); Service (business); Knowledge management; Business; Finance; Computer science; Marketing; Political science; Politics; Law,,,,,https://www.ewadirect.com/proceedings/aemps/article/view/14589/pdf https://doi.org/10.54254/2754-1169/90/20241947,http://dx.doi.org/10.54254/2754-1169/90/20241947,,10.54254/2754-1169/90/20241947,,,0,,1,true,cc-by,hybrid
082-038-978-085-74X,Human-AI Interaction in FinTech: The Prospects and Risks of Using Chatbots as Conversational Agents in Financial Customer Service,,2024,book chapter,"Studies in Systems, Decision and Control",21984182; 21984190,Springer Nature Switzerland,,Aysha Alaamer; Anupama Prasanth,,,,489,502,Chatbot; Business; Dialog system; Customer service; Financial services; Service (business); Knowledge management; Computer science; Marketing; Finance; World Wide Web; Dialog box,,,,,,http://dx.doi.org/10.1007/978-3-031-71649-2_42,,10.1007/978-3-031-71649-2_42,,,0,001-536-892-769-300; 007-581-025-024-301; 009-827-059-232-31X; 025-512-013-524-564; 036-384-294-609-358; 076-316-406-643-902; 085-250-227-675-391; 105-753-084-464-981; 111-495-971-544-951; 130-751-685-397-830; 135-189-641-687-525; 147-024-863-114-722,2,false,,
082-072-115-316-951,Financial Statement Analysis with Large Language Models,2024-07-25,2024,preprint,arXiv (Cornell University),,,,Alex Kim; Maximilian Muhn; Valeri Nikolaev,"We investigate whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of future earnings. Even without any narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, we find that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, we find that the LLM generates useful narrative insights about a company's future performance. Lastly, our trading strategies based on GPT's predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, our results suggest that LLMs may take a central role in decision-making.",,,,,Statement (logic); Financial statement analysis; Financial statement; Business; Financial analysis; Linguistics; Finance; Accounting; Philosophy; Audit,,,,,https://arxiv.org/abs/2407.17866,http://dx.doi.org/10.48550/arxiv.2407.17866,,10.48550/arxiv.2407.17866,,,0,,2,true,,green
082-188-971-152-392,The Impact of Large Language Models on Financial Shared Service Centers,2024-12-31,2024,journal article,Finance & Economics,29601312; 29596130,Dean & Francis Press,,Sitong Liu,"<jats:p>Using a case study approach, two representative companies, Henkel and Shanghai GFC Consulting, are examined for their application of the large model in their financial shared service centers. By analyzing the impact of two AI applications, Henkel’s SMART AR and Shanghai GFC Consulting’s G-smart, in the real world, the positive and significant impact of the emergence of large model AI applications on financial shared service centers is further analyzed. The feasibility of large-scale use of AI applications of large models in financial shared service centers can be found, and the rise of large language models has triggered a new round of financial changes in financial shared service centers, realizing the mutual empowerment of financial shared service centers and large language models. It brings many benefits to enterprise groups, such as reducing operating costs, improving operational efficiency, strengthening group control and improving service quality.</jats:p>",1,10,,,Business; Service (business); Marketing,,,,,,http://dx.doi.org/10.61173/0fcprs62,,10.61173/0fcprs62,,,0,,0,false,,
082-246-720-948-743,"<p>Financial Consumer Protection Suite with Next-Generation, AI-Powered, Chatbot-Supported Complaints Management System</p>",,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Simone di Castri; Nathalie Lenehan,,,,,,Chatbot; Suite; Business; Computer science; World Wide Web; Political science; Law,,,,,,http://dx.doi.org/10.2139/ssrn.5386761,,10.2139/ssrn.5386761,,,0,,0,false,,
082-319-578-424-029,Prevention of Prompt Injection Attacks Over Financial Applications Integrated with LLM,2025-04-17,2025,conference proceedings article,2025 3rd International Conference on Advancement in Computation & Computer Technologies (InCACCT),,IEEE,,Tanaya Joshi; Vaishnavi Naik; Isha Mistry; Ramchandra Mangrulkar,,,,225,230,Computer science; Business; Computer security,,,,,,http://dx.doi.org/10.1109/incacct65424.2025.11011372,,10.1109/incacct65424.2025.11011372,,,0,023-963-049-185-189; 039-868-093-827-56X; 045-221-846-690-175; 099-478-914-260-162; 195-194-313-204-726,0,false,,
082-413-414-654-793,A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Ritwik Sinha; Zhao Song; Tianyi Zhou,"Large Language Models have become popular for their remarkable capabilities in human-oriented tasks and traditional natural language processing tasks. Its efficient functioning is attributed to the attention mechanism in the Transformer architecture, enabling it to concentrate on particular aspects of the input. LLMs are increasingly being used in domains such as generating prose, poetry or art, which require the model to be creative (e.g. Adobe firefly). LLMs possess advanced language generation abilities that enable them to generate distinctive and captivating content. This utilization of LLMs in generating narratives shows their flexibility and potential for use in domains that extend beyond conventional natural language processing duties. In different contexts, we may expect the LLM to generate factually correct answers, that match reality; e.g., question-answering systems or online assistants. In such situations, being correct is critical to LLMs being trusted in practice. The Bing Chatbot provides its users with the flexibility to select one of the three output modes: creative, balanced, and precise. Each mode emphasizes creativity and factual accuracy differently. In this work, we provide a mathematical abstraction to describe creativity and reality based on certain losses. A model trained on these losses balances the trade-off between the creativity and reality of the model.",,,,,Creativity; Computer science; Abstraction; Flexibility (engineering); Narrative; Natural language; Human–computer interaction; Artificial intelligence; Linguistics; Psychology; Epistemology; Social psychology; Management; Philosophy; Economics,,,,,https://arxiv.org/abs/2306.02295,http://dx.doi.org/10.48550/arxiv.2306.02295,,10.48550/arxiv.2306.02295,,,0,,1,true,cc-by-nc-sa,green
082-699-701-589-935,XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: ACL 2025,,Association for Computational Linguistics,,Zhihan Zhang; Yixin Cao; Lizi Liao,,,,8715,8758,Benchmarking; Case-based reasoning; Computer science; Economics; Artificial intelligence; Management,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-acl.457,,10.18653/v1/2025.findings-acl.457,,,0,,1,false,,
082-865-212-054-324,"AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative
  Contracts",2025-06-01,2025,preprint,arXiv (Cornell University),,,,Maruf Ahmed Mridul; Ian Sloyan; Aparna Gupta; Oshani Seneviratne,"Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are reshaping how AI systems extract and organize information from unstructured text. A key challenge is designing AI methods that can incrementally extract, structure, and validate information while preserving hierarchical and contextual relationships. We introduce CDMizer, a template-driven, LLM, and RAG-based framework for structured text transformation. By leveraging depth-based retrieval and hierarchical generation, CDMizer ensures a controlled, modular process that aligns generated outputs with predefined schema. Its template-driven approach guarantees syntactic correctness, schema adherence, and improved scalability, addressing key limitations of direct generation methods. Additionally, we propose an LLM-powered evaluation framework to assess the completeness and accuracy of structured representations. Demonstrated in the transformation of Over-the-Counter (OTC) financial derivative contracts into the Common Domain Model (CDM), CDMizer establishes a scalable foundation for AI-driven document understanding, structured synthesis, and automated validation in broader contexts.",,,,,Derivative (finance); Business; Encoding (memory); Finance; Computer science; Artificial intelligence,,,,,https://arxiv.org/abs/2506.01063,http://dx.doi.org/10.48550/arxiv.2506.01063,,10.48550/arxiv.2506.01063,,,0,,0,true,,green
082-882-908-494-998,"Enhancing literature review with LLM and NLP methods. Algorithmic
  trading case",2024-10-23,2024,preprint,arXiv (Cornell University),,,,Stanisław Łaniewski; Robert Ślepaczuk,"This study utilizes machine learning algorithms to analyze and organize knowledge in the field of algorithmic trading. By filtering a dataset of 136 million research papers, we identified 14,342 relevant articles published between 1956 and Q1 2020. We compare traditional practices-such as keyword-based algorithms and embedding techniques-with state-of-the-art topic modeling methods that employ dimensionality reduction and clustering. This comparison allows us to assess the popularity and evolution of different approaches and themes within algorithmic trading. We demonstrate the usefulness of Natural Language Processing (NLP) in the automatic extraction of knowledge, highlighting the new possibilities created by the latest iterations of Large Language Models (LLMs) like ChatGPT. The rationale for focusing on this topic stems from our analysis, which reveals that research articles on algorithmic trading are increasing at a faster rate than the overall number of publications. While stocks and main indices comprise more than half of all assets considered, certain asset classes, such as cryptocurrencies, exhibit a much stronger growth trend. Machine learning models have become the most popular methods in recent years. The study demonstrates the efficacy of LLMs in refining datasets and addressing intricate questions about the analyzed articles, such as comparing the efficiency of different models. Our research shows that by decomposing tasks into smaller components and incorporating reasoning steps, we can effectively tackle complex questions supported by case analyses. This approach contributes to a deeper understanding of algorithmic trading methodologies and underscores the potential of advanced NLP techniques in literature reviews.",,,,,Computer science; Artificial intelligence; Natural language processing; Machine learning; Data science,,,,,https://arxiv.org/abs/2411.05013,http://dx.doi.org/10.48550/arxiv.2411.05013,,10.48550/arxiv.2411.05013,,,0,,0,true,,green
083-184-817-259-27X,Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains,,2024,conference proceedings article,Findings of the Association for Computational Linguistics ACL 2024,,Association for Computational Linguistics,,Marcio Fonseca; Shay Cohen,,,,8027,8042,Computer science; Annotation; Natural language processing; Data science; Artificial intelligence,,,,,https://arxiv.org/pdf/2311.08704 https://arxiv.org/abs/2311.08704,http://dx.doi.org/10.18653/v1/2024.findings-acl.478,,10.18653/v1/2024.findings-acl.478,,,0,,0,true,,green
083-198-783-596-08X,A Study on Factors Influencing Continuous Usage Intention of Chatbot Services in South Korean Financial Institutions,2025-04-05,2025,journal article,International Journal of Financial Studies,22277072,MDPI AG,,Yeun-su Choi; Seung-zoon Lee; Jeongil Choi,"<jats:p>The South Korean AI market has grown significantly, yet while chatbot adoption is well-studied, sustained use remains underexplored. This study surveyed 250 financial institution chatbot users in South Korea in February 2024, using SPSS and R to investigate quality effects on intention of continuous usage. Results showed that social presence, response accuracy, assurance, and interactivity positively influenced expectation confirmation. In contrast, responsiveness, reliability, and usability had no significant effect, while security negatively impacted frequent users, reflecting trust concerns. These findings guide financial institutions in enhancing chatbot retention through interactivity and trust. Conducted in a pre-generative AI context, this study suggests future longitudinal research to address evolving AI technologies and user behaviors.</jats:p>",13,2,56,56,Chatbot; Business; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.3390/ijfs13020056,,10.3390/ijfs13020056,,,0,007-538-491-635-579; 008-230-815-720-725; 009-951-353-757-924; 023-161-206-593-57X; 025-399-519-844-07X; 028-973-297-055-94X; 031-310-070-860-152; 031-824-009-360-923; 032-637-445-493-370; 038-080-463-906-397; 038-925-283-945-155; 042-987-124-456-432; 048-571-241-240-648; 055-807-088-113-125; 056-410-373-664-795; 067-962-635-457-169; 068-864-529-160-799; 077-923-894-357-710; 077-946-279-594-919; 079-110-907-291-197; 096-718-684-971-428; 100-844-882-568-816; 106-518-329-252-175; 107-219-832-197-990; 124-168-437-732-104; 126-983-838-445-898; 144-246-611-527-170; 168-036-345-097-619; 169-168-861-110-999,1,true,cc-by,gold
083-489-102-388-067,Designing Heterogeneous LLM Agents for Financial Sentiment Analysis,2025-02-10,2025,journal article,ACM Transactions on Management Information Systems,2158656x; 21586578,Association for Computing Machinery (ACM),United States,Frank Xing,"<jats:p>Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focus from massive data acquisition and new model training to human alignment and strategic elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA) due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage existing generative models in such a context. This study investigates the effectiveness of the new paradigm, that is, using LLMs without fine-tuning for FSA. Rooted in Minsky’s theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed and applied to FSA. The framework instantiates specialized agents using prior guiding knowledge from both linguistics and finance. Then, a summative agent reasons on the aggregated agent discussions. Comprehensive evaluations using six FSA datasets show that the framework yields better accuracies compared to many alternative multi-LLM agent settings, especially when the discussion contents are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA and potentially other tasks. Implications for business and management have also been discussed.</jats:p>",16,1,1,24,Leverage (statistics); Summative assessment; Context (archaeology); Computer science; Generative grammar; Knowledge management; Artificial intelligence; Formative assessment; Psychology; Paleontology; Pedagogy; Biology,,,,Singapore Ministry of Education Academic Research Fund,https://dl.acm.org/doi/pdf/10.1145/3688399 https://doi.org/10.1145/3688399 https://arxiv.org/pdf/2401.05799 https://arxiv.org/abs/2401.05799,http://dx.doi.org/10.1145/3688399,,10.1145/3688399,,,0,004-853-485-857-424; 006-399-176-500-152; 008-102-153-925-863; 014-503-732-193-606; 014-613-376-281-980; 019-224-057-563-30X; 019-642-621-498-863; 028-169-927-550-228; 036-974-218-967-030; 037-853-357-392-375; 039-348-892-969-119; 047-773-957-263-019; 049-177-639-368-845; 051-161-858-118-526; 057-040-097-643-496; 057-982-454-558-150; 065-751-794-789-985; 070-360-035-700-487; 070-504-773-098-766; 071-423-367-050-689; 081-222-858-750-630; 083-449-940-348-930; 090-135-685-849-51X; 091-429-168-008-78X; 092-453-382-594-560; 095-053-794-199-504; 096-704-258-326-787; 098-415-082-667-239; 109-184-420-030-073; 110-510-842-185-282; 122-339-922-726-171; 134-795-837-065-860; 137-548-243-845-043; 139-711-048-505-537; 141-101-292-256-222; 146-073-788-869-088; 146-335-085-869-417; 149-506-605-473-137; 150-397-545-090-569; 155-498-300-221-705; 158-668-811-647-562; 169-806-297-534-97X; 172-594-396-767-011; 176-346-988-774-760; 186-221-799-922-73X; 188-489-248-199-012; 192-806-908-794-847; 193-395-379-323-072; 194-863-934-924-67X; 195-697-682-736-933,36,true,,bronze
083-489-475-210-608,Semantic Understanding Meets Financial Decision Support: A Review of LLM-Augmented Analysis of Unstructured Financial Reports,2025-09-19,2025,preprint,,,Institute of Electrical and Electronics Engineers (IEEE),,Trent S. Reams; Alex Carter; Jacob Nguyen,,,,,,Computer science; Unstructured data; Finance; Business; Data mining; Big data,,,,,https://www.techrxiv.org/doi/pdf/10.36227/techrxiv.175825794.48682435/v1 https://doi.org/10.36227/techrxiv.175825794.48682435/v1,http://dx.doi.org/10.36227/techrxiv.175825794.48682435/v1,,10.36227/techrxiv.175825794.48682435/v1,,,0,,0,true,,unknown
084-028-598-567-636,Evaluating Company-specific Biases in Financial Sentiment Analysis using Large Language Models,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Kei Nakagawa; Masanori Hirano; Yugo Fujimoto,,,,6614,6623,Sentiment analysis; Computer science; Language model; Natural language processing; Artificial intelligence,,,,,http://arxiv.org/pdf/2411.00420 http://arxiv.org/abs/2411.00420,http://dx.doi.org/10.1109/bigdata62323.2024.10826008,,10.1109/bigdata62323.2024.10826008,,,0,007-401-074-296-919; 014-554-361-946-25X; 022-353-383-534-204; 022-916-079-093-025; 026-050-474-503-865; 033-044-731-120-869; 035-014-823-490-790; 040-530-686-977-958; 040-672-185-519-039; 050-836-640-545-601; 051-448-485-398-729; 057-819-837-905-711; 069-507-976-179-943; 069-560-796-007-725; 075-356-136-044-036; 101-520-573-807-639; 111-343-234-678-99X; 112-890-203-635-590; 127-982-575-913-793; 128-211-512-285-962; 137-408-709-935-186; 137-925-447-012-933; 139-239-027-110-484; 160-739-948-066-491; 173-116-597-012-168,3,true,,green
084-062-553-476-828,SCC - Chatbot Assisted Marketing in Financial Service Industry,2019-06-19,2019,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer International Publishing,Germany,Jon T. S. Quah; Y. W. Chua,The rise of chatbots in the finance sector is the latest disruptive force that has change the way customers interact. The adoption of Artificial Intelligence powered chatbots particularly in the banking industry has changed the face of communication interface between bank and customers. This paper explores the effectiveness of the current use of chatbot in Singapore’s banking industry. The banking sector in Singapore play a significant role in Singapore economy. It also investigates the current chatbot functionality to determine if it can meet the ever-changing expectation of customers.,,,107,114,Financial services; Business; Marketing; Service level; Chatbot; Face (sociological concept); Banking industry; Communication interface; Banking sector,,,,,https://link.springer.com/chapter/10.1007/978-3-030-23554-3_8 https://link.springer.com/content/pdf/10.1007%2F978-3-030-23554-3_8.pdf https://rd.springer.com/chapter/10.1007/978-3-030-23554-3_8 https://dblp.uni-trier.de/db/conf/scc2/scc2019.html#QuahC19,http://dx.doi.org/10.1007/978-3-030-23554-3_8,,10.1007/978-3-030-23554-3_8,2952337279,,0,000-557-690-731-810; 015-770-840-146-826; 023-742-217-441-622; 152-395-699-462-250; 175-693-082-794-95X; 194-888-667-557-027,40,false,,
084-736-365-791-490,Simulating Financial Market via Large Language Model based Agents,2024-06-28,2024,preprint,arXiv (Cornell University),,,,Shen Gao; Yuntao Wen; Minghang Zhu; Jianing Wei; Yuhan Cheng; Qunzi Zhang; Shuo Shang,"Most economic theories typically assume that financial market participants are fully rational individuals and use mathematical models to simulate human behavior in financial markets. However, human behavior is often not entirely rational and is challenging to predict accurately with mathematical models. In this paper, we propose \textbf{A}gent-based \textbf{S}imulated \textbf{F}inancial \textbf{M}arket (ASFM), which first constructs a simulated stock market with a real order matching system. Then, we propose a large language model based agent as the stock trader, which contains the profile, observation, and tool-learning based action module. The trading agent can comprehensively understand current market dynamics and financial policy information, and make decisions that align with their trading strategy. In the experiments, we first verify that the reactions of our ASFM are consistent with the real stock market in two controllable scenarios. In addition, we also conduct experiments in two popular economics research directions, and we find that conclusions drawn in our \model align with the preliminary findings in economics research. Based on these observations, we believe our proposed ASFM provides a new paradigm for economic research.",,,,,Financial market; Finance; Business; Computer science,,,,,https://arxiv.org/abs/2406.19966,http://dx.doi.org/10.48550/arxiv.2406.19966,,10.48550/arxiv.2406.19966,,,0,,0,true,,green
084-893-437-417-684,The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,2025-02-11,2025,preprint,,,Center for Open Science,,Minkyu Shin; Jin Kim; Jiwoong Shin,"<p>Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. Utilizing a leading AI detection tool, we analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We establish a causal relationship between LLM usage and an increased likelihood of obtaining relief by employing instrumental variables to address endogeneity in LLM adoption. Experimental data further support this link, demonstrating that LLMs enhance the clarity and persuasiveness of consumer narratives. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.</p>",,,,,Business; Marketing,,,,,,http://dx.doi.org/10.31234/osf.io/fdzqg_v2,,10.31234/osf.io/fdzqg_v2,,,0,,0,true,,bronze
085-183-136-082-964,DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Wei Chen; Qiushi Wang; Zefei Long; Xianyin Zhang; Zhongtian Lu; Bingxuan Li; Siyuan Wang; Jiarong Xu; Xiang Bai; Xuanjing Huang; Zhongyu Wei,"We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.",,,,,Computer science; Baseline (sea); Computation; Domain (mathematical analysis); Language model; Question answering; Artificial intelligence; Natural language processing; Finance; Machine learning; Programming language; Mathematical analysis; Oceanography; Mathematics; Economics; Geology,,,,,https://arxiv.org/abs/2310.15205,http://dx.doi.org/10.48550/arxiv.2310.15205,,10.48550/arxiv.2310.15205,,,0,,0,true,other-oa,green
086-153-181-013-975,Trading-Chatbot verspricht individualisierten Service,2020-10-30,2020,journal article,Bankmagazin,09443223; 21928770,Springer Science and Business Media LLC,,Stefanie Hüthig,,69,11,50,50,,,,,,http://link.springer.com/content/pdf/10.1007/s35127-020-0628-4.pdf,http://dx.doi.org/10.1007/s35127-020-0628-4,,10.1007/s35127-020-0628-4,3096103947,,0,,0,false,,
086-445-751-597-47X,Event-Driven Alpha from Large Language Models: A Theoretical Framework for Contrastive Financial Representation Learning,,2025,preprint,,,Elsevier BV,,Igor Jouravlev,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5835542,,10.2139/ssrn.5835542,,,0,,0,false,,
086-730-647-813-593,Sentiment Spin: Attacking Financial Sentiment with GPT-3,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Markus Leippold,"The use of dictionaries in financial sentiment analysis and other financial and economic applications remains widespread because keyword-based methods appear more transparent and explainable than more advanced techniques commonly used in computer science. However, this paper demonstrates the vulnerability of using dictionaries by exploiting the eloquence of GPT-3, a sophisticated transformer model, to generate successful adversarial attacks on keyword-based approaches with a success rate close to 99% for negative sentences in the financial phrase base, a well-known human-annotated database for financial sentiment analysis. In contrast, more advanced methods, such as those using context-aware approaches like BERT, remain robust.",,,,,Sentiment analysis; Business; Computer science; Financial system; Artificial intelligence,,,,,https://www.zora.uzh.ch/id/eprint/234515/1/1_s2.0_S154461232300329X_main.pdf https://www.zora.uzh.ch/id/eprint/236069/1/Sentiment_Spin.pdf,http://dx.doi.org/10.2139/ssrn.4337182,,10.2139/ssrn.4337182,,,0,,3,true,,green
086-824-033-670-636,: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design_supp1-3593370.pdf,,,component,,,Institute of Electrical and Electronics Engineers (IEEE),,Yangyang Yu,,,,,,,,,,,,http://dx.doi.org/10.1109/tbdata.2025.3593370/mm1,,10.1109/tbdata.2025.3593370/mm1,,,0,,0,false,,
086-948-876-121-242,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Interdisciplinary resources",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905750,https://guides.library.harvard.edu/c.php?g=1082940&p=7905750,,,3093957027,,0,,0,false,,
087-152-900-647-969,Can We Statically Locate Knowledge in Large Language Models? Financial Domain and Toxicity Reduction Case Studies,,2024,conference proceedings article,Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,,Association for Computational Linguistics,,Jordi Armengol-Estapé; Lingyu Li; Sebastian Gehrmann; Achintya Gopal; David S Rosenberg; Gideon S. Mann; Mark Dredze,,,,140,176,Reduction (mathematics); Computer science; Domain (mathematical analysis); Mathematics; Mathematical analysis; Geometry,,,,,,http://dx.doi.org/10.18653/v1/2024.blackboxnlp-1.9,,10.18653/v1/2024.blackboxnlp-1.9,,,0,,0,false,,
087-317-813-537-422,AI-Enabled Chatbot to Drive Marketing Automation for Financial Services,2021-07-14,2021,journal article,Journal of Management Information and Decision Sciences,15247252,,,Kiran S Nair; Suhaib Anagreh; Ambili Sunil; Ruchi Gupta,"The purpose of this study was to determine how AI-enabled chat bots could stimulate marketing automation for financial services. Secondary data from the Consumer Financial Protection Bureau (CFPB) was utilized to identify the most frequent issues customers cited in accessing financial services, products, and information. This information was then contextualized with both the identified challenges from the literature as well as the innate benefits of chat bots. From this, it was identified that chat bots not only offers a potential to enhance sales and marketing operations by enhancing both customer understanding and personalization of customer service and service delivery, but it is also the future of service delivery. Specifically, the study identified that chat bots are instrumental in sales and marketing automation, promotion of self-service type of customer support, and ensuring digital financial inclusion and financial sustainability. Therefore, in line with the prevalent knowledge economy, which is characterized by problem solving and critical thinking, chat bots serve the need of not only addressing present-day customer centrism requirements, but also aligning the delivery of financial services, products, and information with emergent trends and pattern. They do so by ingraining proactivity in banking and other financial institutions’ operations, and strategic outlook as they relate to customer experience, and product innovation.",24,,,,Financial services; Proactivity; Business; Knowledge economy; Marketing; Personalization; Chatbot; Financial inclusion; Service delivery framework; Product innovation,,,,,https://www.abacademies.org/articles/aienabled-chatbot-to-drive-marketing-automation-for-financial-services.pdf https://www.abacademies.org/articles/aienabled-chatbot-to-drive-marketing-automation-for-financial-services-11516.html,https://www.abacademies.org/articles/aienabled-chatbot-to-drive-marketing-automation-for-financial-services-11516.html,,,3182126418,,0,,0,false,,
087-739-821-614-205,Exploring users' adoption intentions of intelligent virtual assistants in financial services: An anthropomorphic perspectives and socio-psychological perspectives,,2023,journal article,Computers in Human Behavior,07475632; 18737692,Elsevier BV,United Kingdom,Bhanu Priya; Vivek Sharma,,148,,107912,107912,Chatbot; Psychology; Financial services; Animacy; Context (archaeology); Service (business); Customer service; Knowledge management; Social psychology; Applied psychology; Marketing; Computer science; Finance; Business; Cognitive psychology; World Wide Web; Paleontology; Biology,,,,,,http://dx.doi.org/10.1016/j.chb.2023.107912,,10.1016/j.chb.2023.107912,,,0,000-240-551-842-478; 001-316-375-064-631; 001-734-495-526-441; 002-924-317-409-959; 003-118-998-063-278; 004-195-800-920-568; 006-371-191-935-459; 007-187-940-345-891; 007-454-697-247-661; 008-972-388-060-058; 009-299-623-546-736; 010-260-594-255-856; 011-057-105-680-492; 011-195-430-027-417; 011-742-143-072-995; 013-470-693-377-976; 013-553-718-914-082; 014-684-417-715-713; 015-524-112-483-919; 016-389-882-874-526; 017-153-238-198-411; 017-388-667-415-37X; 017-618-171-729-878; 017-728-756-779-456; 018-449-977-039-210; 018-600-398-397-40X; 019-442-492-453-514; 022-550-661-705-767; 024-745-271-770-785; 025-271-691-958-105; 026-570-259-486-299; 027-158-539-409-214; 027-835-440-496-763; 027-934-605-631-614; 028-190-026-087-674; 028-289-734-658-693; 028-504-544-061-465; 028-743-291-528-757; 029-819-130-333-152; 029-970-935-163-244; 030-678-140-590-784; 030-951-221-183-894; 031-240-068-166-515; 032-200-546-890-971; 032-587-245-037-159; 032-775-496-263-992; 033-171-354-077-456; 033-504-361-988-242; 033-525-740-901-402; 033-826-575-023-317; 035-606-231-893-667; 035-994-511-280-635; 037-154-238-083-286; 037-258-093-988-629; 039-169-568-114-81X; 039-240-335-182-562; 040-573-097-005-093; 041-340-621-976-144; 041-400-792-768-210; 041-447-015-497-020; 044-685-744-601-943; 047-393-916-312-588; 050-420-291-350-822; 050-597-169-069-889; 050-716-829-482-548; 051-116-859-808-366; 052-245-171-683-03X; 052-764-550-033-576; 052-893-645-460-643; 054-037-501-151-746; 054-994-955-531-158; 055-562-051-555-058; 055-594-760-386-281; 055-963-682-045-801; 058-062-562-310-747; 058-237-210-680-260; 058-687-501-124-289; 059-032-018-174-576; 061-678-143-259-418; 062-948-580-981-656; 063-233-158-964-102; 065-101-489-934-256; 065-324-719-324-575; 066-284-184-166-701; 067-624-070-521-861; 068-417-770-993-055; 069-697-443-425-048; 070-273-057-201-456; 072-813-199-606-608; 073-444-821-777-980; 074-601-462-746-690; 075-472-890-988-897; 075-963-043-022-297; 077-930-616-453-628; 078-692-001-546-161; 084-168-511-723-275; 084-720-591-769-228; 085-964-143-703-476; 087-356-437-994-292; 090-679-830-439-134; 093-068-966-014-086; 093-238-693-459-192; 094-135-298-524-825; 095-576-828-176-398; 096-327-786-483-777; 098-057-246-730-672; 099-701-379-993-979; 101-058-311-970-367; 102-766-923-614-703; 108-834-059-755-131; 110-515-937-016-281; 110-787-687-638-991; 112-705-785-604-366; 112-818-453-623-529; 114-039-986-924-13X; 115-902-399-774-860; 116-709-195-695-951; 116-953-447-460-185; 118-177-665-933-853; 119-275-549-696-155; 124-171-824-969-31X; 125-340-044-846-504; 127-235-399-599-352; 128-266-888-487-309; 128-698-850-224-123; 129-324-374-869-536; 131-568-073-527-092; 132-063-614-851-374; 132-792-457-519-924; 136-211-937-581-097; 138-440-879-598-406; 138-850-669-179-07X; 139-341-996-679-224; 139-728-534-969-805; 146-295-241-260-395; 148-774-975-818-934; 150-053-943-438-920; 150-887-963-505-239; 153-982-639-705-321; 156-315-025-619-39X; 156-920-525-462-451; 159-549-280-195-935; 163-195-257-812-770; 163-884-760-901-520; 172-853-937-648-263; 178-678-688-768-266; 183-919-412-917-297; 186-680-373-848-993,86,false,,
087-807-064-907-108,"Large Language Models and Sentiment Analysis in Financial Markets: A Review, Datasets, and Case Study",,2024,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers (IEEE),United States,Chenghao Liu; Arunkumar Arulappan; Ranesh Naha; Aniket Mahanti; Joarder Kamruzzaman; In-Ho Ra,"This paper comprehensively examines Large Language Models (LLMs) in sentiment analysis, specifically focusing on financial markets and exploring the correlation between news sentiment and Bitcoin prices.We systematically categorize various LLMs used in financial sentiment analysis, highlighting their unique applications and features.We also investigate the methodologies for effective data collection and categorization, underscoring the need for diverse and comprehensive datasets.Our research features a case study investigating the correlation between news sentiment and Bitcoin prices, utilizing advanced sentiment analysis and financial analysis methods to demonstrate the practical application of LLMs.The findings reveal a modest but discernible correlation between news sentiment and Bitcoin price fluctuations, with historical news patterns showing a more substantial impact on Bitcoin's longer-term price than immediate news events.This highlights LLMs' potential in market trend prediction and informed investment decision-making.",12,,134041,134061,Sentiment analysis; Computer science; Financial market; Natural language processing; Artificial intelligence; Finance; Economics,,,,"School of Computer Science Engineering and Information Systems, Vellore Institute of Technology",,http://dx.doi.org/10.1109/access.2024.3445413,,10.1109/access.2024.3445413,,,0,003-641-764-774-508; 004-215-338-766-112; 010-000-097-357-041; 010-423-042-177-734; 011-525-976-591-254; 015-038-258-198-324; 015-063-175-189-638; 020-858-402-731-766; 022-486-017-682-206; 025-736-030-541-131; 026-294-977-463-130; 029-736-315-308-945; 031-530-419-558-939; 033-868-500-936-600; 034-111-475-208-944; 038-508-124-511-192; 039-809-217-419-120; 040-195-777-853-381; 042-267-267-198-535; 044-456-506-571-452; 044-806-689-553-253; 046-352-365-586-832; 051-207-248-900-001; 056-444-941-497-91X; 056-542-756-996-88X; 057-982-454-558-150; 058-384-382-374-622; 062-027-974-769-598; 063-299-253-494-657; 066-277-557-325-395; 070-360-035-700-487; 071-602-982-256-295; 074-239-662-148-901; 079-326-466-198-087; 079-927-095-270-775; 080-600-379-598-503; 080-813-398-501-825; 081-108-374-405-444; 082-942-709-831-746; 084-604-660-293-254; 089-844-745-151-655; 091-381-236-484-180; 096-028-122-046-611; 097-125-830-329-04X; 101-997-359-718-799; 104-202-238-477-850; 106-061-797-072-477; 111-074-098-894-168; 114-555-171-003-613; 118-672-542-268-820; 130-818-416-479-700; 133-096-548-648-307; 140-691-351-504-561; 140-921-627-035-630; 143-456-236-580-963; 146-335-085-869-417; 154-042-138-562-576; 155-374-339-828-155; 155-498-300-221-705; 155-771-820-039-231; 157-165-583-357-246; 160-257-113-689-234; 186-790-839-166-175,29,true,"CC BY, CC BY-NC-ND",gold
087-872-700-596-701,Landbot-Based Interactive Chatbot as a Tax Learning Medium for 11th-Grade Vocational High School Students in Accounting and Financial Institutions,2025-07-31,2025,journal article,JURNAL AKUNTANSI DAN AUDITING,25497650; 14126699,Institute of Research and Community Services Diponegoro University (LPPM UNDIP),,Siti Hidanati Mafaza; Hanjar Ikrima Nanda; Eren Prinstin,"<jats:p>Taxation learning in vocational high schools still faces various challenges, such as difficult material, limited use of interactive media, and low student motivation. This study aims to examine the effectiveness of an interactive chatbot based on Landbot as a learning medium for Income Tax material for 11th-grade Accounting and Financial Administration students at a private SMK in Malang Regency. Using a Research and Development method at the implementation stage and a one-group pretest-posttest design, this study measured students’ understanding before and after using the chatbot through learning outcome tests. The Wilcoxon Signed-Rank test results showed a significance value of 0.000 (&lt; 0.05), indicating a significant improvement in learning outcomes. The average student score increased from 67.41 to 91.30 after the treatment. This proves that the interactive chatbot is effective enhancing comprehension of taxation material and provides a more engaging and flexible learning experience tailored to the needs of SMK students.</jats:p>",22,1,39,60,Vocational education; Chatbot; Accounting; Vocational school; Business; Mathematics education; Psychology; Finance; Pedagogy; Computer science; Natural language processing,,,,,,http://dx.doi.org/10.14710/jaa.22.1.39-60,,10.14710/jaa.22.1.39-60,,,0,,0,true,cc-by,gold
088-019-837-859-494,SAFENUDGE: Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs,,2025,conference proceedings article,Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,,Association for Computational Linguistics,,Joao Fonseca; Andrew Bell; Julia Stoyanovich,,,,19966,19980,,,,,,,http://dx.doi.org/10.18653/v1/2025.emnlp-main.1010,,10.18653/v1/2025.emnlp-main.1010,,,0,,0,false,,
089-198-259-804-16X,FinVerse Agent: Resource-Efficient Financial LLMs with Modular LoRA Fine-Tuning and Task Routing,2025-11-07,2025,conference proceedings article,2025 IEEE 12th International Conference on Cyber Security and Cloud Computing (CSCloud),,IEEE,,Laiba Rana; Ahmed Amin; Huma Ameer; Seemab Latif,,,,338,340,,,,,,,http://dx.doi.org/10.1109/cscloud66326.2025.00061,,10.1109/cscloud66326.2025.00061,,,0,027-550-453-512-479,0,false,,
089-514-852-636-904,<b>AlphaPortfolio:</b> Discovery of Portfolio Optimization and Allocation Methods Using LLMs,2025-01-01,2025,preprint,,,Elsevier BV,,Kamer Ali Yuksel,,,,,,Portfolio; Economics; Computer science; Financial economics,,,,,,http://dx.doi.org/10.2139/ssrn.5118317,,10.2139/ssrn.5118317,,,0,034-886-620-234-617; 051-010-208-217-831; 059-896-966-271-462; 083-581-230-091-411; 088-852-902-759-028; 103-750-677-215-394; 142-637-644-580-520,0,false,,
089-684-850-032-543,InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Yi Yang; Yixuan Tang; Kar Yan Tam,"We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned using a small set of carefully curated instructions on a well-trained foundation model, which is consistent with the Superficial Alignment Hypothesis (Zhou et al., 2023). From a practical perspective, this work develops a state-of-the-art financial domain LLM with superior capability in understanding financial texts and providing helpful investment advice, potentially enhancing the work efficiency of financial professionals. We release the model parameters to the research community.",,,,,Finance; Generalizability theory; Investment (military); Perspective (graphical); Computer science; Financial modeling; Set (abstract data type); Machine learning; Artificial intelligence; Business; Political science; Psychology; Developmental psychology; Politics; Law; Programming language,,,,,https://arxiv.org/abs/2309.13064,http://dx.doi.org/10.48550/arxiv.2309.13064,,10.48550/arxiv.2309.13064,,,0,,3,true,cc-by,green
089-752-231-256-54X,Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models,2023-12-15,2023,conference proceedings article,2023 IEEE International Conference on Big Data (BigData),,IEEE,,Armin Berger; Lars Hillebrand; David Leonhard; Tobias Deußer; Thiago Bell Felix De Oliveira; Tim Dilmaghani; Mohamed Khaled; Bernd Kliem; Rudiger Loitz; Christian Bauckhage; Rafet Sifa,"The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAI's GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.",,,,,Audit; Compliance (psychology); Realm; Variety (cybernetics); Computer science; Process (computing); Accounting; Conformance testing; Conformity assessment; Risk analysis (engineering); Business; Artificial intelligence; Political science; Psychology; Social psychology; Standardization; Law; Operating system,,,,Ministry of Education,http://arxiv.org/pdf/2507.16642 http://arxiv.org/abs/2507.16642,http://dx.doi.org/10.1109/bigdata59044.2023.10386518,,10.1109/bigdata59044.2023.10386518,,,2,009-429-532-265-827; 016-750-756-337-724; 028-177-414-077-965; 032-509-322-900-566; 037-297-734-603-234; 040-504-787-044-496; 071-423-367-050-689; 071-989-405-769-92X; 083-449-940-348-930; 088-157-603-923-110; 092-812-301-112-972; 097-114-639-970-011; 113-961-214-877-335; 125-007-830-851-395; 145-931-944-659-640; 150-612-233-271-263; 154-042-138-562-576; 178-224-887-933-440; 189-973-804-567-981,21,true,,unknown
089-851-486-021-586,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Tax",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905472,https://guides.library.harvard.edu/c.php?g=1082940&p=7905472,,,3087886777,,0,,0,false,,
089-869-205-096-318,"AI-Driven Security for Financial Transactions: Leveraging LLMs, Federated Learning, and Behavioral Biometrics",2024-12-30,2024,journal article,International Journal of Emerging Research in Engineering and Technology,3050922x,ScienceTech Xplore,,,,5,4,,,Biometrics; Behavioral economics; Computer security; Business; Internet privacy; Finance; Computer science,,,,,,http://dx.doi.org/10.63282/3050-922x.ijeret-v5i4p107,,10.63282/3050-922x.ijeret-v5i4p107,,,0,,0,false,,
089-899-692-239-933,Systematic Evaluation of Long-Context LLMs on Financial Concepts,,2024,conference proceedings article,Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track,,Association for Computational Linguistics,,Lavanya Gupta; Saket Sharma; Yiyun Zhao,"Long-context large language models (LC LLMs) promise to increase reliability of LLMs in real-world tasks requiring processing and understanding of long input documents. However, this ability of LC LLMs to reliably utilize their growing context windows remains under investigation. In this work, we evaluate the performance of state-of-the-art GPT-4 suite of LC LLMs in solving a series of progressively challenging tasks, as a function of factors such as context length, task difficulty, and position of key information by creating a real world financial news dataset. Our findings indicate that LC LLMs exhibit brittleness at longer context lengths even for simple tasks, with performance deteriorating sharply as task complexity increases. At longer context lengths, these state-of-the-art models experience catastrophic failures in instruction following resulting in degenerate outputs. Our prompt ablations also reveal unfortunate continued sensitivity to both the placement of the task instruction in the context window as well as minor markdown formatting. Finally, we advocate for more rigorous evaluation of LC LLMs by employing holistic metrics such as F1 (rather than recall) and reporting confidence intervals, thereby ensuring robust and conclusive findings.",,,1163,1175,Context (archaeology); Computer science; Business; Finance; History; Archaeology,,,,,http://arxiv.org/pdf/2412.15386 http://arxiv.org/abs/2412.15386,http://dx.doi.org/10.18653/v1/2024.emnlp-industry.88,,10.18653/v1/2024.emnlp-industry.88,,,0,,0,true,,green
089-966-076-340-269,Designing Heterogeneous LLM Agents for Financial Sentiment Analysis,2024-01-01,2024,preprint,arXiv (Cornell University),,,,Frank Xing,"Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed.",,,,,Leverage (statistics); Computer science; Context (archaeology); Generative grammar; Discriminative model; Task (project management); Domain (mathematical analysis); Knowledge management; Data science; Artificial intelligence; Management; Economics; Paleontology; Biology; Mathematical analysis; Mathematics,,,,,https://arxiv.org/abs/2401.05799,http://dx.doi.org/10.48550/arxiv.2401.05799,,10.48550/arxiv.2401.05799,,,0,,1,true,cc-by-nc-nd,bronze
090-475-698-330-072,FI-NL2PY2SQL: Financial Industry NL2SQL Innovation Model Based on Python and Large Language Model,2025-01-02,2025,journal article,Future Internet,19995903,MDPI AG,Switzerland,Xiaozheng Du; Shijing Hu; Feng Zhou; Cheng Wang; Binh Minh Nguyen,"<jats:p>With the rapid development of prominent models, NL2SQL has made many breakthroughs, but customers still hope that the accuracy of NL2SQL can be continuously improved through optimization. The method based on large models has brought revolutionary changes to NL2SQL. This paper innovatively proposes a new NL2SQL method based on a large language model (LLM), which could be adapted to an edge-cloud computing platform. First, natural language is converted into Python language, and then SQL is generated through Python. At the same time, considering the traceability characteristics of financial industry regulatory requirements, this paper uses the open-source big model DeepSeek. After testing on the BIRD dataset, compared with most NL2SQL models based on large language models, EX is at least 2.73% higher than the original method, F1 is at least 3.72 higher than the original method, and VES is 6.34% higher than the original method. Through this innovative algorithm, the accuracy of NL2SQL in the financial industry is greatly improved, which can provide business personnel with a robust database access mode.</jats:p>",17,1,12,12,Computer science; Python (programming language); Financial modeling; Programming language; Software engineering; Finance; Business,,,,National Key Research and Development Program of China; National Key Research and Development Program of China; National Key Research and Development Program of China; National Key Research and Development Program of China; National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China; National Natural Science Foundation of China,,http://dx.doi.org/10.3390/fi17010012,,10.3390/fi17010012,,,0,007-537-777-452-01X; 020-651-059-171-555; 041-473-023-426-811; 047-366-403-255-617; 062-371-059-108-30X; 085-259-981-728-062; 124-339-146-710-566; 126-701-156-167-538; 128-428-452-415-936; 129-636-939-016-163; 145-107-271-668-28X; 147-574-338-372-447,3,true,cc-by,gold
090-505-042-165-814,"Large Language Models in Financial Services: FAIR -A Framework for Implementation, Risk Mitigation, and Remediation",2025-01-01,2025,preprint,,,Elsevier BV,,Miquel Noguer I Alonso; harry mendell,,,,,,Environmental remediation; Business; Financial services; Financial risk; Finance; Model risk; Risk management; Risk analysis (engineering); Contamination; Ecology; Biology,,,,,,http://dx.doi.org/10.2139/ssrn.5195816,,10.2139/ssrn.5195816,,,0,,0,false,,
090-684-660-049-294,"Leveraging AI, ML, and LLMs for Predictive Trade Analytics and Automated Metadata Management",2025-04-15,2025,journal article,European Journal of Computer Science and Information Technology,20540957; 20540965,European Centre for Research Training and Development,,Bujjibabu Katta,"<jats:p>The integration of Artificial Intelligence (AI), Machine Learning (ML), and Large Language Models (LLMs) has revolutionized trade data analytics and metadata management within cloud environments. The implementation of advanced predictive models, coupled with sophisticated cloud architectures, enables organizations to process vast amounts of heterogeneous data while delivering real-time insights for strategic decision-making. The architecture encompasses multiple layers of data processing, including event-driven systems for trade pattern recognition, automated metadata extraction, and intelligent classification mechanisms. Through the deployment of specialized ML models, including time series analysis, natural language processing, and graph neural networks, the system achieves enhanced prediction accuracy across diverse trading scenarios. The incorporation of AI-driven metadata management strengthens data governance through automated lineage tracking, compliance monitoring, and dynamic access control. Performance optimization techniques, including adaptive model selection and dynamic resource allocation, ensure sustained system efficiency. The implementation demonstrates significant improvements in processing speed, prediction accuracy, and resource utilization while maintaining robust security and compliance frameworks.</jats:p>",13,30,78,92,Metadata; Analytics; Predictive analytics; Computer science; Metadata management; Data science; World Wide Web,,,,,,http://dx.doi.org/10.37745/ejcsit.2013/vol13n307892,,10.37745/ejcsit.2013/vol13n307892,,,0,008-295-250-677-084; 011-813-532-626-184; 015-099-450-501-644; 015-973-412-970-632; 057-321-632-940-304; 176-816-747-621-118,0,false,,
090-699-165-504-052,"GPT-Guided Monte Carlo Tree Search for Symbolic Regression in Financial
  Fraud Detection",2024-11-07,2024,preprint,arXiv (Cornell University),,,,Prashank Kadam,"With the increasing number of financial services available online, the rate of financial fraud has also been increasing. The traffic and transaction rates on the internet have increased considerably, leading to a need for fast decision-making. Financial institutions also have stringent regulations that often require transparency and explainability of the decision-making process. However, most state-of-the-art algorithms currently used in the industry are highly parameterized black-box models that rely on complex computations to generate a score. These algorithms are inherently slow and lack the explainability and speed of traditional rule-based learners. This work introduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational GPT model to guide the MCTS, significantly enhancing its convergence speed and the quality of the generated expressions which are further extracted to rules. Our experiments show that SR-MCTS can detect fraud more efficiently than widely used methods in the industry while providing substantial insights into the decision-making process.",,,,,Monte Carlo method; Tree (set theory); Econometrics; Regression; Computer science; Decision tree; Regression analysis; Statistics; Mathematics; Data mining; Machine learning; Combinatorics,,,,,https://arxiv.org/abs/2411.04459,http://dx.doi.org/10.48550/arxiv.2411.04459,,10.48550/arxiv.2411.04459,,,0,,0,true,,green
090-886-695-189-081,GPT-4 Powered Virtual Analyst for Fundamental Stock Investment by Leveraging Qualitative Data,2024-09-03,2024,conference proceedings article,2024 5th International Conference on Artificial Intelligence and Data Sciences (AiDAS),,IEEE,,Teck Wan Teo; Soung-Yue Liew; Chee Henn Chng; Jing Ying Ng; Zong Zheng Khoo,,,,304,309,Computer science; Investment (military); Stock (firearms); Business; Engineering; Mechanical engineering; Politics; Political science; Law,,,,,,http://dx.doi.org/10.1109/aidas63860.2024.10730589,,10.1109/aidas63860.2024.10730589,,,0,011-958-235-543-718; 026-294-977-463-130; 036-585-041-842-638; 049-016-424-530-422; 049-796-713-674-73X; 190-050-627-029-962,0,false,,
091-375-865-091-514,SELA: Smart Edge LLM Agent to Optimize Response Trade-offs of AI Assistants,2025-09-03,2025,journal article,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",24749567,Association for Computing Machinery (ACM),,Shreshth Tuli; Giuliano Casale; Manuel Roveri,"<jats:p>The advent of Large Language Models (LLMs) has changed the way we process information today and has unlocked new ways of delivering intelligence to the user. One of the ways of interfacing with AI is via smart assistants and chatbots that take multimodal inputs. However, the diversity of input tasks imply the possibility of both latency-critical and complex input instructions for AI assistants. Further, LLMs cannot be deployed on the edge for low-latency outputs, as that presents challenges due to their high computational demands and memory requirements. This work explores such trade-offs and contributes a smart LLM selection policy, called SELA, that leverages a suite of LLM models with disparate characteristics to optimize overall quality of service (QoS). SELA uses a time-criticality and complexity predictor at the edge to identify the optimal LLM choice for a given input instruction. Experiments on public instruction benchmarks demonstrate that SELA provides 9% to 62% higher QoS scores compared to the state-of-the-art selection policies.</jats:p>",9,3,1,18,Enhanced Data Rates for GSM Evolution; Computer science; Edge device; Artificial intelligence; Operating system; Cloud computing,,,,,,http://dx.doi.org/10.1145/3749483,,10.1145/3749483,,,0,001-518-036-364-117; 006-210-949-063-117; 018-237-482-574-137; 021-026-110-880-019; 035-876-619-354-109; 039-147-456-850-364; 040-238-483-085-265; 040-927-979-665-996; 044-571-015-747-947; 058-913-364-111-838; 068-864-486-586-772; 075-923-507-528-127; 085-248-120-387-506; 090-154-362-063-575; 096-816-659-965-199; 140-390-044-729-403; 158-739-792-081-844; 175-355-504-940-610,0,false,,
091-561-437-913-058,A Self-Improving Method for Generating Descriptions of Financial Data Quality Grading Using LLMs,,2025,conference proceedings article,Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Yang Zhao; Yohei Ikawa; Bishwaranjan Bhattacharjee,,,,237,245,,,,,,,http://dx.doi.org/10.18653/v1/2025.finnlp-2.16,,10.18653/v1/2025.finnlp-2.16,,,0,,0,false,,
091-589-815-863-305,Large Language Model Agent in Financial Trading: A Survey,2024-07-26,2024,preprint,arXiv (Cornell University),,,,Han Ding; Yinheng Li; Junhao Wang; Hang Chen,"Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.",,,,,Business; Finance; Computer science,,,,,https://arxiv.org/abs/2408.06361,http://dx.doi.org/10.48550/arxiv.2408.06361,,10.48550/arxiv.2408.06361,,,0,,2,true,,green
091-778-160-378-802,Identifying Representation Bias in Large Language Models Used in Financial Sentiment Analysis,2025-03-17,2025,conference proceedings article,2025 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CiFer),,IEEE,,Alpay Sabuncuoglu; Carsten Maple,,,,1,7,Computer science; Representation (politics); Sentiment analysis; Natural language processing; Artificial intelligence; Econometrics; Mathematics; Political science; Politics; Law,,,,Innovate UK,,http://dx.doi.org/10.1109/cifer64978.2025.10975731,,10.1109/cifer64978.2025.10975731,,,0,016-556-654-810-446; 018-866-913-622-957; 030-902-247-987-89X; 031-961-851-709-120; 039-641-612-378-457; 078-141-527-597-27X; 105-739-809-090-948; 150-110-808-411-176; 154-042-138-562-576; 186-823-133-597-449,0,false,,
092-234-413-806-731,DeepFinLLM: an intelligent financial advisor unleashing strategic insights with large language models,2025-06-16,2025,journal article,Journal of Intelligent Information Systems,09259902; 15737675,Springer Science and Business Media LLC,Netherlands,Veerababu Reddy; N. Veeranjaneyulu,,63,5,1713,1736,Computer science; Software engineering; Knowledge management; Human–computer interaction; Artificial intelligence; Process management; Management science; Business; Economics,,,,,,http://dx.doi.org/10.1007/s10844-025-00959-z,,10.1007/s10844-025-00959-z,,,0,009-647-830-681-728; 013-352-330-919-767; 016-531-084-325-607; 016-938-308-196-336; 030-316-794-717-440; 032-081-199-937-966; 032-117-169-875-647; 044-293-842-546-981; 044-579-322-796-833; 052-032-881-946-86X; 055-525-042-582-414; 056-690-465-482-39X; 063-840-533-415-012; 067-260-376-432-891; 073-031-995-357-278; 088-259-858-201-627; 092-557-334-874-193; 097-547-394-710-513; 107-366-730-984-07X; 107-701-475-478-916; 116-022-714-546-130; 139-239-027-110-484; 155-000-481-060-11X; 162-480-980-657-50X; 169-664-751-543-841; 173-031-218-503-969; 173-521-634-095-118; 193-222-672-874-089; 199-392-235-944-050,1,true,,green
092-307-866-151-111,On a Chatbot Conducting a Virtual Dialogue in Financial Domain,2019-08-01,2019,,,,,,Boris Galitsky; Dmitry I. Ilvovsky,,,,99,101,Human–computer interaction; Domain (software engineering); Chatbot; Computer science,,,,,https://www.aclweb.org/anthology/W19-5517/,https://www.aclweb.org/anthology/W19-5517/,,,3088662396,,0,002-088-862-269-292; 017-858-823-438-930; 018-221-443-826-809; 028-429-137-376-286; 069-727-374-647-871; 077-817-411-975-735,3,false,,
092-506-012-237-466,QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading,2025-09-12,2025,preprint,arXiv (Cornell University),,,,Fei Xiong; Xiang Zhang; Aosong Feng; Siqi Sun; Chenyu You,"Recent advances in Large Language Models (LLMs) have shown remarkable capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks by leveraging fundamental and sentiment-based inputs for strategic decision-making. However, these approaches are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT typically requires rapid, risk-aware decisions driven by structured, short-horizon signals, such as technical indicators, chart patterns, and trend features. These signals stand in sharp contrast to the long-horizon, text-driven reasoning that characterizes most existing LLM-based systems in finance. To bridge this gap, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents--Indicator, Pattern, Trend, and Risk--each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. Extensive experiments across nine financial instruments, including Bitcoin and Nasdaq futures, demonstrate that QuantAgent consistently outperforms baseline methods, achieving higher predictive accuracy at both 1-hour and 4-hour trading intervals across multiple evaluation metrics. Our findings suggest that coupling structured trading signals with LLM-based reasoning provides a viable path for traceable, real-time decision systems in high-frequency financial markets.",,,,,High-frequency trading; Monetary economics; Business; Economics; Natural resource economics; Financial economics; Algorithmic trading,,,,,https://arxiv.org/abs/2509.09995,http://dx.doi.org/10.48550/arxiv.2509.09995,,10.48550/arxiv.2509.09995,,,0,,0,true,,green
092-531-431-132-989,"Investigating fake financial news: a systematic review of trends, themes, LLM-based detection methods, and theories",2025-07-02,2025,journal article,Aslib Journal of Information Management,20503806; 20503814,Emerald,United Kingdom,Mohotarema Rashid; Lingzi Hong; Sarah Ryan,"<jats:sec>;                     <jats:title>Purpose</jats:title>;                     <jats:p>This paper conducts a systematic literature review (SLR) to explore research directions in fake financial news, addressing a growing concern in today’s digital financial landscape.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Design/methodology/approach</jats:title>;                     <jats:p>The paper applies the search, appraisal, synthesis, and analysis (SALSA) framework, utilizing nine databases and reviewing 40 studies from 2010–2025 (March).</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Findings</jats:title>;                     <jats:p>This study unveils major trends, themes, state-of-the-art detection methods, and theoretical foundations in fake financial news research. It identifies significant variables such as impact, timing, target, and responses, providing a deeper understanding of fake financial news. Moreover, this review highlights that despite advances in large language models (LLMs)- based detection, human-centered factors remain largely overlooked. By extracting empirical and analytical insights from prior research, this study proposes a framework that connects detection methods and behavioral theories to guide future research in human-centered, interdisciplinary approaches for financial misinformation detection.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Research limitations/implications</jats:title>;                     <jats:p>The study offers practical take-aways for misinformation detection and amelioration for various financial stakeholders, including investors, firms, and policymakers. It recommends integration of LLM tools for real-time misinformation detection, establishing financial fact-checking platforms, incorporating corporate responses, and increasing financial literacy education. The study also suggests that stronger regulatory interventions by Congress and the SEC are essential to mitigating the threat of financial misinformation.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Originality/value</jats:title>;                     <jats:p>This paper contributes to the field of information management by synthesizing the current knowledge on fake financial news. It is among the frontier systematic reviews, integrating research trends, themes, theoretical foundations, and LLM-based detection advancements beyond prior computational approaches.</jats:p>;                   </jats:sec>",77,6,1223,1254,Fake news; Psychology; Data science; Computer science; Internet privacy,,,,,,http://dx.doi.org/10.1108/ajim-08-2024-0632,,10.1108/ajim-08-2024-0632,,,0,003-229-435-199-990; 003-982-570-881-01X; 004-592-223-825-526; 005-820-979-159-437; 006-263-109-671-258; 007-015-420-414-303; 007-425-618-461-342; 007-731-071-585-725; 007-852-865-168-115; 008-275-063-628-684; 009-381-629-605-55X; 009-394-843-070-587; 009-806-441-635-491; 009-813-572-712-755; 009-962-985-015-527; 010-318-059-902-416; 011-470-381-172-40X; 014-020-882-417-672; 014-023-309-652-85X; 014-053-775-237-132; 014-067-889-830-732; 014-653-376-135-795; 015-519-207-250-433; 018-728-256-901-466; 018-849-602-639-167; 020-279-004-944-84X; 022-607-869-348-572; 023-512-431-626-501; 023-922-299-055-466; 024-813-346-139-886; 025-271-384-564-536; 025-847-465-411-67X; 028-895-595-269-750; 029-815-854-834-210; 030-113-510-409-404; 034-589-970-752-309; 041-871-520-325-126; 043-446-151-356-941; 043-524-750-659-806; 047-868-139-556-487; 048-535-754-084-590; 051-123-031-508-30X; 053-424-678-065-732; 053-760-242-491-32X; 056-684-752-436-133; 057-866-315-126-309; 057-907-845-827-282; 058-456-244-323-755; 059-353-528-548-109; 060-563-284-554-086; 064-346-968-018-799; 064-784-488-619-405; 070-213-259-963-665; 070-567-291-798-784; 074-655-971-464-054; 076-358-707-899-43X; 076-873-759-659-530; 077-773-462-630-137; 079-081-002-219-916; 081-547-256-855-879; 083-137-544-456-89X; 083-273-411-910-717; 083-702-578-301-182; 091-707-500-970-151; 094-449-294-951-936; 095-404-775-006-983; 101-595-413-202-901; 102-225-342-060-405; 102-649-282-300-989; 103-725-507-112-857; 105-167-915-528-500; 105-270-883-059-169; 106-981-842-382-696; 110-458-125-848-279; 113-628-998-310-054; 115-009-401-235-398; 116-302-631-797-766; 120-536-758-595-017; 121-148-073-830-047; 122-495-515-808-537; 123-691-914-223-370; 127-272-131-847-215; 127-682-308-569-815; 128-165-821-629-154; 134-185-061-496-162; 137-227-654-953-64X; 139-835-449-783-751; 148-497-863-189-907; 149-518-387-200-007; 149-939-036-165-233; 153-986-337-473-199; 154-032-885-728-358; 155-894-555-502-132; 156-450-434-439-041; 159-070-846-996-05X; 159-764-981-155-188; 167-262-013-958-67X; 168-009-952-136-911; 174-240-778-725-073; 176-484-561-344-521; 178-358-256-234-591; 179-112-512-080-410; 179-737-068-474-033; 182-765-548-658-188; 183-286-645-207-133; 185-671-613-908-123; 190-760-609-208-293; 191-957-237-138-372,1,false,,
092-757-362-383-38X,Quantifying the speed-accuracy trade-off of large language models on oral and maxillofacial surgery multiple-choice questions.,2025-11-19,2025,journal article,Scientific reports,20452322,Springer Science and Business Media LLC,United Kingdom,Viet Anh Nguyen; Thi Bich Ngoc Ha; Minh Ngoc Tran; Ngo The Minh Pham; Thuy Linh Nguyen; Thi Quynh Trang Vuong,"Large language models (LLMs) such as GPT-4o, Copilot and Gemini are entering dental curricula, yet their suitability for real-time decision support remains unclear because most evaluations report accuracy alone. This prospective in silico diagnostic-accuracy study benchmarked six engines-GPT-4o, OpenAI o3, Copilot-Quick, Copilot-Deep, Gemini-Flash and Gemini-Pro-against 1766 single-best-answer multiple-choice questions from a contemporary oral and maxillofacial surgery (OMFS) board-review text. Textbook keys served as the reference standard. Overall and domain-level accuracy, intra-model answer consistency and per-batch response latency were recorded; χ² tests compared accuracies and Kruskal-Wallis with multiplicity-adjusted Mann-Whitney U tests compared response times. Accuracy differed significantly across engines (χ² = 97.31, p < 0.001), ranging from 77.9% for Copilot-Quick to 88.3% for Gemini-Pro. Reasoning-optimised variants (o3, Copilot-Deep, Gemini-Pro) exceeded their speed-tuned counterparts by 3.8-6.2% points, with the largest gains in trauma, craniofacial deformity and orthognathic surgery domains. These improvements incurred a marked latency penalty: median response times of 2.1-3.1 s versus 0.1-0.2 s for the faster engines. Each additional 3-6 correct answers per 100 items therefore required roughly 2-3 s of extra processing. Items unanswered by all models clustered around rare numeric facts and negatively worded stems. Reasoning-optimised LLMs deliver clinically meaningful accuracy gains on OMFS board questions, but educators and clinicians must weigh this benefit against slower output and maintain expert oversight to mitigate residual knowledge gaps.",15,1,40657,,,Artificial intelligence; Diagnostic accuracy; Large language model; Multiple-choice examination; Oral and maxillofacial surgery; Response latency,"Humans; Surgery, Oral/education; Computer Simulation; Language; Large Language Models",,,,http://dx.doi.org/10.1038/s41598-025-27256-7,41258208,10.1038/s41598-025-27256-7,,PMC12630698,0,003-636-041-405-155; 003-948-645-597-701; 007-851-776-277-497; 046-097-830-830-381; 050-688-307-284-223; 052-418-743-833-519; 111-606-580-752-218; 126-437-256-402-339; 132-798-061-325-338; 155-108-363-537-84X; 155-500-973-361-905; 170-627-889-071-763,0,true,"CC BY, CC BY-NC-ND",gold
093-717-308-902-302,Comparing AI-Generated Preview and Portfolio Feedback: Gpt 4.o vs. Claude 4,2025-10-14,2025,journal article,European Journal of Artificial Intelligence and Machine Learning,27960072,European Open Science Publishing,,Thomas Kropmans; Oleh Bilokrylyi; Dmytro Predchyshyn; David Cunningham; Edward Melvin; Gabia Neverauskaité,"<jats:p>This report provides an in-depth comparative analysis of AI-generated portfolio feedback delivered through two leading Large Language Model platforms (LLM): Gpt4.o OpenAI and Claude-sonnet-4 (Anthropic) via Amazon Bedrock. The feedback was analyzed in two distinct stages: preview feedback, which serves as a safety and verification layer for examiners/administrators, and portfolio feedback, which is delivered directly to the students. These systems are integral to Qpercom’s digital assessment tools and support high-stakes clinical assessments such as Objective Structured Clinical Examinations (OSCEs), high-stake recruitment using Multiple Mini Interviews (MMIs), and Video Interviewing and Digital Scoring (VIDS). This evaluation examines how accurately each model reflects students’ actual high-, mid-, and underperformance and whether its feedback provides safe, constructive, and educationally valuable input. </jats:p>",4,5,26,32,Portfolio; Computer science; Artificial intelligence; Psychology; Economics; Financial economics,,,,,https://ej-ai.org/index.php/ejai/article/download/82/115 https://doi.org/10.24018/ejai.2025.4.5.82,http://dx.doi.org/10.24018/ejai.2025.4.5.82,,10.24018/ejai.2025.4.5.82,,,0,,0,false,,
093-740-285-692-618,The Financial Anatomy of Climate Solutions: A Large Language Model Approach to Company Classification and Analysis,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Shirley Lu; George Serafeim,,,,,,Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5018775,,10.2139/ssrn.5018775,,,0,003-249-688-836-343; 017-153-449-730-092; 017-358-296-871-204; 033-170-911-113-954; 034-286-544-155-634; 050-448-166-311-271; 050-816-436-557-13X; 053-015-995-110-341; 055-318-598-916-211; 072-182-908-297-994; 075-491-770-315-472; 080-715-357-808-274; 097-128-239-369-005; 097-494-016-471-264; 103-654-431-992-361; 128-490-868-275-401; 141-527-979-559-703; 166-543-850-883-42X; 180-933-248-408-969,0,false,,
093-903-483-464-134,Implementing Financial Regulations Using Large Language Models,2025-01-01,2025,preprint,,,Elsevier BV,,Bledar Fazlija; Meriton Ibraimi; Aynaz Forouzandeh; Arber Fazlija,,,,,,Business; Finance; Computer science,,,,,,http://dx.doi.org/10.2139/ssrn.5010694,,10.2139/ssrn.5010694,,,0,009-278-725-876-522; 015-992-321-385-693; 023-480-643-288-651; 035-876-619-354-109; 097-914-300-871-028; 185-040-867-698-372,4,false,,
094-659-147-797-663,Evaluating the Ethical Judgment of Large Language Models in Financial Market Abuse Cases,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Avinash Kumar Pandey; Swati Rajwal,,,,132,140,,,,,,,http://dx.doi.org/10.1145/3768292.3770439,,10.1145/3768292.3770439,,,0,008-429-285-934-192; 041-047-135-157-618; 050-785-292-020-819; 071-865-699-989-965; 152-168-627-585-446; 164-692-697-121-993; 185-774-871-143-508; 191-386-949-114-188; 192-949-063-372-324,0,false,,
094-800-464-242-16X,The AI digital asset management assistant: Testing GPT-4's description and keyword tagging abilities on product imagery,2024-09-01,2024,journal article,Journal of Digital Media Management,20471319; 20471300,Henry Stewart Publications,,Jake Athey; Jacob Williamson,"<jats:p xml:lang=""en"">The process of tagging descriptive metadata to digital assets remains a time-consuming and largely manual process for digital asset management (DAM) system administrators. This paper explores the potential of OpenAI’s GPT-4, a large language model, to automate product descriptions, keyword tagging and alt-text. The research team developed six generative AI prompts that instruct GPT-4 to draft one-sentence descriptions and ten keywords for sample product images from six categories of household brands, namely: bicycles, food &amp; beverage, home goods, office furniture, footwear and tools. Using an assessment framework that measures accuracy and precision, the team evaluated GPT-4’s performance by prompt and product category. GPT-4 demonstrated the highest accuracy when describing food &amp; beverage images and the highest keyword precision when tagging footwear images. GPT-4 struggled to be accurate when images displayed low colour contrast or partially obstructed text. It also struggled when attempting to correctly identify gender, relationships and settings. However, GPT-4 showed surprising aptitude at identifying product materials like carbon fibre and species of wood. An analysis of prompts revealed that changes in persona, task description and specifications significantly influence accuracy and precision. The highest average accuracy score and highest average precision score among the prompts suggest that GPT-4 requires careful human oversight when generating keywords, product descriptions and alt-text for accessibility. Even so, it likely saves time for DAM administrators and professionals in marketing and e-commerce.</jats:p>",13,1,6,6,Computer science; Product (mathematics); Digital asset management; Asset (computer security); Asset management; Natural language processing; Information retrieval; Computer security; Business; Mathematics; Geometry; Finance; Investment strategy; Market liquidity,,,,,,http://dx.doi.org/10.69554/ymmi7940,,10.69554/ymmi7940,,,0,,0,false,,
094-868-484-213-746,Strategies for Enhancing Explainability in Financial Underwriting Using RAG and LLM,2025-04-30,2025,journal article,The Korean Data Analysis Society,12292354; 27339173,The Korean Data Analysis Society,,Jiyun Hong; Ah-Rim Joo; Sooyoung Cheon,"<jats:p>Recent advances in large language models (LLMs) have significantly expanded the capabilities of natural language processing systems. In the financial domain, delivering explainable and reliable personalised feedback has become increasingly important. However, traditional rule-based loan screening systems fall short in reflecting an individual’s financial context and providing actionable guidance. To address these limitations, this study proposes an LLM based loan screening system enhanced with RAG (retrieval augmented generation). The system retrieves relevant financial documents via vector-based search and feeds the context into a generative model. By leveraging external knowledge in real time, the system offers document-grounded, personalised financial recommendations. Experiments using real customer data show that the proposed RAG-based system significantly outperforms existing methods in both information retrieval and language generation. These results demonstrate the system’s potential to enhance the reliability and effectiveness of personalised financial services through the integration of professional document references.</jats:p>",27,2,465,476,Underwriting; Business; Financial system; Finance,,,,,,http://dx.doi.org/10.37727/jkdas.2025.27.2.465,,10.37727/jkdas.2025.27.2.465,,,0,,0,false,,
095-074-183-898-026,Conversational AI Agents for Sales Forecasting in Financial Enterprises,2023-12-31,2023,journal article,International Journal for Research Publication and Seminar,22786848,Shodh Sagar,,Srikanth Balla; Sanjay Kumar Bahl,"<jats:p>Sales forecasting is an integral part of financial institution strategic planning that has implications for budgeting, inventories, and resource allocation. Traditional forecasting methods like time-series analysis and statistical modeling might not be able to encapsulate the nuances of modern market trends and changing consumer behavior. The advent of artificial intelligence and machine learning has opened up new avenues for designing forecasting models; however, the models tend to be technical in nature and lack intuitive interfaces for non-technical end-users. This research realizes a pertinent gap in the literature—the less-highlighted role of conversational AI agents in providing interactive, user-friendly, and advanced sales forecasting platforms in financial institutions.</jats:p>",14,5,481,495,Business; Finance,,,,,,http://dx.doi.org/10.36676/jrps.v14.i5.1660,,10.36676/jrps.v14.i5.1660,,,0,018-822-581-363-834; 056-596-839-648-18X; 062-287-641-526-53X; 080-570-109-612-964; 137-992-885-023-712,0,false,,
095-412-254-671-507,LLM-Powered Information Extraction for the Dairy Financial Domain: Tackling Data Scarcity and Ambiguity,2025-11-10,2025,conference proceedings article,Proceedings of the 34th ACM International Conference on Information and Knowledge Management,,ACM,,Chunyan An; Yuying Huang; Qiang Yang; Siyu Yuan; Zhixu Li,,,,55,64,,,,,Key-Area Research and Development Program of Guangdong Province; Suzhou Key Laboratory of Artificial Intelligence and Social Governance Technologies; Smart Social Governance Technology and Innovative Application Platform; Leadership Talent Program?Science and Education?of SIP; Natural Science Foundation of Inner Mongolia; Inner Mongolia Key Research and Development and Achievement Transformation; Natural Science Foundation of China; Engineering Research Center of Ecological Big Data,,http://dx.doi.org/10.1145/3746252.3761030,,10.1145/3746252.3761030,,,0,017-723-811-811-576; 020-766-327-830-052; 050-035-505-201-056; 051-428-500-984-235; 060-998-079-653-379; 067-475-288-408-398; 069-205-805-130-236; 072-325-084-876-488; 081-562-955-547-774; 082-535-792-409-854; 088-306-660-345-579; 091-547-558-424-591; 092-230-389-714-678; 107-921-884-002-411; 116-177-081-334-707; 143-057-409-516-313,0,false,,
095-424-010-770-339,Toward a Unified Theory of Customer Continuance Model for Financial Technology Chatbots,2021-08-24,2021,journal article,"Sensors (Basel, Switzerland)",14248220; 14243210,Multidisciplinary Digital Publishing Institute (MDPI),Switzerland,Stanley Y. B. Huang; Chih-Jen Lee; Shih-Chin Lee,"With the popularity of financial technology (fintech) chatbots equipped with artificial intelligence, understanding the user’s response mechanism can help bankers formulate precise marketing strategies, which is a crucial issue in the social science field. Nevertheless, the user’s response mechanism towards financial technology chatbots has been relatively under-investigated. To fill these literature gaps, latent growth curve modeling was adopted by the present research to survey Taiwanese users of fintech chatbots. The present study proposed a customer continuance model to predict continuance intention for fintech chatbots and that cognitive and emotional dimensions positively influence the growth in a user’s attitude toward fintech chatbots, which in turn, positively influences continuance intention over time. In total, 401 customers of fintech chatbots were surveyed through three time points to examine the relationship between these variables over six months. The results support the theoretical model of this research and can advance the literature of fintech chatbots and the information technology adoption model.",21,17,5687,,Information technology; Psychology; Latent growth modeling; Technology acceptance model; Continuance; Popularity; Adoption model; Field (computer science); FinTech; Knowledge management,continuance intention; financial technology chatbots; latent growth curve modeling; technology acceptance model,"Artificial Intelligence; Intention; Models, Theoretical; Surveys and Questionnaires; Technology",,,https://www.mdpi.com/1424-8220/21/17/5687/pdf https://www.mdpi.com/1424-8220/21/17/5687 http://ui.adsabs.harvard.edu/abs/2021Senso..21.5687H/abstract https://www.ncbi.nlm.nih.gov/pubmed/34502578 https://dblp.uni-trier.de/db/journals/sensors/sensors21.html#HuangLL21 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8433661 https://doi.org/10.3390/s21175687,http://dx.doi.org/10.3390/s21175687,34502578,10.3390/s21175687,3195960951,PMC8433661,0,004-729-141-973-749; 005-661-903-979-847; 007-217-300-202-010; 007-601-594-833-884; 007-886-227-805-285; 008-974-579-054-354; 008-977-327-808-678; 009-299-623-546-736; 010-057-464-671-591; 013-290-691-738-397; 014-372-454-625-327; 014-729-669-563-150; 015-027-781-773-940; 018-089-228-914-771; 020-827-374-101-684; 022-821-517-004-001; 027-158-500-381-63X; 028-491-707-155-74X; 029-624-451-062-189; 029-671-905-945-026; 030-721-680-449-643; 032-637-445-493-370; 035-435-666-144-880; 035-606-231-893-667; 040-305-235-734-956; 041-682-684-365-025; 044-205-797-037-260; 045-303-197-179-818; 049-629-586-882-73X; 054-572-110-638-180; 056-015-783-392-50X; 058-081-424-637-980; 059-321-808-340-904; 059-644-067-934-268; 059-805-994-516-643; 063-460-503-377-196; 063-760-454-460-261; 064-356-987-474-836; 071-608-205-619-028; 072-438-089-310-724; 074-693-093-306-040; 075-322-206-574-53X; 077-946-279-594-919; 078-008-034-331-454; 080-105-678-541-04X; 081-914-679-401-299; 086-722-718-169-863; 087-764-255-315-490; 097-066-510-920-570; 098-586-004-554-577; 102-169-007-053-650; 114-393-649-573-916; 114-587-299-904-10X; 124-780-109-968-530; 127-133-340-222-430; 134-692-988-441-892; 135-872-831-789-430; 137-859-558-494-207; 139-247-442-555-499; 141-759-763-669-439; 143-875-130-007-110; 153-016-191-535-483; 155-933-685-765-597; 169-138-332-997-953; 177-912-150-327-852; 183-496-547-861-84X,19,true,cc-by,gold
095-937-921-560-483,Investor Sentiment Analysis of Financial Texts Based on GPT and RoBERTa,2024-06-30,2024,conference proceedings article,2024 International Joint Conference on Neural Networks (IJCNN),,IEEE,,Jia Miao; Jianwu Lin; Tong Luo; Guangling Liu,,9,,1,8,Sentiment analysis; Computer science; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/ijcnn60899.2024.10650089,,10.1109/ijcnn60899.2024.10650089,,,0,000-603-860-875-353; 002-416-232-151-070; 011-425-314-130-274; 011-916-048-879-069; 014-584-126-186-85X; 016-744-133-423-158; 017-527-307-794-609; 035-083-606-205-833; 042-991-640-112-849; 043-155-576-645-511; 048-080-375-726-460; 051-181-512-576-659; 055-620-518-255-693; 067-336-033-148-395; 078-296-914-062-917; 081-053-078-724-330; 110-352-060-681-264; 113-778-707-630-01X; 113-997-126-801-618; 114-653-505-665-828; 126-548-437-013-203; 127-430-479-062-807; 143-161-225-634-219; 145-213-122-055-796; 152-900-742-180-772; 166-937-547-892-528; 190-046-210-716-909,2,false,,
095-960-824-315-342,Artificial Intelligence in Day Trading: An Intraday Trading Framework with Economic Indicators and Large Language Model Analysis,2025-01-01,2025,preprint,,,Elsevier BV,,Zhuokai Chen,,,,,,Algorithmic trading; Computer science; Econometrics; Financial economics; Business; Economics,,,,,,http://dx.doi.org/10.2139/ssrn.5246516,,10.2139/ssrn.5246516,,,0,,0,false,,
095-987-917-023-672,GPT as a Financial Advisor,2023-01-01,2023,preprint,,,Elsevier BV,,Paweł Niszczota; Sami Abbas,,,,,,Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.4384861,,10.2139/ssrn.4384861,,,0,000-805-539-723-738; 009-122-864-785-531; 013-975-074-051-604; 027-892-634-351-23X; 035-626-688-794-042; 042-277-346-850-48X; 052-185-371-876-715; 068-013-933-803-965; 070-905-281-768-062; 077-444-269-577-533; 087-248-655-017-600; 095-475-937-136-456; 129-238-717-649-180; 155-676-717-437-906; 196-974-555-759-760,6,false,,
096-138-286-230-252,Large Language Models on Small Resource-Constrained Systems: Performance Analysis and Trade-Offs,2025-03-22,2025,conference proceedings article,SoutheastCon 2025,,IEEE,,Liam Seymour; Basar Kutukcu; Sabur Baidya,,,,1362,1369,Computer science; Resource (disambiguation); Distributed computing; Computer network,,,,,,http://dx.doi.org/10.1109/southeastcon56624.2025.10971638,,10.1109/southeastcon56624.2025.10971638,,,0,001-265-795-849-497; 014-702-105-971-562; 038-650-220-591-02X; 052-095-272-860-487; 055-102-532-637-073; 062-410-713-931-050; 073-822-049-799-783; 087-167-450-171-79X; 089-166-666-571-985; 108-858-121-692-976; 112-684-190-511-144; 117-568-013-146-549; 117-585-481-795-683; 128-281-148-405-705; 138-608-930-049-290; 164-067-775-003-068,1,false,,
096-210-579-017-23X,Integrating Large Language Models with Deep Reinforcement Learning for Portfolio Optimization,,2025,journal article,International Journal of Advanced Computer Science and Applications,21565570; 2158107x,The Science and Information Organization,,Renad Alsweed; Mohammed Alsuhaibani,"<jats:p>This paper explores the application of Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) to portfolio optimization, a critical financial task requiring strategies to balance risk and return in volatile markets. Traditional models often struggle with the complexity of financial markets, whereas Reinforcement Learning (RL) provides end-to-end frameworks for learning optimal, dynamic trading policies through sequential decision-making and trial-and-error interactions. The study examines key DRL algorithms, including Q-learning, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Twin-Delayed Deep Deterministic Policy Gradient (TD3), emphasizing their strengths in dynamic asset allocation. Crucial components of financial RL systems are discussed, such as state representations, reward function designs, its algorithms, and main approaches. Furthermore, the survey investigates how LLMs enhance decision-making by analyzing unstructured data (like news and social media) for sentiment and risk assessment, often integrating these insights to augment state representations or guide reward shaping within DRL frameworks.</jats:p>",16,11,,,,,,,,,http://dx.doi.org/10.14569/ijacsa.2025.0161184,,10.14569/ijacsa.2025.0161184,,,0,,0,false,,
096-248-714-811-684,Formulating Financial Trading Strategies Using LLM: A DSL-Mediated Approach via in-Context Learning,2025-03-21,2025,conference proceedings article,2025 7th International Conference on Natural Language Processing (ICNLP),,IEEE,,Jinheng Wu; Di Zhang; Qiang Niu,,,,6,13,Digital subscriber line; Context (archaeology); Computer science; Knowledge management; Process management; Business; Finance; Telecommunications; Paleontology; Biology,,,,,,http://dx.doi.org/10.1109/icnlp65360.2025.11108657,,10.1109/icnlp65360.2025.11108657,,,0,006-853-249-277-391; 018-866-913-622-957; 028-466-972-983-138; 032-323-291-720-742; 036-581-954-979-278; 061-087-374-030-560; 071-143-899-460-850; 094-103-262-467-252; 123-659-448-198-496; 124-339-146-710-566; 140-921-627-035-630; 198-237-284-797-150,0,false,,
096-364-707-776-428,"Parameter-Efficient Instruction Tuning of Large Language Models For
  Extreme Financial Numeral Labelling",2024-05-03,2024,preprint,arXiv (Cornell University),,,,Subhendu Khatuya; Rajdeep Mukherjee; Akash Ghosh; Manjunath Hegde; Koustuv Dasgupta; Niloy Ganguly; Saptarshi Ghosh; Pawan Goyal,"We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification problem using a generative paradigm through instruction tuning of Large Language Models (LLMs). To this end, we leverage metric metadata information to frame our target outputs while proposing a parameter efficient solution for the task using LoRA. We perform experiments on two recently released financial numeric labeling datasets. Our proposed model, FLAN-FinXC, achieves new state-of-the-art performances on both the datasets, outperforming several strong baselines. We explain the better scores of our proposed model by demonstrating its capability for zero-shot as well as the least frequently occurring tags. Also, even when we fail to predict the XBRL tags correctly, our generated output has substantial overlap with the ground-truth in majority of the cases.",,,,,Numeral system; Labelling; Computer science; Finance; Artificial intelligence; Psychology; Business; Criminology,,,,,https://arxiv.org/abs/2405.06671,http://dx.doi.org/10.48550/arxiv.2405.06671,,10.48550/arxiv.2405.06671,,,0,,0,true,,green
096-585-644-488-830,Predictive Power of LLMs in Financial Markets,2024-11-25,2024,preprint,arXiv (Cornell University),,,,Jerick Shi; Burton Hollifield,"Predicting the movement of the stock market and other assets has been valuable over the past few decades. Knowing how the value of a certain sector market may move in the future provides much information for investors, as they use that information to develop strategies to maximize profit or minimize risk. However, market data are quite noisy, and it is challenging to choose the right data or the right model to create such predictions. With the rise of large language models, there are ways to analyze certain data much more efficiently than before. Our goal is to determine whether the GPT model provides more useful information compared to other traditional transformer models, such as the BERT model. We shall use data from the Federal Reserve Beige Book, which provides summaries of economic conditions in different districts in the US. Using such data, we then employ the LLM's to make predictions on the correlations. Using these correlations, we then compare the results with well-known strategies and determine whether knowing the economic conditions improves investment decisions. We conclude that the Beige Book does contain information regarding correlations amongst different assets, yet the GPT model has too much look-ahead bias and that traditional models still triumph.",,,,,Predictive power; Financial market; Power (physics); Business; Economics; Financial system; Monetary economics; Finance; Financial economics; Philosophy; Physics; Epistemology; Quantum mechanics,,,,,https://arxiv.org/abs/2411.16569,http://dx.doi.org/10.48550/arxiv.2411.16569,,10.48550/arxiv.2411.16569,,,0,,0,true,,green
096-718-684-971-428,A Study on the Factors Affecting the Acceptance Intention of Chatbot Service in the Financial Industry,2021-10-31,2021,journal article,Journal of Korea Technology Innovation Society,15982912; 27138666,Korea Technology Innovation Society,,Seungjoon Lee; Jeha Lee; Doohee Chung,,24,5,845,869,Financial services; Business; Marketing; Chatbot; Service (business),,,,,http://dx.doi.org/10.35978/jktis.2021.10.24.5.845,http://dx.doi.org/10.35978/jktis.2021.10.24.5.845,,10.35978/jktis.2021.10.24.5.845,3212591405,,0,,2,false,,
096-752-665-174-078,Combining LLMs and Simheuristics: An Application to the Project Portfolio Selection Problem,2025-01-31,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Miguel Saiz; Laura Calvet,,,,3,13,Computer science; Portfolio; Selection (genetic algorithm); Project portfolio management; Operations research; Artificial intelligence; Project management; Systems engineering; Finance; Engineering; Economics,,,,,,http://dx.doi.org/10.1007/978-3-031-78238-1_1,,10.1007/978-3-031-78238-1_1,,,0,006-342-036-776-483; 009-257-768-706-694; 016-669-057-642-203; 029-201-580-226-616; 044-797-190-122-92X; 062-187-620-275-489; 066-373-768-842-624; 067-563-994-037-264; 074-750-119-776-525; 082-330-423-472-98X; 086-066-405-537-140; 112-265-783-697-662; 116-195-181-437-518; 134-983-314-777-269; 166-306-568-477-189; 175-237-813-984-652; 196-083-624-626-568,0,false,,
096-823-229-297-712,The New Quant: A Survey of Large Language Models in Financial Prediction and Trading,2025-10-07,2025,preprint,arXiv (Cornell University),,,,Weilong Fu,"Large language models are reshaping quantitative investing by turning unstructured financial information into evidence-grounded signals and executable decisions. This survey synthesizes research with a focus on equity return prediction and trading, consolidating insights from domain surveys and more than fifty primary studies. We propose a task-centered taxonomy that spans sentiment and event extraction, numerical and economic reasoning, multimodal understanding, retrieval-augmented generation, time series prompting, and agentic systems that coordinate tools for research, backtesting, and execution. We review empirical evidence for predictability, highlight design patterns that improve faithfulness such as retrieval first prompting and tool-verified numerics, and explain how signals feed portfolio construction under exposure, turnover, and capacity controls. We assess benchmarks and datasets for prediction and trading and outline desiderata-for time safe and economically meaningful evaluation that reports costs, latency, and capacity. We analyze challenges that matter in production, including temporal leakage, hallucination, data coverage and structure, deployment economics, interpretability, governance, and safety. The survey closes with recommendations for standardizing evaluation, building auditable pipelines, and advancing multilingual and cross-market research so that language-driven systems deliver robust and risk-controlled performance in practice.",,,,,Business; Econometrics; Financial economics; Economics,,,,,https://arxiv.org/abs/2510.05533,http://dx.doi.org/10.48550/arxiv.2510.05533,,10.48550/arxiv.2510.05533,,,0,,0,true,,green
097-241-164-415-314,"Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in
  Prompting and Fine-Tuning",2025-02-13,2025,preprint,arXiv (Cornell University),,,,Ang Li; Yichuan Mo; Mingjie Li; Yifei Wang; Yisen Wang,"Large Language Models (LLMs) have demonstrated remarkable success across various NLP benchmarks. However, excelling in complex tasks that require nuanced reasoning and precise decision-making demands more than raw language proficiency--LLMs must reason, i.e., think logically, draw from past experiences, and synthesize information to reach conclusions and take action. To enhance reasoning abilities, approaches such as prompting and fine-tuning have been widely explored. While these methods have led to clear improvements in reasoning, their impact on LLM safety remains less understood. In this work, we investigate the interplay between reasoning and safety in LLMs. We highlight the latent safety risks that arise as reasoning capabilities improve, shedding light on previously overlooked vulnerabilities. At the same time, we explore how reasoning itself can be leveraged to enhance safety, uncovering potential mitigation strategies. By examining both the risks and opportunities in reasoning-driven LLM safety, our study provides valuable insights for developing models that are not only more capable but also more trustworthy in real-world deployments.",,,,,SAFER; Cognitive psychology; International trade; Psychology; Business; Computer science; Computer security,,,,,https://arxiv.org/abs/2502.09673,http://dx.doi.org/10.48550/arxiv.2502.09673,,10.48550/arxiv.2502.09673,,,0,,0,true,,green
097-409-164-563-436,FinLLMs: A Framework for Financial Reasoning Dataset Generation With Large Language Models,,2025,journal article,IEEE Transactions on Big Data,23327790; 23722096,Institute of Electrical and Electronics Engineers (IEEE),,Ziqiang Yuan; Kaiyuan Wang; Shoutai Zhu; Ye Yuan; Jingya Zhou; Yanlin Zhu; Wenqi Wei,,11,5,2264,2277,,,,,National Natural Science Foundation of China; National Natural Science Foundation of China,,http://dx.doi.org/10.1109/tbdata.2024.3524083,,10.1109/tbdata.2024.3524083,,,0,010-918-453-969-378; 011-664-225-545-898; 011-804-254-756-004; 018-539-076-888-732; 027-096-746-989-53X; 031-109-865-045-253; 038-461-335-978-803; 039-313-866-945-674; 043-952-544-308-723; 050-233-396-111-576; 056-542-756-996-88X; 060-724-186-665-166; 080-878-370-554-949; 081-776-456-098-496; 096-082-868-445-261; 096-252-378-991-189; 110-127-601-868-566; 139-239-027-110-484; 140-503-889-364-109; 156-937-801-541-203; 158-534-212-420-864; 182-530-615-679-928; 197-673-280-254-524,5,false,,
097-547-394-710-513,"A Survey of Large Language Models for Financial Applications: Progress,
  Prospects and Challenges",2024-06-15,2024,preprint,arXiv (Cornell University),,,,Yuqi Nie; Yaxuan Kong; Xiaowen Dong; John M. Mulvey; H. Vincent Poor; Qingsong Wen; Stefan Zohren,"Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.",,,,,Computer science,,,,,https://arxiv.org/abs/2406.11903,http://dx.doi.org/10.48550/arxiv.2406.11903,,10.48550/arxiv.2406.11903,,,0,,7,true,,green
097-770-750-955-923,Development of AI Chatbots for Cancer Information: Reducing Hallucinations and Trade-Offs in Responses with Reliable Data (Preprint),2024-12-17,2024,preprint,,,JMIR Publications Inc.,,Sota Nishisako; Takahiro Higashi; Fumihiko Wakao,"<sec>;                     <title>BACKGROUND</title>;                         <p>Generative artificial intelligence (AI) is increasingly used to find information. Providing accurate information is essential to support cancer patients and their families; however, information returned by generative AIs is sometimes wrong. Returning wrong information is called hallucination.</p>;                 </sec>;                                 <sec>;                     <title>OBJECTIVE</title>;                         <p>We aimed to examine cancer information returned by generative AIs with retrieval-augmented generation (RAG) using cancer-specific information sources and general internet search.</p>;                 </sec>;                                 <sec>;                     <title>METHODS</title>;                         <p>We compiled 62 cancer-related questions in Japanese and compared the responses of conventional chatbots with GPT-4 and GPT-3.5 (-turbo-16K) without RAG. We developed generative AI chatbots with different reference information sources—RAG-equipped Cancer Information Service (CIS) chatbot and Google chatbot—and compared the characteristics of their responses with those generated by a conventional chatbot without RAG. The CIS chatbot system included CIS as the reference information source. The characteristics of the responses were analyzed.</p>;                 </sec>;                                 <sec>;                     <title>RESULTS</title>;                         <p>For questions on information issued by CIS, the rates of hallucinations for the CIS chatbot were 0% for GPT-4 and 6% for GPT-3.5, whereas those for the Google chatbot were 6% and 10%. For questions on information that is not issued by CIS, the Google chatbot generated hallucinations in 19% of cases using GPT-4 and 35% using GPT-3.5. The conventional chatbot returned hallucinations in approximately 40% of the responses. The reference data from Google searches was higher compared to CIS for producing hallucinations, with an odds ratio of 9.4, (95% confidence interval 1.2-17.5, P &lt; .01), and the odd ratio for the conventional chatbot was 16.1 (95% CI, 3.7-50.0, P &lt; .001).　The conventional chatbot responded to all questions, but the response rate decreased (36% to 81%) for chatbots with RAG. For questions on information not covered by CIS, the CIS chatbot did not respond, while the Google chatbot generated responses in 52% of the cases using GPT-4 and 71% using GPT-3.5.</p>;                 </sec>;                                 <sec>;                     <title>CONCLUSIONS</title>;                         <p>Using RAG with reliable information sources significantly reduced the hallucination rate of generative AI chatbots, and increased the ability to admit lack of information, making them more suitable for general use, where users need to be provided with accurate information.</p>;                 </sec>",,,,,Preprint; Psychology; Computer science; Data science; World Wide Web,,,,,,http://dx.doi.org/10.2196/preprints.70176,,10.2196/preprints.70176,,,0,029-930-188-233-926,0,false,,
097-775-815-258-831,Exploring the Boundaries of Financial Statement Fraud Detection with Large Language Models,2024-01-01,2024,preprint,,,Elsevier BV,,Georgia Boskou; Evrikleia Chatzipetrou; Eleftherios Tiakas; Efstathios Kirkos; Charalambos Spathis,,,,,,Financial statement; Statement (logic); Business; Computer science; Accounting; Linguistics; Philosophy; Audit,,,,,,http://dx.doi.org/10.2139/ssrn.4895081,,10.2139/ssrn.4895081,,,0,,1,false,,
097-778-642-489-765,Bridging finance and AI: a comprehensive survey of large language models in financial system,2025-07-16,2025,journal article,Digital Finance,25246984; 25246186,Springer Science and Business Media LLC,,Ameer Tamoor Khan; Shuai Li; Xinwei Cao,,7,4,679,701,Bridging (networking); Finance; Structured finance; Business; Economics; Computer science; Financial crisis; Macroeconomics; Computer network,,,,,,http://dx.doi.org/10.1007/s42521-025-00146-3,,10.1007/s42521-025-00146-3,,,0,002-189-600-394-643; 006-957-545-318-174; 012-132-514-521-998; 024-528-232-067-519; 032-471-769-867-765; 033-460-327-085-11X; 042-930-476-612-42X; 044-456-506-571-452; 044-579-322-796-833; 049-815-920-147-239; 051-097-954-026-767; 056-542-756-996-88X; 064-653-811-375-608; 070-541-703-797-636; 072-263-062-519-219; 073-925-440-642-545; 075-356-529-898-550; 077-969-127-026-500; 081-403-485-531-031; 081-776-456-098-496; 084-944-822-589-244; 099-598-524-559-309; 113-600-332-572-31X; 119-266-024-375-880; 121-915-742-459-383; 122-460-880-672-13X; 129-472-630-411-247; 135-400-689-428-435; 139-239-027-110-484; 141-407-691-449-883; 142-036-085-872-338; 146-335-085-869-417; 154-042-138-562-576; 165-599-890-459-662; 169-757-241-137-707; 171-228-001-252-94X; 189-973-804-567-981; 193-231-387-167-937; 197-673-280-254-524; 198-343-113-948-304; 199-793-312-825-387,0,false,,
098-233-156-717-634,From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models,2025-08-19,2025,preprint,arXiv (Cornell University),,,,Ziyan Kuang; Feiyu Zhu; Maowei Jiang; Yanzhao Lai; Zelin Wang; Zhitong Wang; Meikang Qiu; Jiajia Huang; Min Peng; Qianqian Xie; Sophia Ananiadou,"Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-KQA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research.",,,,,Cognition; Psychology; Finance; Computer science; Economics; Neuroscience,,,,,https://arxiv.org/abs/2508.13491,http://dx.doi.org/10.48550/arxiv.2508.13491,,10.48550/arxiv.2508.13491,,,0,,0,true,,green
098-347-418-846-360,Zero-Shot Question Answering over Financial Documents using Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Karmvir Singh Phogat; Chetan Harsha; Sridhar Dasaratha; Shashishekar Ramakrishna; Sai Akhil Puranam,"We introduce a large language model (LLM) based approach to answer complex questions requiring multi-hop numerical reasoning over financial reports. While LLMs have exhibited remarkable performance on various natural language and reasoning tasks, complex reasoning problems often rely on few-shot prompts that require carefully crafted examples. In contrast, our approach uses novel zero-shot prompts that guide the LLM to encode the required reasoning into a Python program or a domain specific language. The generated program is then executed by a program interpreter, thus mitigating the limitations of LLM in performing accurate arithmetic calculations. We evaluate the proposed approach on three financial datasets using some of the recently developed generative pretrained transformer (GPT) models and perform comparisons with various zero-shot baselines. The experimental results demonstrate that our approach significantly improves the accuracy for all the LLMs over their respective baselines. We provide a detailed analysis of the results, generating insights to support our findings. The success of our approach demonstrates the enormous potential to extract complex domain specific numerical reasoning by designing zero-shot prompts to effectively exploit the knowledge embedded in LLMs.",,,,,Computer science; Transformer; Exploit; Interpreter; Shot (pellet); Generative grammar; Python (programming language); Natural language understanding; Artificial intelligence; Natural language processing; Machine learning; Natural language; Programming language; Computer security; Chemistry; Physics; Organic chemistry; Quantum mechanics; Voltage,,,,,https://arxiv.org/abs/2311.14722,http://dx.doi.org/10.48550/arxiv.2311.14722,,10.48550/arxiv.2311.14722,,,0,,1,true,cc-by-nc-nd,green
098-367-211-806-116,AI-Driven Financial Chart Analysis with Benchmarks: A Domain-Specific Large Language Model Approach,2025-08-22,2025,book chapter,Blockchain Technologies,26618338; 26618346,Springer Nature Singapore,,Hyoseok Jang; Sangchul Lee; Haneol Cho; Chansoo Kim,,,,173,187,Chart; Computer science; Domain (mathematical analysis); Finance; Econometrics; Business; Economics; Statistics; Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.1007/978-981-96-5833-6_10,,10.1007/978-981-96-5833-6_10,,,0,000-846-266-307-085; 045-201-107-411-584; 059-645-631-899-796; 086-354-238-807-647; 126-705-680-320-410; 132-092-018-666-706,0,false,,
098-400-022-049-955,Comparative Analysis of Large Language Models for Question Answering from Financial Documents,2024-05-11,2024,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Singapore,,Shivam Panwar; Anukriti Bansal; Farhana Zareen,,,,297,308,Question answering; Computer science; Linguistics; Natural language processing; Philosophy,,,,,,http://dx.doi.org/10.1007/978-981-97-2079-8_23,,10.1007/978-981-97-2079-8_23,,,0,003-103-139-358-869; 005-407-048-718-561; 006-096-525-177-667; 007-583-154-053-159; 011-518-061-224-960; 014-330-638-044-118; 021-679-491-347-397; 056-542-756-996-88X; 066-916-104-242-448; 089-712-403-745-387; 097-244-637-392-866; 122-912-780-160-721; 141-982-660-885-472; 186-523-242-363-743,1,false,,
098-417-412-007-778,Adaptive Confidence-Weighted LLM Infusion for Financial Reinforcement Learning,2025-05-09,2025,conference proceedings article,2025 IEEE 11th International Conference on Intelligent Data and Security (IDS),,IEEE,,Emran Y. Alturki; Aydin Javadov; Qiyang Sun; Björn W. Schuller,,,,79,82,Reinforcement learning; Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/ids66066.2025.00025,,10.1109/ids66066.2025.00025,,,0,041-105-509-197-650; 067-240-567-001-391; 149-160-037-003-51X,0,false,,
098-428-380-981-106,Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting,,2023,conference proceedings article,Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track,,Association for Computational Linguistics,,Xinli Yu; Zheng Chen; Yanbin Lu,"Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100 stocks, we utilize public historical stock data, company metadata, and economic/financial news. Our experiments employ GPT-4 for zero-shot/few-shot inference and Open LLaMA for instruction-based fine-tuning. The study demonstrates LLMs' ability to generate well-reasoned decisions by leveraging cross-sequence information and extracting insights from text and price time series. We show that our LLM-based approach outperforms classic ARMA-GARCH and gradient-boosting tree models. Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4.",,,739,753,Computer science; Financial market; Metadata; Boosting (machine learning); Inference; Stock market; Artificial intelligence; Finance; Time series; Machine learning; Economics; World Wide Web; Paleontology; Horse; Biology,,,,,https://aclanthology.org/2023.emnlp-industry.69.pdf https://doi.org/10.18653/v1/2023.emnlp-industry.69,http://dx.doi.org/10.18653/v1/2023.emnlp-industry.69,,10.18653/v1/2023.emnlp-industry.69,,,1,,44,true,cc-by,hybrid
099-674-181-737-75X,Adoption of Chatbots for investment: An Extended model of UTAUT,,2025,preprint,,,Elsevier BV,,Yutong Chen; Chongwei Lu; Feng Xie,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5849100,,10.2139/ssrn.5849100,,,0,011-551-506-191-175; 018-681-457-732-114; 018-852-574-052-074; 028-725-407-844-307; 035-303-261-889-193; 040-676-326-566-073; 041-137-428-618-085; 045-055-256-086-671; 048-221-042-703-267; 052-764-013-367-562; 058-081-424-637-980; 058-284-800-892-927; 063-640-900-677-73X; 072-813-199-606-608; 074-601-462-746-690; 083-569-308-042-483; 083-989-154-270-555; 084-715-378-310-956; 084-981-486-982-751; 091-501-530-765-42X; 093-264-327-635-685; 103-480-535-142-049; 121-026-764-828-645; 134-431-995-899-732; 143-875-130-007-110; 174-058-398-744-992; 178-347-732-094-080; 178-417-584-995-194,0,false,,
100-285-062-417-432,National Research and Development Investment Trends for the Growth of the Artificial Intelligence Chatbot Service Industry,2025-04-30,2025,journal article,Journal of the Korea Academia-Industrial cooperation Society,19754701; 22884688,The Korea Academia-Industrial Cooperation Society,,Doyeon Lee; Keunhwan Kim,,26,4,691,705,Chatbot; Investment (military); Service (business); Engineering management; Business; Engineering; Computer science; Data science; Artificial intelligence; Marketing; Political science; Politics; Law,,,,,,http://dx.doi.org/10.5762/kais.2025.26.4.691,,10.5762/kais.2025.26.4.691,,,0,,0,true,,bronze
100-326-053-962-832,Deep Learning with LLM: A New Paradigm for Financial Market Prediction and Analysis,,2025,journal article,International Journal of Multidisciplinary Research and Growth Evaluation,25827138,Anfo Publication House,,Yuhong Mo; Hao Qin; Yushan Dong,"<jats:p>This study presents the development of an AI text generation detection tool based on the Transformer model, aiming to enhance the accuracy of AI text generation detection and provide a reference for future research. The tool employs a series of preprocessing steps on the text, including Unicode normalization, conversion to lowercase, removal of non-alphabetic characters using regular expressions, and the addition of spaces around punctuation marks. It also eliminates leading and trailing spaces, replaces consecutive ellipses with a single space, and connects text segments with a specified delimiter. Subsequently, non-alphabetic characters and redundant spaces are removed, and multiple consecutive spaces are replaced with a single space, followed by a second conversion to lowercase.; The deep learning model integrates layers of LSTM, Transformer, and CNN for text classification and sequence labeling tasks. The training and validation sets demonstrate that the model's loss decreased from 0.127 to 0.005, while the accuracy increased from 94.96% to 99.8%, indicating strong detection and classification capabilities for AI-generated text. The confusion matrix and accuracy metrics of the test set reveal a prediction accuracy of 99% for AI-generated text, with a precision of 0.99, recall of 1, and an F1 score of 0.99, achieving a highly accurate classification. The results suggest that this model holds great potential for widespread application in the field of AI text detection.</jats:p>",6,2,211,215,Deep learning; Artificial intelligence; Finance; Computer science; Business,,,,,,http://dx.doi.org/10.54660/.ijmrge.2025.6.2.211-215,,10.54660/.ijmrge.2025.6.2.211-215,,,0,,0,false,,
100-786-083-588-578,AI Chatbots in Financial Services,2025-09-05,2025,book chapter,Advances in Computational Intelligence and Robotics,23270411; 2327042x,IGI Global Scientific Publishing,,K. Balaji,"<jats:p>The rapid evolution of artificial intelligence (AI) has transformed customer interactions in financial services, with AI-driven chatbots emerging as a pivotal tool for enhancing personalized policy recommendations. These intelligent systems leverage machine learning algorithms, natural language processing (NLP), and real-time data analytics to understand customer preferences and provide tailored financial solutions. By automating policy advisory and customer support, AI chatbots improve service efficiency, reduce response times, and enhance user satisfaction. This article explores the role of AI chatbots in delivering personalized policy solutions, their impact on customer experience, and the challenges associated with their adoption in financial institutions. Additionally, it highlights emerging trends and future possibilities in AI-driven financial advisory services.</jats:p>",,,399,426,Financial services; Business; Finance,,,,,,http://dx.doi.org/10.4018/979-8-3373-2822-5.ch013,,10.4018/979-8-3373-2822-5.ch013,,,0,,0,false,,
101-689-089-648-852,IMPRESSIONS OF VIABILITY: HOW CURRENT ENROLLMENT MANAGEMENT PERSONNEL AND FORMER STUDENTS PERCEIVE THE IMPLEMENTATION OF A CHATBOT FOCUSED ON STUDENT FINANCIAL COMMUNICATION,2019-05-03,2019,,,,,,Courtney Robinson,,,,,,Psychology; Enrollment management; Chatbot; Medical education,,,,,https://aquila.usm.edu/cgi/viewcontent.cgi?article=1002&context=highereddoctoralprojects https://aquila.usm.edu/highereddoctoralprojects/2/,https://aquila.usm.edu/highereddoctoralprojects/2/,,,2948650390,,0,,0,false,,
101-751-761-108-737,"What Teaches Robots to Walk, Teaches Them to Trade too -- Regime
  Adaptive Execution using Informed Data and LLMs",2024-06-19,2024,preprint,arXiv (Cornell University),,,,Raeid Saqur,"Machine learning techniques applied to the problem of financial market forecasting struggle with dynamic regime switching, or underlying correlation and covariance shifts in true (hidden) market variables. Drawing inspiration from the success of reinforcement learning in robotics, particularly in agile locomotion adaptation of quadruped robots to unseen terrains, we introduce an innovative approach that leverages world knowledge of pretrained LLMs (aka. 'privileged information' in robotics) and dynamically adapts them using intrinsic, natural market rewards using LLM alignment technique we dub as ""Reinforcement Learning from Market Feedback"" (**RLMF**). Strong empirical results demonstrate the efficacy of our method in adapting to regime shifts in financial markets, a challenge that has long plagued predictive models in this domain. The proposed algorithmic framework outperforms best-performing SOTA LLM models on the existing (FLARE) benchmark stock-movement (SM) tasks by more than 15\% improved accuracy. On the recently proposed NIFTY SM task, our adaptive policy outperforms the SOTA best performing trillion parameter models like GPT-4. The paper details the dual-phase, teacher-student architecture and implementation of our model, the empirical results obtained, and an analysis of the role of language embeddings in terms of Information Gain.",,,,,Robot; Computer science; Artificial intelligence,,,,,https://arxiv.org/abs/2406.15508,http://dx.doi.org/10.48550/arxiv.2406.15508,,10.48550/arxiv.2406.15508,,,0,,0,true,,green
102-478-571-889-95X,Implicit bias in LLMs: Bias in financial advice based on implied gender,2024-01-01,2024,preprint,,,Elsevier BV,,Shir Etgar; Gal Oestreicher-Singer; Inbal Yahav,,,,,,Advice (programming); Gender bias; Implicit bias; Business; Finance; Psychology; Economics; Social psychology; Computer science; Programming language,,,,,,http://dx.doi.org/10.2139/ssrn.4880335,,10.2139/ssrn.4880335,,,0,000-767-845-510-787; 003-268-634-092-776; 006-635-819-665-420; 015-937-769-717-578; 019-121-752-495-765; 019-424-452-899-627; 020-628-841-233-896; 032-047-894-946-080; 033-208-737-829-148; 035-736-293-898-645; 037-926-184-571-299; 056-420-337-446-490; 080-079-881-374-949; 104-582-184-609-420; 105-612-643-957-602; 111-262-741-694-359; 112-613-740-253-349; 142-534-561-295-466; 147-270-667-334-350; 165-608-426-893-117; 177-272-378-550-388; 181-809-755-765-694,4,false,,
102-883-050-244-629,Incremental Information Mining of Inquiry Letters for Financial Distress Prediction --- Experimental Evidence Based on LLMs,2025-08-25,2025,preprint,,,Wiley,,Xiaojian Yang; Chao Zhang; wei cao; Weidong Zhu,"math_shortcuts This study researches the incremental value of textual information from regulatory inquiry letters and their responses for financial distress prediction (FDP). Specifically, this paper dissects incremental information from four perspectives: sentiment, readability, topic, and large language models (LLMs)-enhanced information. Taking China's listed companies from 2014 to 2023 as samples, this study constructs nine models by integrating the classic LightGBM model with incremental information across different dimensions. The findings indicate that using inquiry letter text data yields an AUC value of 0.7817 for predicting financial distress, representing a 6.22% enhancement compared to relying solely on financial indicators. More interestingly, incremental information features processed by LLMs offer greater incremental value compared to machine learning-based methods. Moreover, the topic incremental information in the inquiry letters, particularly that related to production and operations, plays a significant role in identifying financial distress. This study provides novel insights into the relationship between inquiry letters and FDP, while also broadening the application of LLMs in financial risk management. Simultaneously, the interpretability study of the SHAP model offers a more precise reference for investment decisions by information users.",,,,,Financial distress; Distress; Psychology; Business; Financial system; Clinical psychology,,,,,https://www.authorea.com/doi/pdf/10.22541/au.175611813.32693088/v1 https://doi.org/10.22541/au.175611813.32693088/v1,http://dx.doi.org/10.22541/au.175611813.32693088/v1,,10.22541/au.175611813.32693088/v1,,,0,,0,false,,
103-054-864-675-581,The Integration of Large Language Models in Financial Services: From Fraud Detection to Generative AI Applications,2024-12-12,2024,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,null Snehansh Devera Konda,"<jats:p>This comprehensive article examines the transformative impact of artificial intelligence in the financial services sector, focusing on the evolution from traditional applications to advanced AI systems. Through systematic analysis of implementation frameworks and regulatory considerations, this article demonstrates the sector's technological leadership, evidenced by a 56% higher AI implementation success rate compared to other industries. The article reveals how Large Language Models have revolutionized customer interactions, achieving 92% query resolution accuracy and 89% improvement in user engagement. Documentation and compliance processes have been transformed through AI automation, demonstrating an 82% improvement in real-time compliance monitoring and 89% enhancement in automated reporting accuracy. Financial institutions' established regulatory frameworks and mature governance structures have enabled superior technology integration, with an 82% cloud adoption rate and 85% risk management effectiveness. The article’s analysis of software development infrastructure shows significant advancement, with a 73% improvement in deployment frequency and 82% enhancement in code quality metrics. The article highlights how financial institutions' robust regulatory expertise provides a significant competitive advantage, demonstrated by 89% security compliance and 76% stronger compliance protocols. Drawing from comprehensive industry analyses and empirical evidence, this article contributes to the growing body of literature on AI implementation in regulated industries while providing practical insights for organizations balancing innovation with compliance requirements.</jats:p>",10,6,1652,1665,Financial services; Software deployment; Corporate governance; Computer science; Transformative learning; Competitive advantage; Process management; Accounting; Knowledge management; Business; Finance; Software engineering; Marketing; Psychology; Pedagogy,,,,,,http://dx.doi.org/10.32628/cseit241061208,,10.32628/cseit241061208,,,0,031-252-973-799-839; 037-154-238-083-286; 142-268-715-908-191,0,true,,gold
103-917-331-145-036,"Review of: ""A Dutch Financial Large Language Model""",2024-12-31,2024,review,,,Qeios Ltd,,Vipula Rawte,,,,,,Business; Finance,,,,,,http://dx.doi.org/10.32388/wiskgx,,10.32388/wiskgx,,,0,,0,false,,
104-016-269-525-553,CARE: A Framework for Correcting Numerical Hallucinations in LLM-Generated Financial Texts,2025-05-05,2025,conference proceedings article,2025 IEEE Conference on Artificial Intelligence (CAI),,IEEE,,Jian Kim; Woohwan Jung,,,,69,74,Computer science,,,,,,http://dx.doi.org/10.1109/cai64502.2025.00018,,10.1109/cai64502.2025.00018,,,0,008-917-803-121-216; 023-223-508-694-895; 027-036-803-742-740; 044-456-506-571-452; 049-612-770-510-353; 062-002-339-330-454; 064-236-814-735-667; 068-187-295-207-543; 100-079-979-085-604; 146-335-085-869-417; 151-368-045-382-905; 169-511-556-328-970; 182-073-478-206-401,0,false,,
104-130-596-086-171,Research on the application of large language model to financial digital literacy education,2024-12-20,2024,conference proceedings article,Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence,,ACM,,Yanbing Chen,,,,748,752,Financial literacy; Computer science; Finance; Business,,,,,,http://dx.doi.org/10.1145/3724504.3724627,,10.1145/3724504.3724627,,,0,045-044-037-884-290; 047-595-657-603-051; 137-097-409-194-448; 159-979-391-895-66X,1,false,,
104-236-175-981-27X,Can LLMs make trade-offs involving stipulated pain and pleasure states?,2024-11-01,2024,preprint,arXiv (Cornell University),,,,Geoff Keeling; Winnie Street; Martyna Stachaczyk; Daria Zakharova; Iulia M. Comsa; Anastasiya Sakovych; Isabella Logothetis; Zejia Zhang; Blaise Agüera y Arcas; Jonathan Birch,"Pleasure and pain play an important role in human decision making by providing a common currency for resolving motivational conflicts. While Large Language Models (LLMs) can generate detailed descriptions of pleasure and pain experiences, it is an open question whether LLMs can recreate the motivational force of pleasure and pain in choice scenarios - a question which may bear on debates about LLM sentience, understood as the capacity for valenced experiential states. We probed this question using a simple game in which the stated goal is to maximise points, but where either the points-maximising option is said to incur a pain penalty or a non-points-maximising option is said to incur a pleasure reward, providing incentives to deviate from points-maximising behaviour. Varying the intensity of the pain penalties and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-4o, and GPT-4o mini each demonstrated at least one trade-off in which the majority of responses switched from points-maximisation to pain-minimisation or pleasure-maximisation after a critical threshold of stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance over points-maximisation regardless of intensity, while tending to prioritise points over pleasure regardless of intensity. We discuss the implications of these findings for debates about the possibility of LLM sentience.",,,,,Pleasure; Pain and pleasure; Psychology; Economics; Social psychology; Cognitive psychology; Business; Neuroscience,,,,,https://arxiv.org/abs/2411.02432,http://dx.doi.org/10.48550/arxiv.2411.02432,,10.48550/arxiv.2411.02432,,,0,,1,true,,green
104-849-252-356-761,"Large-Language-Model Copilots on the Trading Floor: Impacts on Price Discovery, Conduct Governance, and Desk Productivity",2025-07-31,2025,journal article,International Journal of Innovative Research in Engineering & Multidisciplinary Physical Sciences,23497300,International Journal of Innovative Research in Engineering and Multidisciplinary Physical Sciences,,Nikhil Jarunde,"<jats:p>Major sell-side institutions have begun embedding large-language-model (LLM) “desk copilots” such as Bank of America’s Maestro and Goldman Sachs’ GS AI Assistant into sales-and-trading workflows to synthesize internal research, client flow data, and market-microstructure signals in real time (Financial News London, 2024; Reuters, 2024). This review paper surveys the emerging body of academic, regulatory, and practitioner literature on generative-AI trade assistants (GATAs), framing their potential to reshape pre-trade analytics across equities, foreign exchange, and derivatives markets. We synthesize findings on three core dimensions—information asymmetry, order-routing efficiency, and conduct-risk controls—and propose a conceptual evaluation framework to guide regulators and market participants. The paper concludes by identifying open research questions around model governance, fairness, and systemic risk propagation.</jats:p>",13,4,,,Corporate governance; Framing (construction); Price discovery; Analytics; Algorithmic trading; Desk; Workflow; Financial market; FinTech; Performativity; Business; Industrial organization; Economics; Finance; Financial services; Computer science; Data science; Management; Engineering; Operating system; Futures contract; Gender studies; Structural engineering; Sociology,,,,,https://www.ijirmps.org/papers/2025/4/232668.pdf https://doi.org/10.37082/ijirmps.v13.i4.232668,http://dx.doi.org/10.37082/ijirmps.v13.i4.232668,,10.37082/ijirmps.v13.i4.232668,,,0,,0,false,,
104-874-508-227-950,Accuracy and Bias Mitigation in GenAI / LLM-based Financial Underwriting and Clinical Summarization Systems,2024-10-05,2024,journal article,International Journal of Science and Research (IJSR),23197064,International Journal of Science and Research,,Praveen Kumar; Shailendra Bade,,13,10,55,59,Underwriting; Automatic summarization; Business; Finance; Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.21275/sr24930023705,,10.21275/sr24930023705,,,0,,0,true,,bronze
105-077-081-951-775,Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Xianzhi Li; Samuel Chan; Xiaodan Zhu; Yulong Pei; Zhiqiang Ma; Xiaomo Liu; Sameena Shah,"The most recent large language models(LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation. How effective are such models in the financial domain? Understanding this basic question would have a significant impact on many downstream financial analytical tasks. In this paper, we conduct an empirical study and provide experimental evidences of their performance on a wide variety of financial text analytical problems, using eight benchmark datasets from five categories of tasks. We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domain-specific pretrained models. We hope our study can help understand the capability of the existing models in the financial domain and facilitate further improvements.",,,,,Benchmark (surveying); Variety (cybernetics); Computer science; Domain (mathematical analysis); Analytics; Data science; Domain adaptation; Adaptation (eye); Range (aeronautics); State (computer science); Empirical research; Machine learning; Finance; Artificial intelligence; Engineering; Programming language; Mathematical analysis; Mathematics; Geodesy; Aerospace engineering; Classifier (UML); Geography; Philosophy; Physics; Epistemology; Optics; Economics,,,,,https://arxiv.org/abs/2305.05862,http://dx.doi.org/10.48550/arxiv.2305.05862,,10.48550/arxiv.2305.05862,,,0,,1,true,other-oa,green
105-346-817-844-626,Enhancing Literature Review with LLM and NLP Methods. Algorithmic Trading Case.,2024-01-01,2024,preprint,,,Elsevier BV,,Stanisław Łaniewski; Robert Ślepaczuk,,,,,,Computer science; Artificial intelligence; Natural language processing; Machine learning,,,,,,http://dx.doi.org/10.2139/ssrn.4996945,,10.2139/ssrn.4996945,,,0,026-709-908-303-899; 034-207-794-611-075; 039-989-565-484-384; 064-055-786-717-986; 091-075-555-728-204; 096-732-715-582-013; 104-826-562-930-229; 147-555-159-185-263; 156-445-453-052-591; 167-228-474-361-930; 185-680-468-732-92X; 191-424-188-274-666,1,false,,
105-383-449-371-88X,Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models,,2023,conference proceedings article,Findings of the Association for Computational Linguistics: EMNLP 2023,,Association for Computational Linguistics,,Sumuk Shashidhar; Abhinav Chinta; Vaibhav Sahai; Zhenhailong Wang; Heng Ji,"The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. The SoTA open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A generalized variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by three case studies on personal computing, gaming and enterprise solutions.",,,9070,9084,Computer science; Leverage (statistics); Performance metric; Ranking (information retrieval); Metric (unit); Benchmark (surveying); Machine learning; Business; Operations management; Engineering; Geodesy; Marketing; Geography,,,,,https://aclanthology.org/2023.findings-emnlp.608.pdf https://doi.org/10.18653/v1/2023.findings-emnlp.608 https://arxiv.org/pdf/2310.07611 https://arxiv.org/abs/2310.07611,http://dx.doi.org/10.18653/v1/2023.findings-emnlp.608,,10.18653/v1/2023.findings-emnlp.608,,,0,,5,true,cc-by,hybrid
105-597-916-305-851,LLM-Assisted Financial Fraud Detection with Reinforcement Learning,2025-12-15,2025,journal article,Algorithms,19994893,MDPI AG,Switzerland,Ahmed Djalal Hacini; Mohamed Benabdelouahad; Ishak Abassi; Sohaib Houhou; Aissa Boulmerka; Nadir Farhi,"<jats:p>Effective financial fraud detection requires systems that can interpret complex transaction semantics while dynamically adapting to asymmetric operational costs. We propose a hybrid framework in which a large language model (LLM) serves as an encoder, transforming heterogeneous transaction data into a unified embedding space. These embeddings define the state representation for a reinforcement learning (RL) agent, which acts as a fraud classifier optimized with business-aligned rewards that heavily penalize false negatives while controlling false positives. We evaluate the approach on two benchmark datasets—European Credit Card Fraud and PaySim—demonstrating that policy-gradient methods, particularly A2C, achieve high recall without sacrificing precision. Critically, our ablation study reveals that this hybrid architecture yields substantial performance gains on semantically rich transaction logs, whereas the advantage diminishes on mathematically compressed, anonymized features. Our results highlight the potential of coupling LLM-driven representations with RL policies for cost-sensitive and adaptive fraud detection.</jats:p>",18,12,792,,,,,,,,http://dx.doi.org/10.3390/a18120792,,10.3390/a18120792,,,0,006-245-721-640-352; 010-333-328-643-295; 016-269-848-088-621; 017-037-956-347-495; 020-180-321-732-373; 021-045-889-002-853; 027-728-477-430-991; 027-963-057-662-155; 028-411-078-320-390; 035-277-967-833-996; 035-752-052-278-330; 040-655-498-747-193; 054-684-032-854-553; 058-918-036-879-234; 059-214-373-186-405; 064-277-359-371-988; 072-530-777-297-211; 072-658-198-050-962; 098-731-697-322-493; 099-875-983-110-634; 113-003-744-658-624; 119-982-860-219-610; 124-690-193-709-451; 139-239-027-110-484; 140-787-299-158-679; 145-052-511-072-624; 152-867-377-923-002; 161-087-778-662-439; 161-424-448-927-152; 165-821-102-419-323; 175-366-790-208-597; 175-802-632-517-034; 188-856-834-593-575,0,true,cc-by,gold
105-659-743-642-788,"FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial
  Datasets",2025-05-26,2025,preprint,arXiv (Cornell University),,,,Dannong Wang; Jaisal Patel; Daochen Zha; Steve Y. Yang; Xiao-Yang Liu,"Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at https://github.com/Open-Finance-Lab/FinLoRA",,,,,Benchmarking; Business; Finance; Marketing,,,,,https://arxiv.org/abs/2505.19819,http://dx.doi.org/10.48550/arxiv.2505.19819,,10.48550/arxiv.2505.19819,,,0,,0,true,,green
105-762-817-516-641,Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Dimitris Vamvourellis; Dhagash Mehta,,,,299,307,,,,,,,http://dx.doi.org/10.1145/3768292.3770341,,10.1145/3768292.3770341,,,0,026-951-928-974-847; 032-098-673-230-943; 033-998-659-239-52X; 057-982-454-558-150; 074-027-232-309-66X; 075-356-529-898-550; 095-761-242-666-669; 125-773-831-138-869; 137-949-327-802-777; 143-238-151-763-271; 154-042-138-562-576; 160-059-512-684-417; 185-779-605-347-797; 190-050-627-029-962,0,false,,
105-772-178-512-773,An End-To-End LLM Enhanced Trading System,2025-02-03,2025,preprint,arXiv (Cornell University),,,,Ziyao Zhou; Ronitt Mehra,"This project introduces an end-to-end trading system that leverages Large Language Models (LLMs) for real-time market sentiment analysis. By synthesizing data from financial news and social media, the system integrates sentiment-driven insights with technical indicators to generate actionable trading signals. FinGPT serves as the primary model for sentiment analysis, ensuring domain-specific accuracy, while Kubernetes is used for scalable and efficient deployment.",,,,,End-to-end principle; End user; End of history; Dead end; Business; Computer science; World Wide Web; Computer security; Mathematics; Political science; Flow (mathematics); Geometry; Politics; Law,,,,,https://arxiv.org/abs/2502.01574,http://dx.doi.org/10.48550/arxiv.2502.01574,,10.48550/arxiv.2502.01574,,,0,,1,true,,green
105-813-260-999-646,Estimating the Harmlessness-Accuracy Trade-off in AI: Evidence from LLM-based Hiring Audits,2025-01-01,2025,preprint,,,Elsevier BV,,Yanuo Zhou,,,,,,Audit; Business; Computer science; Accounting; Economics,,,,,,http://dx.doi.org/10.2139/ssrn.5464597,,10.2139/ssrn.5464597,,,0,,0,false,,
105-852-034-846-66X,Leveraging Conversational AI for Adolescent Medical Financial Education,2025-05-22,2025,journal article,Conference on Digital Government Research,30508681,TU Delft OPEN Publishing,,Wesley Chen; Anthony Le; Pavan Athota; Md Ashaduzzaman; Chun-Hua Tsai,"<jats:p>Medical financial literacy is essential to make smart decisions in healthcare settings and prevent unanticipated financial hardships. Existing literature has shown that young adults often struggle to understand information associated with health insurance and the financial planning necessary for health-related costs. AI-driven chatbots are emerging as educational tools that have the potential to address this issue. This exploratory study examined an AI chatbot aimed at enhancing medical financial literacy among high school students. Participants engaged with the chatbot’s responses to medical financial questions while also rating the clarity, ease of use, trustworthiness, and educational value of the chatbot engagement. Our experiment results supported that the chatbot increased students’ understanding of the financial aspect of healthcare - 76.9 percent of students reported a high degree of understanding, 80.8 percent rated the chatbot’s responses as clear, and 73.1 percent reported they would recommend it to a peer. The responses indicated that students found the chatbot helpful, but suggested that interactive features be added and/or real-world finance features be incorporated into the chatbot.</jats:p>",1,,,,Psychology; Finance; Computer science; Medical education; Business; Medicine,,,,,,http://dx.doi.org/10.59490/dgo.2025.1020,,10.59490/dgo.2025.1020,,,0,,0,false,,
105-957-861-341-500,GPTQuant's Conversational AI: Simplifying Investment Research for All,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Thomas Yue; Chi Chung Au,,,,,,Investment (military); Psychology; Computer science; Business; Economics; Natural language processing; Political science; Law; Politics,,,,,,http://dx.doi.org/10.2139/ssrn.4380516,,10.2139/ssrn.4380516,,,0,,6,false,,
106-032-767-175-122,"Enhancing Financial Inclusion and Regulatory Challenges: A Critical
  Analysis of Digital Banks and Alternative Lenders Through Digital Platforms,
  Machine Learning, and Large Language Models Integration",2024-04-18,2024,preprint,arXiv (Cornell University),,,,Luke Lee,"This paper explores the dual impact of digital banks and alternative lenders on financial inclusion and the regulatory challenges posed by their business models. It discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) in enhancing financial services accessibility for underserved populations. Through a detailed analysis of operational frameworks and technological infrastructures, this research identifies key mechanisms that facilitate broader financial access and mitigate traditional barriers. Additionally, the paper addresses significant regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection. Employing a mixed-methods approach, which combines quantitative financial data analysis with qualitative insights from industry experts, this paper elucidates the complexities of leveraging digital technology to foster financial inclusivity. The findings underscore the necessity of evolving regulatory frameworks that harmonize innovation with comprehensive risk management. This paper concludes with policy recommendations for regulators, financial institutions, and technology providers, aiming to cultivate a more inclusive and stable financial ecosystem through prudent digital technology integration.",,,,,Financial inclusion; Inclusion (mineral); Computer science; Financial services; Business; Finance; Psychology; Social psychology,,,,,https://arxiv.org/abs/2404.11898,http://dx.doi.org/10.48550/arxiv.2404.11898,,10.48550/arxiv.2404.11898,,,0,,0,true,,green
106-064-413-375-633,AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative Contracts,,2025,conference proceedings article,Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence,,International Joint Conferences on Artificial Intelligence Organization,,Maruf Ahmed Mridul; Ian Sloyan; Aparna Gupta; Oshani Seneviratne,"<jats:p>Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) are reshaping how AI systems extract and organize information from unstructured text. A key challenge is designing AI methods that can incrementally extract, structure, and validate information while preserving hierarchical and contextual relationships. We introduce CDMizer, a template driven, LLM, and RAG-based framework for structured text transformation. By leveraging depth-based retrieval and hierarchical generation, CDMizer ensures a controlled, modular process that aligns generated outputs with predefined schemas. Its template-driven approach guarantees syntactic correctness, schema adherence, and improved scalability, addressing key limitations of direct generation methods. Additionally, we propose an LLM-powered evaluation framework to assess the completeness and accuracy of structured representations. Demonstrated in the transformation of Over-the-Counter (OTC) financial derivative contracts into the Common Domain Model (CDM), CDMizer establishes a scalable foundation for AI-driven document understanding, structured synthesis, and automated validation in broader contexts.</jats:p>",,,9305,9312,Derivative (finance); Encoding (memory); Business; Computer science; Finance; Artificial intelligence,,,,,,http://dx.doi.org/10.24963/ijcai.2025/1034,,10.24963/ijcai.2025/1034,,,0,,2,false,,
106-350-573-043-424,Adversarially Enhanced Financial Misinformation: A Comparative Analysis of LLM- vs. GAN-Generated Content Exposing AI Moderation Vulnerabilities,2025-05-07,2025,conference proceedings article,"2025 6th International Conference on Artificial Intelligence, Robotics and Control (AIRC)",,IEEE,,Christopher Santorelli; Victor Ginart Belmonte; Ryan Mastropaolo,,,,250,256,Moderation; Misinformation; Computer science; Content (measure theory); Computer security; Machine learning; Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.1109/airc64931.2025.11077509,,10.1109/airc64931.2025.11077509,,,0,003-550-864-380-166; 087-629-626-981-547; 095-710-293-832-743; 096-704-258-326-787; 145-635-742-936-690; 186-221-799-922-73X,0,false,,
106-750-809-861-515,"WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine",2023-01-01,2023,preprint,arXiv (Cornell University),,,,Siqiao Xue; Fan Zhou; Yi Xu; Ming Jin; Qingsong Wen; Hongyan Hao; Qingyang Dai; Caigao Jiang; Hongyu Zhao; Shuo Xie; Jianshan He; James Zhang; Hongyuan Mei,"We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as ""How should I manage my investments during inflation?"", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeqnlrMc.",,,,,Knowledge base; Base (topology); Computer science; Business; Knowledge management; Finance; Artificial intelligence; Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2308.05361,http://dx.doi.org/10.48550/arxiv.2308.05361,,10.48550/arxiv.2308.05361,,,0,,1,true,cc-by,green
106-963-046-279-605,Are LLMs Rational Investors? A Study on the Financial Bias in LLMs,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: ACL 2025,,Association for Computational Linguistics,,Yuhang Zhou; Yuchen Ni; Zhiheng Xi; Zhangyue Yin; Yu He; Gan Yunhui; Xiang Liu; Zhang Jian; Sen Liu; Xipeng Qiu; Yixin Cao; Guangnan Ye; Hongfeng Chai,,,,24139,24173,Business; Finance,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-acl.1239,,10.18653/v1/2025.findings-acl.1239,,,0,,2,false,,
107-034-407-842-417,Harnessing large language models for ESG analysis: Evaluating non-financial factors in stock prices,2025-01-01,2025,preprint,,,Elsevier BV,,Mengdi Zhang; Bo Zhang; Zhiheng Zhao; Yulan Wang; George Q. Huang,,,,,,Stock (firearms); Business; Financial economics; Economics; Geography; Archaeology,,,,,,http://dx.doi.org/10.2139/ssrn.5508485,,10.2139/ssrn.5508485,,,0,007-372-720-824-14X; 009-542-155-285-279; 010-191-196-034-023; 014-671-789-822-728; 017-576-416-109-712; 022-161-437-661-000; 053-978-676-821-551; 071-729-792-301-794; 120-923-204-104-693; 131-337-012-300-857; 153-939-232-752-691; 157-014-112-050-549; 172-927-272-622-876; 192-743-385-236-655,0,false,,
107-065-484-449-565,MEASURING INVESTOR SENTIMENT WITH LARGE LANGUAGE MODELS FOR SYSTEMIC RISK EARLY WARNING IN THE FINANCIAL SECTOR: AN EMPIRICAL STUDY BASED ON THE INFORMER MODEL,,2025,preprint,,,Elsevier BV,,Jinwang Wu; Zhongrui Wu; hongliang tao,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5737541,,10.2139/ssrn.5737541,,,0,006-399-176-500-152; 022-060-283-781-188; 022-836-714-907-869; 023-688-164-797-611; 028-809-688-907-391; 040-672-185-519-039; 044-413-431-647-829; 052-975-949-278-68X; 055-375-800-335-754; 064-082-736-992-676; 065-439-824-042-211; 075-173-331-513-254; 078-981-577-025-706; 084-090-075-592-338; 087-053-648-444-575; 088-730-699-845-696; 089-844-745-151-655; 095-037-640-171-955; 104-632-298-356-698; 109-815-146-075-401; 111-232-363-465-50X; 115-806-665-215-624; 120-219-827-138-284; 154-042-138-562-576; 160-838-562-631-770; 161-926-073-817-466; 191-273-643-101-746,0,false,,
107-160-994-672-439,Sentiment trading with large language models,,2024,journal article,Finance Research Letters,15446123; 15446131,Elsevier BV,Netherlands,Kemal Kirtac; Guido Germano,"We analyze the performance of large language models (LLMs) including OPT, BERT, and FINBERT, alongside the traditional Loughran-McDonald dictionary, in sentiment analysis of 965,375 U.S. financial news articles from 2010 to 2023. Our findings reveal that OPT significantly outperforms others, accurately predicting stock market returns with a sentiment prediction accuracy of 74.4%. A long-short strategy based on OPT with 10 bps transactions costs yields a superior Sharpe ratio of 3.05. From August 2021 to July 2023, this strategy produces an impressive 355% gain, outperforming other strategies and traditional market portfolios. This underscores the potential of LLMs to transform financial market prediction and portfolio management, and the necessity of employing sophisticated language models to develop effective investment strategies based on news sentiment.",62,,105227,105227,Sharpe ratio; Portfolio; Project portfolio management; Stock market; Investment strategy; Financial market; Trading strategy; Financial economics; Sentiment analysis; Stock (firearms); Economics; Computer science; Econometrics; Finance; Artificial intelligence; Management; Engineering; Mechanical engineering; Paleontology; Horse; Project management; Market liquidity; Biology,,,,,http://arxiv.org/pdf/2412.19245 http://arxiv.org/abs/2412.19245 http://eprints.lse.ac.uk/122592/1/1_s2.0_S1544612324002575_main.pdf,http://dx.doi.org/10.1016/j.frl.2024.105227,,10.1016/j.frl.2024.105227,,,0,006-399-176-500-152; 017-358-296-871-204; 019-719-570-401-015; 021-412-594-162-528; 022-486-017-682-206; 036-979-852-259-747; 047-871-035-706-003; 052-170-807-894-470; 057-982-454-558-150; 078-981-577-025-706; 086-260-434-815-196; 094-992-121-743-156; 100-446-896-445-856; 141-501-772-987-035; 154-042-138-562-576; 154-058-408-974-033; 183-026-309-652-990; 184-448-049-219-598,40,true,cc-by,hybrid
107-162-277-677-308,FIN2SUM: Advancing AI-Driven Financial Text Summarization with LLMs,2024-03-22,2024,conference proceedings article,2024 International Conference on Trends in Quantum Computing and Emerging Business Technologies,,IEEE,,Ezhilan Wilson; Anshul Saxena; Jayant Mahajan; Lekha Panikulangara; Shruti Kulkarni; Pritty Jain,,,,1,5,Automatic summarization; Computer science; Finance; Business; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/tqcebt59414.2024.10545078,,10.1109/tqcebt59414.2024.10545078,,,0,009-679-682-056-58X; 018-866-913-622-957; 021-323-111-834-306; 039-187-427-813-456; 060-223-297-932-671; 085-308-973-734-532; 092-887-621-998-764; 118-133-750-452-937; 122-748-484-687-367; 142-461-378-054-432; 153-933-761-161-582; 161-163-126-260-743; 191-420-643-158-483,5,false,,
107-265-267-277-628,Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment,2025-09-07,2025,preprint,arXiv (Cornell University),,,,Gang Cheng; Haibo Jin; Wenbin Zhang; Haohan Wang; Jun Zhuang,"Large Language Models (LLMs) are increasingly integrated into financial applications, yet existing red-teaming research primarily targets harmful content, largely neglecting regulatory risks. In this work, we aim to investigate the vulnerability of financial LLMs through red-teaming approaches. We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that iteratively conceals regulatory risks to provoke seemingly compliant yet regulatory-violating responses from LLMs. To enable systematic evaluation, we construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA effectively bypasses nine mainstream LLMs, achieving an average attack success rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1. These findings reveal a critical gap in current alignment techniques and underscore the urgent need for stronger moderation mechanisms in financial domains. We hope this work offers practical insights for advancing robust and domain-aware LLM alignment.",,,,,Vulnerability (computing); Financial risk; Domain (mathematical analysis); Business; Computer science; Psychology; Finance; Computer security; Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2509.10546,http://dx.doi.org/10.48550/arxiv.2509.10546,,10.48550/arxiv.2509.10546,,,0,,0,true,,green
107-287-375-600-290,LangKG at the FinNLP 2025 - Earnings2Insights: Task-Adaptive LLMs To Generate Human-Persuasive Investment Reports,,2025,conference proceedings article,Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Shivika Prasanna; Hui Su,,,,301,305,,,,,,,http://dx.doi.org/10.18653/v1/2025.finnlp-2.23,,10.18653/v1/2025.finnlp-2.23,,,0,,0,false,,
107-407-328-795-199,Quantitative Financial Models with Scenarios from LLM: Temporal Fusion Transformers as Alternative Monte-Carlo,2024-01-01,2024,preprint,,,Elsevier BV,,Irene Aldridge; Daham Kim,,,,,,Monte Carlo method; Computer science; Econometrics; Economics; Mathematics; Statistics,,,,,,http://dx.doi.org/10.2139/ssrn.4999492,,10.2139/ssrn.4999492,,,0,006-142-658-920-471; 014-321-074-078-595; 116-052-853-310-74X; 128-717-674-700-24X; 145-337-594-751-479; 157-415-704-475-53X,0,false,,
107-513-938-873-392,Large language models (LLMs) and financial analysis,2024-09-30,2024,journal article,The Business & Management Collection,20597177,Henry Stewart Talks,,Alejandro Lopez-Lira,,2024,9,e1006319,e1006319,Business; Economics; Finance,,,,,,http://dx.doi.org/10.69645/pldt5076,,10.69645/pldt5076,,,0,,0,false,,
107-586-477-142-331,LLM-based TypeScript generation and asset management for procedural synthesis of scenes and data for AI,2025-05-29,2025,conference proceedings article,"Synthetic Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and Applications III",,SPIE,,Jeffrey Kerley; Derek Anderson; Brendan Alvey,,,,38,38,TypeScript; Computer science; Asset management; Asset (computer security); Natural language processing; Artificial intelligence; Programming language; Business; Computer security; Finance,,,,,,http://dx.doi.org/10.1117/12.3052839,,10.1117/12.3052839,,,0,,0,false,,
107-646-382-723-215,When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning,2025-11-15,2025,conference proceedings article,"2025 IEEE International Performance, Computing, and Communications Conference (IPCCC)",,IEEE,,Sichen Zhu; Hoyeung Leung; Xiaoyi Wang; Jia Wei; Honghui Xu,,,,1,6,,,,,,,http://dx.doi.org/10.1109/ipccc66453.2025.11304628,,10.1109/ipccc66453.2025.11304628,,,0,004-853-485-857-424; 013-784-700-024-207; 018-866-913-622-957; 041-234-936-513-540; 042-850-917-448-128; 057-982-454-558-150; 060-300-769-233-251; 066-372-354-709-749; 078-591-989-283-257; 110-080-967-348-578; 124-154-878-857-378; 127-554-902-275-40X; 147-129-547-890-798; 150-971-795-857-826; 154-042-138-562-576; 159-585-251-610-202; 162-360-693-129-265; 192-806-908-794-847,0,false,,
107-938-414-149-68X,Leveraging Large Language Models and Retrieval-Augmented Generation for Enhanced Multi-Asset Portfolio Construction,2025-03-17,2025,conference proceedings article,2025 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CiFer),,IEEE,,Ahmadreza Hajaghaie; Ruppa K. Thulasiram,,,,1,7,Computer science; Portfolio; Asset (computer security); Artificial intelligence; Natural language processing; Business; Finance; Computer security,,,,SEGS; Natural Sciences and Engineering Research Council,,http://dx.doi.org/10.1109/cifer64978.2025.10975739,,10.1109/cifer64978.2025.10975739,,,0,003-904-255-188-895; 127-554-902-275-40X; 166-989-897-223-732; 192-806-908-794-847; 196-395-945-746-897; 198-573-119-668-956,0,false,,
107-971-086-167-962,EFS: Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models,2025-07-23,2025,preprint,arXiv (Cornell University),,,,Haochen Luo; Yuan Zhang; Chen Liu,"Sparse portfolio optimization is a fundamental yet challenging problem in quantitative finance, since traditional approaches heavily relying on historical return statistics and static objectives can hardly adapt to dynamic market regimes. To address this issue, we propose Evolutionary Factor Search (EFS), a novel framework that leverages large language models (LLMs) to automate the generation and evolution of alpha factors for sparse portfolio construction. By reformulating the asset selection problem as a top-m ranking task guided by LLM-generated factors, EFS incorporates an evolutionary feedback loop to iteratively refine the factor pool based on performance. Extensive experiments on five Fama-French benchmark datasets and three real-market datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly outperforms both statistical-based and optimization-based baselines, especially in larger asset universes and volatile conditions. Comprehensive ablation studies validate the importance of prompt composition, factor diversity, and LLM backend choice. Our results highlight the promise of language-guided evolution as a robust and interpretable paradigm for portfolio optimization under structural constraints.",,,,,Factor (programming language); Computer science; Portfolio; Portfolio optimization; Artificial intelligence; Programming language; Economics; Financial economics,,,,,https://arxiv.org/abs/2507.17211,http://dx.doi.org/10.48550/arxiv.2507.17211,,10.48550/arxiv.2507.17211,,,0,,0,true,,green
108-194-166-742-464,"FinTral: A Family of GPT-4 Level Multimodal Financial Large Language
  Models",2024-02-16,2024,preprint,arXiv (Cornell University),,,,Gagan Bhatia; El Moatez Billah Nagoudi; Hasan Cavusoglu; Muhammad Abdul-Mageed,"We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to excel in real-time analysis and decision-making in diverse financial contexts.",,,,,Business; Finance; Computer science,,,,,https://arxiv.org/abs/2402.10986,http://dx.doi.org/10.48550/arxiv.2402.10986,,10.48550/arxiv.2402.10986,,,0,,1,true,,green
108-454-016-279-265,From Earnings Calls to Investment Reports: Evaluating Role-based Multi-Agent LLM Systems,,2025,conference proceedings article,Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Ranjan Satapathy; Raphael Liew; Joyjit Chattorj; Erik Cambria; Rick Goh,,,,258,267,,,,,,,http://dx.doi.org/10.18653/v1/2025.finnlp-2.19,,10.18653/v1/2025.finnlp-2.19,,,0,,0,false,,
109-050-116-444-489,CyberLLMInstruct: A Pseudo-Malicious Dataset Revealing Safety-Performance Trade-offs in Cyber Security LLM Fine-tuning,2025-10-13,2025,conference proceedings article,Proceedings of the 18th ACM Workshop on Artificial Intelligence and Security,,ACM,,Adel ElZemity; Budi Arief; Shujun Li,,,,77,88,,,,,,,http://dx.doi.org/10.1145/3733799.3762968,,10.1145/3733799.3762968,,,0,001-879-270-324-379; 002-875-647-652-638; 005-184-552-810-943; 008-856-164-963-372; 039-641-612-378-457; 048-459-823-810-567; 048-601-780-968-122; 048-649-620-902-629; 051-234-923-896-725; 057-576-932-354-037; 069-502-796-364-524; 079-927-095-270-775; 084-155-873-388-133; 092-861-242-986-733; 094-038-313-269-566; 095-129-489-214-788; 096-051-952-316-25X; 102-345-743-067-567; 106-186-923-771-367; 110-020-772-803-724; 110-297-812-542-568; 114-134-814-831-637; 125-812-396-894-254; 141-982-660-885-472; 145-681-469-101-509; 149-284-503-897-560; 174-954-248-355-217; 178-379-881-684-769; 182-186-775-754-763; 182-738-555-672-662; 188-749-005-097-511; 199-001-419-660-975,0,false,,
109-184-420-030-073,Large Language Model Adaptation for Financial Sentiment Analysis,,2023,conference proceedings article,Proceedings of the Sixth Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Pau Rodriguez Inserte; Mariam Nakhlé; Raheel Qader; Gaetan Caillaut; Jingshu Liu,"Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",,,1,10,Adaptation (eye); Computer science; Sentiment analysis; Language model; Artificial intelligence; Psychology; Neuroscience,,,,,https://aclanthology.org/2023.finnlp-2.1.pdf https://doi.org/10.18653/v1/2023.finnlp-2.1 http://arxiv.org/pdf/2401.14777 http://arxiv.org/abs/2401.14777,http://dx.doi.org/10.18653/v1/2023.finnlp-2.1,,10.18653/v1/2023.finnlp-2.1,,,0,,6,true,cc-by,hybrid
109-291-135-952-726,Toward profitable energy futures trading strategies using reinforcement learning incorporating disagreement and connectedness methods enabled by large language models,,2025,journal article,Energy and AI,26665468,Elsevier BV,,Tianxiang Cui; Yujian Ye; Yiran Li; Nanjiang Du; Xingke Song; Yicheng Zhu; Xiaoying Yang; Goran Strbac,,21,,100562,100562,Futures contract; Social connectedness; Reinforcement learning; Energy (signal processing); Computer science; Reinforcement; Artificial intelligence; Financial economics; Psychology; Economics; Mathematics; Social psychology; Statistics,,,,Zhejiang Province Natural Science Foundation; Jiangsu Province Natural Science Foundation; National Natural Science Foundation of China; National Natural Science Foundation of China; University of Nottingham Ningbo China; Ningbo Municipal Natural Science Foundation,,http://dx.doi.org/10.1016/j.egyai.2025.100562,,10.1016/j.egyai.2025.100562,,,0,002-386-175-636-156; 002-664-761-217-998; 002-863-763-370-269; 003-370-904-320-830; 007-723-336-693-087; 008-703-624-505-499; 010-679-941-947-586; 012-101-617-967-675; 013-985-422-719-500; 014-613-376-281-980; 016-030-231-154-903; 016-258-874-719-198; 018-845-177-223-735; 019-026-077-243-664; 020-983-309-049-925; 022-166-475-896-954; 023-125-017-386-661; 026-928-528-336-218; 027-862-320-774-945; 029-307-412-221-790; 031-946-680-939-128; 034-649-431-839-415; 037-466-913-023-002; 037-945-187-262-559; 039-322-713-367-142; 046-765-103-611-167; 046-819-307-377-844; 047-663-730-812-338; 048-869-275-837-807; 049-875-704-834-194; 050-196-442-758-822; 052-761-216-342-96X; 053-062-410-727-007; 055-620-518-255-693; 056-315-536-932-227; 057-561-278-822-489; 059-962-957-997-731; 066-576-953-528-725; 066-804-148-567-926; 072-968-404-922-439; 077-560-755-173-807; 081-124-566-543-737; 083-724-816-251-019; 085-019-302-285-018; 086-459-181-701-825; 087-400-578-803-539; 087-722-209-165-529; 094-910-748-277-61X; 095-131-490-050-912; 099-693-358-936-310; 100-921-446-906-037; 105-393-256-094-440; 107-160-994-672-439; 107-436-978-604-721; 108-741-260-578-293; 109-148-848-665-258; 109-252-301-514-27X; 112-997-799-317-98X; 115-271-906-928-177; 119-031-570-848-300; 119-704-652-341-371; 121-557-998-550-757; 126-265-063-204-936; 131-598-297-153-154; 136-601-270-260-722; 136-991-039-890-357; 141-094-776-970-93X; 141-212-095-187-127; 146-217-758-609-314; 150-594-071-229-41X; 151-166-039-926-675; 160-721-312-702-338; 160-838-562-631-770; 167-624-408-906-319; 174-056-834-482-281; 183-103-008-779-775; 185-831-899-214-134; 187-488-793-112-305; 190-127-914-397-883; 197-082-584-937-098,1,true,"CC BY, CC BY-NC-ND, CC BY-NC",gold
109-730-326-161-566,Enhancing Portfolio Construction with Correlation Estimates from Large Language Models,2025-12-16,2025,journal article,The Journal of Financial Data Science,26403943,With Intelligence LLC,,Jang Ho Kim; Jaekyun Park; Jungwoo Shin; Geumil Bae,,,,jfds.2025.1.209,,,,,,,,http://dx.doi.org/10.3905/jfds.2025.1.209,,10.3905/jfds.2025.1.209,,,0,,0,false,,
109-809-639-073-737,BERT vs GPT for financial engineering,2024-04-24,2024,preprint,arXiv (Cornell University),,,,Edward Sharkey; Philip Treleaven,"The paper benchmarks several Transformer models [4], to show how these models can judge sentiment from a news event. This signal can then be used for downstream modelling and signal identification for commodity trading. We find that fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this task. Transformer models have revolutionized the field of natural language processing (NLP) in recent years, achieving state-of-the-art results on various tasks such as machine translation, text summarization, question answering, and natural language generation. Among the most prominent transformer models are Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), which differ in their architectures and objectives. A CopBERT model training data and process overview is provided. The CopBERT model outperforms similar domain specific BERT trained models such as FinBERT. The below confusion matrices show the performance on CopBERT & CopGPT respectively. We see a ~10 percent increase in f1_score when compare CopBERT vs GPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights the importance of considering alternatives to GPT models for financial engineering tasks, given risks of hallucinations, and challenges with interpretability. We unsurprisingly see the larger LLMs outperform the BERT models, with predictive power. In summary BERT is partially the new XGboost, what it lacks in predictive power it provides with higher levels of interpretability. Concluding that BERT models might not be the next XGboost [2], but represent an interesting alternative for financial engineering tasks, that require a blend of interpretability and accuracy.",,,,,Business; Computer science,,,,,https://arxiv.org/abs/2405.12990,http://dx.doi.org/10.48550/arxiv.2405.12990,,10.48550/arxiv.2405.12990,,,0,,0,true,,green
110-198-517-819-129,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Environmental and Energy Law Sources",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Energy law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905473,https://guides.library.harvard.edu/c.php?g=1082940&p=7905473,,,3088928908,,0,,0,false,,
110-691-934-540-515,Effects of Cuteness Level and Gender Congruence of AI Financial Service Chatbot on Zone of Tolerance in Service Failure,2023-03-31,2023,journal article,JOURNAL OF THE KOREA CONTENTS ASSOCIATION,15984877; 25086723,The Korea Contents Association,,Sungjoon Lee,"본 연구는 AI(Artificial Intelligence) 금융 챗봇의 귀여움 수준이 서비스 실패 허용 정도에 어떤 영향을 미치는 지를 확인해보고자 수행되었다. 귀여움 수준이 서비스 실패 허용에 영향을 미치는 과정에서 젠더 일치성이 조절 효과를 가지는 지 역시 살펴보았다. 공감적 관여로서 부드러움이 매개 효과를 나타내는 지도 살펴보았다. 총 138명의 20대의 AI 챗봇 인지자들이 온라인 실험에 참가하였고, 실험 자극물로서 AI 금융 챗봇의 시각적 귀여움 수준 높고 낮음의 두 유형, AI 금융 챗봇과 이용자 간의 젠더 일치와 젠더 불일치 두 유형이 조작되었다. 실험을 통해 수집된 자료들은 부스트래핑 기반의 Process Macro(모델 5)을 통하여 분석되었다. 연구 결과, AI 금융 챗봇의 귀여움 수준이 높을수록 서비스 실패 허용 정도는 낮아지는 것으로 확인되었다. 젠더가 불일치하는 경우 귀여움 수준이 높아질수록 서비스 실패 허용 정도가 크게 감소하는 양상을 보이지만 젠더가 일치하는 경우에는 정반대의 양상으로 나타나 젠더 일치성의 조절 효과 또한 확인되었다. 그러나 공감적 관여로서 부드러움은 유의미한 매개 효과를 가지지 않는 것으로 나타났다. 본 연구의 결과들이 지닌 시사점에 대해서도 논의된다.",23,3,124,138,Chatbot; Congruence (geometry); Service (business); Computer science; Psychology; Artificial intelligence; Finance; Business; Social psychology; Marketing,,,,,,http://dx.doi.org/10.5392/jkca.2023.23.03.124,,10.5392/jkca.2023.23.03.124,,,0,,0,true,,gold
111-040-243-909-560,Can GPT-4 Sway Experts' Investment Decisions?,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: NAACL 2025,,Association for Computational Linguistics,,Takehiro Takayanagi; Hiroya Takamura; Kiyoshi Izumi; Chung-Chi Chen,,,,374,383,Investment (military); Computer science; Risk analysis (engineering); Business; Political science; Law; Politics,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-naacl.22,,10.18653/v1/2025.findings-naacl.22,,,0,,1,false,,
111-218-847-746-542,Automated Trading Framework Using LLM-Driven Features and Deep Reinforcement Learning,2025-12-11,2025,journal article,Big Data and Cognitive Computing,25042289,MDPI AG,,Ive Botunac; Tomislav Petković; Jurica Bosna,"<jats:p>Stock trading faces significant challenges due to market volatility and the complexity of integrating diverse data sources, such as financial texts and numerical market data. This paper proposes an innovative automated trading system that integrates advanced natural language processing (NLP) and deep reinforcement learning (DRL) to address these challenges. The system combines two novel components: PrimoGPT, a Transformer-based NLP model fine-tuned on financial texts using instruction-based datasets to generate actionable features like sentiment and trend direction, and PrimoRL, a DRL model that expands its state space with these NLP-derived features for enhanced decision-making precision compared to traditional DRL models like FinRL. An experimental evaluation over seven months of leading technology stocks reveals cumulative returns of up to 58.47% for individual stocks and 27.14% for a diversified portfolio, with a Sharpe ratio of 1.70, outperforming traditional and advanced benchmarks. This work advances AI-driven quantitative finance by offering a scalable framework that bridges qualitative analysis and strategic action, thereby fostering smarter and more equitable participation in financial markets.</jats:p>",9,12,317,,,,,,,,http://dx.doi.org/10.3390/bdcc9120317,,10.3390/bdcc9120317,,,0,000-454-346-263-577; 005-920-585-478-505; 005-989-438-190-750; 016-269-848-088-621; 016-389-729-125-801; 018-866-913-622-957; 022-486-017-682-206; 023-045-944-629-070; 024-199-048-864-679; 030-454-248-865-53X; 034-111-475-208-944; 034-980-852-479-593; 039-677-533-419-964; 039-948-159-918-65X; 040-639-636-867-071; 048-223-712-441-403; 052-588-959-654-431; 053-847-859-862-209; 057-040-097-643-496; 061-573-513-606-552; 062-412-251-485-832; 063-881-861-185-296; 065-295-571-961-753; 066-915-825-592-97X; 070-259-373-337-725; 071-423-367-050-689; 074-480-375-332-782; 080-546-368-819-636; 087-074-689-911-494; 093-020-743-999-444; 105-475-465-258-455; 120-465-148-480-137; 131-285-655-339-913; 134-875-353-218-453; 141-147-551-737-973; 141-501-772-987-035; 142-184-667-156-221; 154-042-138-562-576; 154-201-884-155-679; 158-317-319-889-661; 169-465-533-467-930; 186-827-515-649-986; 190-050-627-029-962; 190-624-303-228-420; 192-806-908-794-847,0,true,cc-by,gold
111-219-779-191-999,FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models,2024-01-01,2024,preprint,arXiv (Cornell University),,,,Ziqiang Yuan; Kaiyuan Wang; Shoutai Zhu; Ye Yuan; Jingya Zhou; Yanlin Zhu; Wenqi Wei,"Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set. Our experiments demonstrate that synthetic data generated by FinLLMs effectively enhances the performance of several large-scale numerical reasoning models in the financial domain, outperforming two established benchmark financial question-answering datasets.",,,,,Computer science; Merge (version control); Annotation; Question answering; Language model; Benchmark (surveying); Graph; Finance; Construct (python library); Natural language processing; Artificial intelligence; Information retrieval; Theoretical computer science; Programming language; Geodesy; Economics; Geography,,,,,https://arxiv.org/abs/2401.10744,http://dx.doi.org/10.48550/arxiv.2401.10744,,10.48550/arxiv.2401.10744,,,0,,1,true,other-oa,green
111-358-200-262-998,Large Language Model for Financial Insights: Building a Digest to Simplify Research Activities,2024-11-26,2024,conference proceedings article,2024 32nd Telecommunications Forum (TELFOR),,IEEE,,Dmitrii Sedov; Andrei Lazarev,,,,1,4,Computer science; Software engineering; Programming language; Finance; Business,,,,,,http://dx.doi.org/10.1109/telfor63250.2024.10819154,,10.1109/telfor63250.2024.10819154,,,0,069-507-976-179-943; 119-431-973-773-422,3,false,,
112-144-839-977-444,FinSearch: A Temporal-Aware Search Agent Framework for Real-Time Financial Information Retrieval with Large Language Models,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Yiqing Shen; Jingshu Zhang; Feng Chen; Kaiyuan Yan; Hongguang Li,,,,10,17,,,,,,,http://dx.doi.org/10.1145/3768292.3770382,,10.1145/3768292.3770382,,,0,018-866-913-622-957; 032-519-555-746-204; 032-590-743-698-790; 033-783-518-352-555; 041-406-141-721-707; 042-161-650-451-921; 051-185-403-638-986; 074-482-334-270-721; 097-409-164-563-436; 120-885-261-278-951; 123-416-761-144-076; 124-399-118-530-124; 127-589-206-449-189; 141-407-691-449-883; 186-557-343-909-915; 195-114-767-474-461,0,false,,
112-689-687-158-49X,FinGPT Agents: Reinforcement-Fine-Tuned LLMs for Financial Analytics,2025-11-07,2025,conference proceedings article,2025 IEEE 12th International Conference on Cyber Security and Cloud Computing (CSCloud),,IEEE,,Satish Chandra; G. Balakrishna,,,,355,357,,,,,,,http://dx.doi.org/10.1109/cscloud66326.2025.00066,,10.1109/cscloud66326.2025.00066,,,0,004-853-485-857-424; 057-982-454-558-150,0,false,,
112-690-022-198-84X,From News to Trends: A Financial Time Series Forecasting Framework with LLM-Driven News Sentiment Analysis and Selective State Spaces,2025-04-17,2025,preprint,,,Springer Science and Business Media LLC,,Renjie Wang; Minghui Sun; Limin Wang,"<title>Abstract</title>;         <p>Stock price prediction is inherently challenging due to market volatility and the influence of external factors. Traditional forecasting methods primarily rely on historical price data, limiting their ability to capture market sentiment embedded in financial news. To address this limitation, we propose Senti-MambaMoE, a novel model that integrates historical stock prices with sentiment information extracted from financial news. Specifically, we fine-tune a DeepSeek-based large language model (LLM) for financial sentiment classification and incorporate the extracted sentiment information into our predictive framework. At the core of our approach is MambaMoE, which leverages the efficiency of state space models (SSMs) to model long-range dependencies while maintaining linear computational complexity, making it well-suited for financial time series forecasting. Additionally, the MoE mechanism improves the model’s ability to capture diverse market behaviors by dynamically selecting specialized experts based on stock data patterns. Experimental results demonstrate that Senti-MambaMoE outperforms LSTM-based models by 23.7% and Transformer-based models by 6.3%, highlighting its superior performance in short-term stock prediction.﻿</p>",,,,,Series (stratigraphy); State (computer science); Time series; Sentiment analysis; News analytics; Finance; Computer science; Economics; Artificial intelligence; Machine learning; Algorithm; Paleontology; Biology,,,,,,http://dx.doi.org/10.21203/rs.3.rs-6277319/v1,,10.21203/rs.3.rs-6277319/v1,,,0,006-701-602-294-537; 008-634-212-021-555; 010-383-853-311-534; 014-301-750-975-569; 017-757-904-250-742; 025-957-680-057-623; 029-316-161-965-596; 031-285-435-745-666; 039-547-517-179-176; 046-080-756-564-802; 048-787-812-918-479; 053-063-534-993-889; 053-094-537-530-97X; 058-049-123-645-012; 060-300-769-233-251; 071-423-367-050-689; 086-527-755-307-256; 089-452-908-421-259; 090-805-534-178-069; 092-230-389-714-678; 097-391-099-286-379; 104-943-960-749-41X; 105-933-274-359-013; 106-176-973-499-980; 120-142-998-515-651; 121-710-368-599-941; 121-823-142-181-244; 154-042-138-562-576; 163-873-231-587-472; 194-799-785-421-425; 195-913-058-104-530; 199-711-961-147-473; 199-833-513-451-370,0,false,,
113-003-744-658-624,Exploring the Boundaries of Financial Statement Fraud Detection with Large Language Models,2024-01-01,2024,preprint,,,Elsevier BV,,Georgia Boskou; Evrikleia Chatzipetrou; Eleftherios Tiakas; Efstathios Kirkos; Charalambos Spathis,,,,,,Financial statement; Statement (logic); Business; Computer science; Accounting; Finance; Linguistics; Philosophy; Audit,,,,,,http://dx.doi.org/10.2139/ssrn.4897041,,10.2139/ssrn.4897041,,,0,000-064-613-799-138; 004-364-724-327-82X; 010-447-540-978-418; 016-872-571-791-520; 017-037-956-347-495; 021-445-736-719-442; 025-956-696-078-141; 029-495-371-904-990; 031-100-988-302-810; 040-985-643-459-086; 043-000-135-018-065; 081-796-938-382-930; 081-801-056-645-587; 085-331-045-741-655; 088-316-419-398-518; 099-462-597-129-024; 101-834-114-740-860; 105-433-037-398-851; 106-081-386-593-970; 108-867-611-312-707; 119-772-189-397-983; 143-650-925-055-733,1,false,,
113-098-221-745-192,Exploring the Boundaries of Financial Statement Fraud Detection with Large Language Models,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Efstathios Kirkos; Georgia Boskou; Evrikleia Chatzipetrou; Eleftherios Tiakas; Charalampos Spathis,,,,,,Financial statement; Business; Statement (logic); Actuarial science; Accounting; Political science; Audit; Law,,,,,,http://dx.doi.org/10.2139/ssrn.4842962,,10.2139/ssrn.4842962,,,0,000-064-613-799-138; 004-364-724-327-82X; 009-275-434-848-173; 010-447-540-978-418; 016-872-571-791-520; 017-037-956-347-495; 021-445-736-719-442; 025-956-696-078-141; 027-018-424-921-213; 029-495-371-904-990; 040-909-985-600-620; 043-000-135-018-065; 047-594-579-848-824; 048-814-506-203-214; 081-796-938-382-930; 083-237-079-564-497; 085-331-045-741-655; 088-316-419-398-518; 106-081-386-593-970; 108-867-611-312-707; 119-772-189-397-983; 143-650-925-055-733; 178-303-722-194-518; 193-770-620-946-367,1,false,,
113-131-925-733-95X,The Flow of Investments in the LLM Space,2024-04-12,2024,book chapter,Large Language Models in Cybersecurity,,Springer Nature Switzerland,,Loïc Maréchal,"<jats:title>Abstract</jats:title>;           <jats:p>The development of <jats:italic>Large Language Models</jats:italic> (LLMs) is heavily dependent on the resources available to the teams working on them, and investments play a crucial role in determining the success of these models. The recent trend in media coverage of LLMs focuses on the investment amount raised. Microsoft’s promise to renew and increase its stake in OpenAI with a $10 billion series or Amazon’s recent new stake of $4 billion in Anthropic are examples of the significant investment that can affect the growth of LLMs. The share of funding in the <jats:italic>Artificial Intelligence</jats:italic> (AI) and <jats:italic>Machine Learning</jats:italic> (ML) sectors has increased significantly over the last year. Conversely, uncovering a trend in the founding of text analytics is challenging. The post-money valuations in the AI and ML sectors have also increased from virtually inexistent to up to 15% of the total valuations (in the private sector, two types of valuations are generally available: the one done before an investment round, the pre-money valuation, and that done after the post-money valuation. Since the latter incorporates the latest information, it is generally used to compute returns and other metrics on investment rounds). However, this increase is accompanied by significant volatility, likely due to uncertainties regarding investors’ expectations. The log distribution of investment amount in each field shows significant outliers, and the majority of investors and investees are present in the US.</jats:p>",,,129,135,Space (punctuation); Flow (mathematics); Business; Computer science; Mechanics; Physics; Operating system,,,,armasuisse,,http://dx.doi.org/10.1007/978-3-031-54827-7_14,,10.1007/978-3-031-54827-7_14,,,0,001-808-464-827-574; 025-734-534-514-728; 134-067-016-053-20X,0,false,,
113-165-126-603-679,AT-FinGPT: Financial risk prediction via an audio-text large language model,,2025,journal article,Finance Research Letters,15446123; 15446131,Elsevier BV,Netherlands,Yingnan Liu; Ningbo Bu; Zhiqiang Li; Yongmin Zhang; Zhenyu Zhao,,77,,106967,106967,Financial risk; Computer science; Business; Finance,,,,National Natural Science Foundation of China,,http://dx.doi.org/10.1016/j.frl.2025.106967,,10.1016/j.frl.2025.106967,,,0,003-872-369-534-14X; 011-592-437-185-337; 014-985-955-875-158; 028-865-332-876-105; 029-695-509-268-680; 032-915-293-233-840; 035-360-516-758-04X; 044-126-162-559-06X; 050-411-223-690-003; 052-700-036-927-582; 070-714-115-391-608; 081-535-653-965-985; 083-989-154-270-555; 084-663-017-930-12X; 094-910-748-277-61X; 102-068-013-162-56X; 112-267-801-827-643; 128-281-148-405-705; 130-346-908-504-439; 133-601-998-373-261; 149-039-817-820-575; 194-347-983-609-632; 196-395-945-746-897,12,false,,
113-196-757-089-559,Ploutos: Towards Explainable Stock Movement Prediction with Financial Large Language Model,2025-05-08,2025,conference proceedings article,Companion Proceedings of the ACM on Web Conference 2025,,ACM,,Hanshuang Tong; Jun Li; Ning Wu; Ming Gong; Dongmei Zhang; Qi Zhang,,,,490,499,Stock (firearms); Computer science; Movement (music); Language model; Finance; Artificial intelligence; Business; History; Philosophy; Archaeology; Aesthetics,,,,,,http://dx.doi.org/10.1145/3701716.3715254,,10.1145/3701716.3715254,,,0,003-721-924-935-180; 004-853-485-857-424; 019-729-659-662-050; 030-451-412-418-253; 031-514-186-700-366; 032-875-018-208-620; 042-831-309-322-798; 050-568-161-714-510; 068-599-422-160-196; 070-049-608-205-466; 074-855-170-207-206; 080-878-370-554-949; 091-075-555-728-204; 101-419-974-995-099; 123-370-469-858-361; 124-158-980-599-57X; 159-089-217-018-843,2,true,cc-by,bronze
113-599-650-856-320,LLM-Based Dynamic Multi-Agent Systems for Evaluation of Financial Trading Mechanism,2025-07-16,2025,conference proceedings article,2025 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan),,IEEE,,Edward Yu-Cheng Cheng; Hsueh-Ting Chu,,,,533,534,Mechanism (biology); Computer science; Multi-agent system; Finance; Business; Artificial intelligence; Philosophy; Epistemology,,,,,,http://dx.doi.org/10.1109/icce-taiwan66881.2025.11207858,,10.1109/icce-taiwan66881.2025.11207858,,,0,038-841-681-792-811,0,false,,
113-641-740-025-009,"D\'olares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs
  Between Spanish and English",2024-02-11,2024,preprint,arXiv (Cornell University),,,,Xiao Zhang; Ruoyu Xiang; Chenhan Yuan; Duanyu Feng; Weiguang Han; Alejandro Lopez-Lira; Xiao-Yang Liu; Sophia Ananiadou; Min Peng; Jimin Huang; Qianqian Xie,"Despite Spanish's pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic instruction tuning and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and benchmarks have been released.",,,,,Economics; Business; Political science,,,,,https://arxiv.org/abs/2402.07405,http://dx.doi.org/10.48550/arxiv.2402.07405,,10.48550/arxiv.2402.07405,,,0,,0,true,,green
113-733-939-965-098,Financial Analysis: Intelligent Financial Data Analysis System Based on LLM-RAG,2025-03-20,2025,preprint,,,MDPI AG,,Jingru Wang; Wen Ding; Xiaotong Zhu,"<jats:p>In the modern financial sector, the exponential growth of data has made efficient and accurate financial data analysis increasingly crucial. Traditional methods, such as statistical analysis and rule-based systems, often struggle to process and derive meaningful insights from complex financial information effectively. These conventional approaches face inherent limitations in handling unstructured data, capturing intricate market patterns, and adapting to rapidly evolving financial contexts, resulting in reduced accuracy and delayed decision-making processes. To address these challenges, this paper presents an intelligent financial data analysis system that integrates Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology. Our system incorporates three key components: a specialized preprocessing module for financial data standardization, an efficient vector-based storage and retrieval system, and a RAG-enhanced query processing module. Using the NASDAQ financial fundamentals dataset from 2010 to 2023, we conducted comprehensive experiments to evaluate system performance. Results demonstrate significant improvements across multiple metrics: the fully optimized configuration (gpt-3.5-turbo-1106+RAG) achieved 78.6% accuracy and 89.2% recall, surpassing the baseline model by 23 percentage points in accuracy while reducing response time by 34.8%. The system also showed enhanced efficiency in handling complex financial queries, though with a moderate increase in memory utilization. Our findings validate the effectiveness of integrating RAG technology with LLMs for financial analysis tasks and provide valuable insights for future developments in intelligent financial data processing systems.</jats:p>",,,,,Finance; Financial analysis; Business; Financial system,,,,,,http://dx.doi.org/10.20944/preprints202503.1532.v1,,10.20944/preprints202503.1532.v1,,,0,,8,true,,bronze
114-242-330-711-047,Exposing Product Bias in LLM Investment Recommendation,2025-03-11,2025,preprint,arXiv (Cornell University),,,,Yuhan Zhi; Xiaoyu Zhang; Longtian Wang; Shumin Jiang; Shiqing Ma; Xiaohong Guan; Chao Shen,"Large language models (LLMs), as a new generation of recommendation engines, possess powerful summarization and data analysis capabilities, surpassing traditional recommendation systems in both scope and performance. One promising application is investment recommendation. In this paper, we reveal a novel product bias in LLM investment recommendation, where LLMs exhibit systematic preferences for specific products. Such preferences can subtly influence user investment decisions, potentially leading to inflated valuations of products and financial bubbles, posing risks to both individual investors and market stability. To comprehensively study the product bias, we develop an automated pipeline to create a dataset of 567,000 samples across five asset classes (stocks, mutual funds, cryptocurrencies, savings, and portfolios). With this dataset, we present the bf first study on product bias in LLM investment recommendations. Our findings reveal that LLMs exhibit clear product preferences, such as certain stocks (e.g., `AAPL' from Apple and `MSFT' from Microsoft). Notably, this bias persists even after applying debiasing techniques. We urge AI researchers to take heed of the product bias in LLM investment recommendations and its implications, ensuring fairness and security in the digital space and market.",,,,,Investment (military); Product (mathematics); Business; Economics; Political science; Mathematics; Law; Geometry; Politics,,,,,https://arxiv.org/abs/2503.08750,http://dx.doi.org/10.48550/arxiv.2503.08750,,10.48550/arxiv.2503.08750,,,0,,0,true,,green
114-607-591-575-986,"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",2023-01-01,2023,preprint,arXiv (Cornell University),,,,Zhaozhuo Xu; Zirui Liu; Beidi Chen; Yuxin Tang; Jue Wang; Kaixiong Zhou; Xia Hu; Anshumali Shrivastava,"While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given the memory and power constraints of such devices, model compression methods are widely employed to reduce both the model size and inference latency, which essentially trades off model quality in return for improved efficiency. Thus, optimizing this accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware. In this paper, we introduce a new perspective to optimize this trade-off by prompting compressed models. Specifically, we first observe that for certain questions, the generation quality of a compressed LLM can be significantly improved by adding carefully designed hard prompts, though this isn't the case for all questions. Based on this observation, we propose a soft prompt learning method where we expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Our experimental analysis suggests our soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model (with a joint 4-bit quantization and 50% weight pruning compression), allowing them to match their uncompressed counterparts on popular benchmarks. Also, we demonstrate that these learned prompts can be transferred across various datasets, tasks, and compression levels. Hence with this transferability, we can stitch the soft prompt to a newly compressed model to improve the test-time accuracy in an ``in-situ'' way.",,,,,Computer science; Inference; Uncompressed video; Computer engineering; Padding; Quantization (signal processing); Process (computing); Machine learning; Artificial intelligence; Algorithm; Computer security; Video tracking; Operating system,,,,,https://arxiv.org/abs/2305.11186,http://dx.doi.org/10.48550/arxiv.2305.11186,,10.48550/arxiv.2305.11186,,,0,,0,true,other-oa,green
114-935-299-687-49X,Advancing Anomaly Detection: Non-Semantic Financial Data Encoding with Large Language Models,2025-01-01,2025,journal article,IEEE Access,21693536,Institute of Electrical and Electronics Engineers Inc.,United States,Alexander Bakumenko; Kateřina Hlaváčková‐Schindler; Claudia Plant; Nina Hubig,,,,1,1,Computer science; Anomaly detection; Encoding (memory); Natural language processing; Artificial intelligence,,,,,,,,,,,0,,0,true,"CC BY, CC BY-NC-ND",gold
115-303-350-620-436,Multimodal detection framework for financial fraud integrating LLMs and interpretable machine learning,2025-09-01,2025,journal article,Journal of Data and Information Science,2543683x; 2096157x,Walter de Gruyter GmbH,,Hui Nie; Zhao-hui Long; Ze-jun Fang; Lu-qiong Gao,"<jats:title>ABSTRACT</jats:title>;                   <jats:sec>;                     <jats:title>Purpose</jats:title>;                     <jats:p>This study aims to integrate large language models (LLMs) with interpretable machine learning methods to develop a multimodal data-driven framework for predicting corporate financial fraud, addressing the limitations of traditional approaches in long-text semantic parsing, model interpretability, and multisource data fusion, thereby providing regulatory agencies with intelligent auditing tools.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Design/methodology/approach</jats:title>;                     <jats:p>;                       Analyzing 5,304 Chinese listed firms’ annual reports (2015-2020) from the CSMAD database, this study leverages;                       <jats:italic>the Doubao</jats:italic>;                       LLMs to generate chunked summaries and 256-dimensional semantic vectors, developing textual semantic features. It integrates 19 financial indicators, 11 governance metrics, and linguistic characteristics (tone, readability) with fraud prediction models optimized through a group of Gradient Boosted Decision Tree (GBDT) algorithms. SHAP value analysis in the final model reveals the risk transmission mechanism by quantifying the marginal impacts of financial, governance, and textual features on fraud likelihood.;                     </jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Findings</jats:title>;                     <jats:p>The study found that LLMs effectively distill lengthy annual reports into semantic summaries, while GBDT algorithms (AUC &gt; 0.850) outperform the traditional Logistic Regression model in fraud detection. Multimodal fusion improved performance by 7.4%, with financial, governance, and textual features providing complementary signals. SHAP analysis revealed financial distress, governance conflicts, and narrative patterns (e.g., tone anchoring, semantic thresholds) as key fraud indicators, highlighting managerial intent in report language.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Research limitations</jats:title>;                     <jats:p>This study identifies three key limitations: 1) lack of interpretability for semantic features, 2) absence of granular fraud-type differentiation, and 3) unexplored comparative validation with other deep learning methods. Future research will address these gaps to enhance fraud detection precision and model transparency.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Practical implications</jats:title>;                     <jats:p>The developed semantic-enhanced evaluation model provides a quantitative tool for assessing listed companies’ information disclosure quality and enables practical implementation through its derivative real-time monitoring system. This advancement significantly strengthens capital market risk early warning capabilities, offering actionable insights for securities regulation.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Originality/value</jats:title>;                     <jats:p>This study presents three key innovations: 1) A novel “chunking-summarizationembedding” framework for efficient semantic compression of lengthy annual reports (30,000 words); 2) Demonstration of LLMs’ superior performance in financial text analysis, outperforming traditional methods by 19.3%; 3) A novel “language-psychology-behavior” triad model for analyzing managerial fraud motives.</jats:p>;                   </jats:sec>",10,4,291,315,Computer science; Project management; Finance; Business; Data science; Artificial intelligence; Knowledge management; Engineering management; Engineering; Systems engineering,,,,,,http://dx.doi.org/10.2478/jdis-2025-0046,,10.2478/jdis-2025-0046,,,0,000-307-511-106-357; 006-299-463-890-28X; 010-333-328-643-295; 014-716-135-672-528; 016-044-031-862-312; 016-872-571-791-520; 017-037-956-347-495; 020-526-892-816-373; 042-402-270-585-965; 072-658-198-050-962; 081-796-938-382-930; 092-760-020-498-31X; 105-433-037-398-851; 125-905-465-169-99X; 152-492-829-595-460; 153-754-452-743-331; 157-029-518-288-823; 166-962-598-231-599; 172-297-858-789-450,1,true,"CC BY, CC BY-SA, CC BY-ND, CC BY-NC, CC BY-NC-SA, CC BY-NC-ND",gold
116-249-681-560-96X,Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective,2025-03-24,2025,preprint,arXiv (Cornell University),,,,Changlun Li; Yao Shi; Yuyu Luo; Nan Tang,"Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.",,,,,Perspective (graphical); Investment fund; Investment (military); Manager of managers fund; Trust fund; Business; Sovereign wealth fund; Finance; Economics; Political science; Market economy; Law; Computer science; Artificial intelligence; Politics; Market liquidity; Incentive,,,,,https://arxiv.org/abs/2503.18313,http://dx.doi.org/10.48550/arxiv.2503.18313,,10.48550/arxiv.2503.18313,,,0,,0,true,,green
116-262-553-652-68X,FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven Question Answering Pipeline,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Kuldeep Singh; Simerjot Kaur; Charese Smiley,"Financial decision-making hinges on the analysis of relevant information embedded in the enormous volume of documents in the financial domain. To address this challenge, we developed FinQAPT, an end-to-end pipeline that streamlines the identification of relevant financial reports based on a query, extracts pertinent context, and leverages Large Language Models (LLMs) to perform downstream tasks. To evaluate the pipeline, we experimented with various techniques to optimize the performance of each module using the FinQA dataset. We introduced a novel clustering-based negative sampling technique to enhance context extraction and a novel prompting method called Dynamic N-shot Prompting to boost the numerical question-answering capabilities of LLMs. At the module level, we achieved state-of-the-art accuracy on FinQA, attaining an accuracy of 80.6%. However, at the pipeline level, we observed decreased performance due to challenges in extracting relevant context from financial reports. We conducted a detailed error analysis of each module and the end-to-end pipeline, pinpointing specific challenges that must be addressed to develop a robust solution for handling complex financial tasks.",,,266,273,Pipeline (software); Question answering; End-to-end principle; Computer science; Finance; Business; Artificial intelligence; Programming language,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698682 https://doi.org/10.1145/3677052.3698682 http://arxiv.org/pdf/2410.13959 http://arxiv.org/abs/2410.13959,http://dx.doi.org/10.1145/3677052.3698682,,10.1145/3677052.3698682,,,0,018-295-889-832-084; 020-915-582-924-757; 021-540-567-429-908; 023-056-432-629-513; 025-216-058-035-979; 036-040-869-680-843; 036-955-672-785-591; 056-542-756-996-88X; 066-547-208-006-297; 068-864-486-586-772; 110-532-379-792-714; 127-011-857-724-622; 141-602-937-615-698; 158-762-483-362-445,3,true,cc-by,hybrid
116-299-035-724-216,Ifqa-llm: intelligent intention-driven financial question-answering with large language models,2025-08-16,2025,journal article,The Journal of Supercomputing,15730484; 09208542,Springer Science and Business Media LLC,Netherlands,Fangshu Chen; Yilin Huang; Jiahui Wang; Chengcheng Yu; Xiankai Meng,,81,13,,,Computer science; Question answering; Information retrieval; Natural language processing; Language model; Artificial intelligence; Terminology; Precision and recall; Embedding; Benchmark (surveying); Sentiment analysis; Matching (statistics); Linguistics; Philosophy; Statistics; Mathematics; Geodesy; Geography,,,,Shanghai Sailing Program; National Natural Science Foundation of China; Natural Science Foundation of Shanghai; Natural Science Foundation of Shanghai,,http://dx.doi.org/10.1007/s11227-025-07726-5,,10.1007/s11227-025-07726-5,,,0,008-004-219-989-474; 009-124-640-402-297; 040-411-049-881-118; 052-588-124-304-716; 069-429-085-425-134; 069-892-285-628-897; 076-924-363-528-486; 079-387-272-408-778; 079-641-106-346-795; 082-078-167-558-298; 090-763-953-826-662; 092-167-546-813-014; 108-289-329-008-11X; 109-461-415-537-446; 120-492-299-266-844; 123-395-474-455-98X; 125-780-172-930-590; 130-841-530-966-217; 133-196-540-202-544; 138-530-644-574-900; 140-595-640-148-727; 158-762-483-362-445; 181-516-044-590-542; 184-476-644-413-289; 190-323-297-450-850,0,false,,
116-301-128-866-15X,"Agentic AI for End-to-End Investment Banking Automation: A No-Code Approach Using GPT, Zapier, and Airtable",2025-01-01,2025,preprint,,,Elsevier BV,,Nithin Kumar Racharla,,,,,,End-to-end principle; End user; Automation; Business; Code (set theory); End milling; Investment (military); Computer science; Accounting; Engineering; World Wide Web; Programming language; Political science; Artificial intelligence; Mechanical engineering; Set (abstract data type); Politics; Law; Machining,,,,,,http://dx.doi.org/10.2139/ssrn.5263449,,10.2139/ssrn.5263449,,,0,,0,false,,
116-624-847-097-939,LLM-RAG for Financial Question Answering: A Case Study from SET50,2025-02-18,2025,conference proceedings article,2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),,IEEE,,Naphatta Chinaksorn; Dittaya Wanvarie,,,,952,957,Question answering; Computer science; Business; Finance; Natural language processing,,,,,,http://dx.doi.org/10.1109/icaiic64266.2025.10920730,,10.1109/icaiic64266.2025.10920730,,,0,015-753-494-860-765; 016-184-518-725-867; 032-475-778-349-60X; 040-085-728-372-12X; 062-342-667-327-87X; 063-214-565-838-898; 065-858-090-699-594; 068-042-879-431-736; 069-883-292-797-811; 078-639-724-536-677; 118-151-319-317-438; 118-372-874-328-32X; 119-177-392-642-351; 125-345-953-424-950; 139-567-738-646-94X; 144-250-083-267-943; 145-860-830-890-726; 162-991-371-806-877; 175-150-635-618-884; 192-635-284-637-072; 192-806-908-794-847,1,false,,
117-008-416-040-890,Performance Trade-offs of Watermarking Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Anirudh Ajith; Sameer Singh; Danish Pruthi,"Amidst growing concerns of large language models (LLMs) being misused for generating misinformation or completing homework assignments, watermarking has emerged as an effective solution for distinguishing human-written and LLM-generated text. A prominent watermarking strategy is to embed a signal into generated text by upsampling a (pseudorandomly-chosen) subset of tokens at every generation step. Although this signal is imperceptible to a human reader, it is detectable through statistical testing. However, implanting such signals alters the model's output distribution and can have unintended effects when watermarked LLMs are used for downstream applications. In this work, we evaluate the performance of watermarked LLMs on a diverse suite of tasks, including text classification, textual entailment, reasoning, question answering, translation, summarization, and language modeling. We find that watermarking has negligible impact on the performance of tasks posed as k-class classification problems in the average case. However, the accuracy can plummet to that of a random classifier for some scenarios (that occur with non-negligible probability). Tasks that are cast as multiple-choice questions and short-form generation are surprisingly unaffected by watermarking. For long-form generation tasks, including summarization and translation, we see a drop of 15-20% in the performance due to watermarking. Our findings highlight the trade-offs that users should be cognizant of when using watermarked models, and point to cases where future research could improve existing trade-offs.",,,,,Digital watermarking; Computer science; Automatic summarization; Language model; Classifier (UML); Artificial intelligence; Natural language processing; Image (mathematics),,,,,https://arxiv.org/abs/2311.09816,http://dx.doi.org/10.48550/arxiv.2311.09816,,10.48550/arxiv.2311.09816,,,0,,0,true,cc-by,green
117-473-019-840-02X,Enhancing LLM Trading Performance with Fact-Subjectivity Aware Reasoning,2024-10-16,2024,preprint,arXiv (Cornell University),,,,Qian Wang; Yuchen Gao; Zhenheng Tang; Bingqiao Luo; Nuo Chen; Bingsheng He,"While many studies prove more advanced LLMs perform better on tasks such as math and coding, we notice that in cryptocurrency trading, stronger LLMs work worse than weaker LLMs often. To study how this counter-intuitive phenomenon occurs, we examine the LLM reasoning processes on making trading decisions. We find that separating the reasoning process into factual and subjective components can lead to higher profits. Building on this insight, we introduce a multi-agent framework, FS-ReasoningAgent, which enables LLMs to recognize and learn from both factual and subjective reasoning. Extensive experiments demonstrate that this framework enhances LLM trading performance in cryptocurrency markets. Additionally, an ablation study reveals that relying on subjective news tends to generate higher returns in bull markets, whereas focusing on factual information yields better results in bear markets. Our code and data are available at \url{https://anonymous.4open.science/r/FS-ReasoningAgent-B55F/}.",,,,,Subjectivity; Computer science; Artificial intelligence; Epistemology; Philosophy,,,,,https://arxiv.org/abs/2410.12464,http://dx.doi.org/10.48550/arxiv.2410.12464,,10.48550/arxiv.2410.12464,,,0,,0,true,,green
118-323-941-724-525,LLM-Driven Active Listwise Tournaments for Portfolio Selection in Large Asset Universes,,2025,preprint,,,Elsevier BV,,Kamer Ali Yuksel; Hassan Sawaf,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5764903,,10.2139/ssrn.5764903,,,0,,0,false,,
118-463-736-976-082,What Do Firms Think About Inflation? LLM Signals from Earnings Calls and Their Real & Financial Effects,2025-01-01,2025,preprint,,,Elsevier BV,,Leo Fratu,,,,,,Earnings; Inflation (cosmology); Economics; Monetary economics; Finance; Business; Theoretical physics; Physics,,,,,,http://dx.doi.org/10.2139/ssrn.5385782,,10.2139/ssrn.5385782,,,0,,0,false,,
118-512-497-675-204,Financial News Summarization: Can Extractive Methods Still Offer a True Alternative to LLMs?,2025-10-24,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Nicolas Reche; Elvys Linhares Pontes; Juan-Manuel Torres-Moreno,,,,169,180,Automatic summarization; Computer science; Finance; Information retrieval; Business,,,,,,http://dx.doi.org/10.1007/978-3-032-09037-9_14,,10.1007/978-3-032-09037-9_14,,,0,016-887-102-988-613; 018-866-913-622-957; 038-595-034-988-091; 044-480-088-187-115; 060-885-885-954-829; 073-018-198-431-408; 097-923-900-268-69X; 133-419-572-269-204; 193-457-519-586-335,0,false,,
118-663-469-213-879,Sensitivity Analysis on Transferred Neural Architectures of BERT and GPT-2 for Financial Sentiment Analysis,2022-01-01,2022,preprint,arXiv (Cornell University),,,,Tracy Qian; Andy Xie; Camille Bruckmann,"The explosion in novel NLP word embedding and deep learning techniques has induced significant endeavors into potential applications. One of these directions is in the financial sector. Although there is a lot of work done in state-of-the-art models like GPT and BERT, there are relatively few works on how well these methods perform through fine-tuning after being pre-trained, as well as info on how sensitive their parameters are. We investigate the performance and sensitivity of transferred neural architectures from pre-trained GPT-2 and BERT models. We test the fine-tuning performance based on freezing transformer layers, batch size, and learning rate. We find the parameters of BERT are hypersensitive to stochasticity in fine-tuning and that GPT-2 is more stable in such practice. It is also clear that the earlier layers of GPT-2 and BERT contain essential word pattern information that should be maintained.",,,,,Transformer; Computer science; Embedding; Artificial neural network; Artificial intelligence; Sensitivity (control systems); Deep learning; Electronic engineering; Engineering; Electrical engineering; Voltage,,,,,https://arxiv.org/abs/2207.03037,http://dx.doi.org/10.48550/arxiv.2207.03037,,10.48550/arxiv.2207.03037,,,0,,1,true,cc-by,green
118-731-588-355-142,LLMs are not weird: Comparing AI and human financial decision-making,,,journal article,Journal of Behavioral and Experimental Economics,22148043; 22148051,Elsevier BV,United States,Orhan Erdem; Ragavi Pobbathi Ashok,,120,,102505,,,,,,,,http://dx.doi.org/10.1016/j.socec.2025.102505,,10.1016/j.socec.2025.102505,,,0,014-337-358-220-115; 015-701-096-256-648; 022-237-779-280-353; 023-323-051-017-701; 028-061-530-920-468; 044-201-891-512-664; 064-620-360-984-293; 072-750-965-244-728; 090-959-336-626-715; 094-159-592-916-885; 102-478-571-889-95X; 104-027-931-069-462; 115-897-343-271-689; 118-148-820-968-886; 125-952-309-625-035; 126-251-983-254-859; 129-805-055-243-832; 135-067-714-451-092; 137-949-327-802-777; 149-850-620-241-206; 176-983-397-617-220,0,false,,
119-082-386-564-711,Addressing investor concerns: a Chinese financial question-answering benchmark with LLM-based evaluation,2025-12-18,2025,journal article,EPJ Data Science,21931127,Springer Science and Business Media LLC,United States,Yujian Gan; Yiyi Tao; Jiawang Mo; Xianzheng Huang; Yiwen Li; Kexin Wang; Yi Cai; Lu Liang; Shuzhen Xiong; Qi Ke; Hua Zheng; Xiaochun Hu,,,,,,,,,,"Supported by the Guangxi Key Laboratory of Seaward Economic Intelligent System Analysis and Decision-making.; Supported by the Guangxi Key Laboratory of Seaward Economic Intelligent System Analysis and Decision-making.; Guangxi Key Laboratory of Big Data in Finance and Economics; Supported by the Exploration and Practice of the Co-construction Mode of China-ASEAN Digital Economy Industry College by Universities, Governments, Industries and Enterprises.; Supported by the Guangxi Key Laboratory of Big Data in Finance and Economics. Supported by the Standard Electronic License for the ""One Map"" of Smart Market Supervision in Guangxi.",,http://dx.doi.org/10.1140/epjds/s13688-025-00601-6,,10.1140/epjds/s13688-025-00601-6,,,0,004-075-119-410-667; 004-853-485-857-424; 009-136-142-682-140; 018-866-913-622-957; 020-915-582-924-757; 031-840-568-192-129; 050-733-119-233-197; 056-542-756-996-88X; 061-793-140-748-957; 071-423-367-050-689; 081-776-456-098-496; 085-358-813-436-611; 092-167-546-813-014; 096-082-868-445-261; 096-372-442-893-804; 108-250-898-234-813; 138-729-962-867-512; 159-212-560-453-683; 163-979-407-953-145; 195-706-909-301-830; 197-673-280-254-524,0,true,"CC BY, CC BY-NC-ND",gold
119-857-839-744-645,"Stock Prediction Based on Technical Indicators and News Using LLAMA, Exploring LLM Possibilities in Financial Context",2025-08-28,2025,conference proceedings article,2025 International Conference on Information Management and Technology (ICIMTech),,IEEE,,Emmanuel Brandon Hamdi; Riyanto Jayadi; Tanty Oktavia,,,,704,709,,,,,,,http://dx.doi.org/10.1109/icimtech67074.2025.11265014,,10.1109/icimtech67074.2025.11265014,,,0,004-260-551-096-704; 011-958-235-543-718; 012-768-312-177-483; 036-585-041-842-638; 047-729-043-873-226; 052-484-787-569-890; 059-605-800-305-172; 068-916-671-812-286; 078-022-740-826-447; 085-500-996-784-700; 087-074-689-911-494; 101-520-573-807-639; 125-531-153-078-569; 126-184-180-797-086; 128-896-448-806-456; 128-950-131-708-238; 140-017-872-454-665; 144-859-833-568-738; 158-143-658-956-729; 173-965-257-382-206; 176-560-905-811-069; 179-611-100-608-975; 193-974-543-792-089,0,false,,
119-911-219-407-984,A Study on the Dual Impact of Generative Prediction (GPT)-Based AI on the Quality of Corporate Financial Disclosure,2025-11-11,2025,journal article,"Frontiers in Business, Economics and Management",30801753; 2766824x,Darcy & Roy Press Co. Ltd.,,Zuoshi Zhang,"<jats:p>Against the backdrop of accelerating digital transformation, GPT-based generative AI technologies are gradually penetrating the entire corporate financial disclosure process, exerting a significant dual impact on disclosure quality. Drawing on information asymmetry theory and principal-agent theory, combined with KPMG's global research data and case studies such as Amazon and AllHere, this paper systematically analyzes the positive impact and potential risks of generative AI on the quality of financial disclosure. The study finds that generative AI can reduce disclosure redundancy through automated processing, compressing MD&amp;A report summaries to 25% of the original while retaining core information, while also improving forecast accuracy and compliance efficiency. However, this also presents risks such as ""AI whitewashing,"" data fabrication, and algorithmic black box manipulation. For example, the US AI startup AllHere overstated its revenue by nearly 700 times by fabricating AI-related financial data. The study further suggests the need to establish a coordinated mechanism across three dimensions: optimizing corporate governance, upgrading regulatory technology, and managing model security. The conclusions indicate that the impact of generative AI on disclosure quality is not one-way; its ultimate effect depends on the alignment between technical application specifications and risk prevention and control systems. This finding provides empirical evidence for companies to rationally utilize AI technology and for regulators to improve governance rules.</jats:p>",21,2,8,11,,,,,,,http://dx.doi.org/10.54097/91vzx479,,10.54097/91vzx479,,,0,,0,false,,
120-027-740-452-784,Do I Desire Chatbots to be like Humans? Exploring Factors for Adoption of Chatbots for Financial Services,2021-01-01,2021,journal article,Journal of International Technology and Information Management,19416679; 15435962,"John M. Pfau Library, California State University San Bernardino",,Moses Sugumar; Shalini Chandra,"AI-powered chatbots are gaining traction across various industries, especially in the financial sector. Despite these implementations, chatbot adoption and usage among consumers is still low. Grounding on the unified theory of acceptance and use of technology 2 (UTAUT2) model and the Belief Desire Intentions (BDI) model, this study explores factors influencing the adoption of chatbots for financial sectors by emphasizing on the role of user desires in addition to human beliefs. Explicitly, the research hypothesizes the role of the humanness in chatbots influencing consumer adoption in the financial services sector. The suggested research model was tested via a sample of possible adopters from India, the USA, and Singapore. Results highlight the key role of consumer desires to make artificial machines indistinguishable from human beings. Implications for research and practice are also presented.",30,3,38,77,Chatbot; Financial services; Implementation; Marketing; Technology acceptance model; Financial sector; Business; Knowledge management; Psychology; Computer science; Finance; World Wide Web; Human–computer interaction; Usability; Programming language,chatbots; customer service; humanness; technology adoption; UTAUT2; BDI model; financial services,,,,https://scholarworks.lib.csusb.edu/cgi/viewcontent.cgi?article=1501&context=jitim https://doi.org/10.58729/1941-6679.1501,http://dx.doi.org/10.58729/1941-6679.1501,,10.58729/1941-6679.1501,,,0,,26,true,,bronze
120-126-457-743-715,"Financial Sentiment Analysis on News and Reports Using Large Language
  Models and FinBERT",2024-10-02,2024,preprint,arXiv (Cornell University),,,,Yanxin Shen; Pulin Kirin Zhang,"Financial sentiment analysis (FSA) is crucial for evaluating market sentiment and making well-informed financial decisions. The advent of large language models (LLMs) such as BERT and its financial variant, FinBERT, has notably enhanced sentiment analysis capabilities. This paper investigates the application of LLMs and FinBERT for FSA, comparing their performance on news articles, financial reports and company announcements. The study emphasizes the advantages of prompt engineering with zero-shot and few-shot strategy to improve sentiment classification accuracy. Experimental results indicate that GPT-4o, with few-shot examples of financial texts, can be as competent as a well fine-tuned FinBERT in this specialized field.",,,,,Sentiment analysis; Business; Economics; Finance; Natural language processing; Computer science,,,,,https://arxiv.org/abs/2410.01987,http://dx.doi.org/10.48550/arxiv.2410.01987,,10.48550/arxiv.2410.01987,,,0,,0,true,,green
120-370-474-946-201,Large Language Models-Based Robots for Enterprise Financial Statement Auditing,2024-11-01,2024,conference proceedings article,"2024 6th International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",,IEEE,,Yuanqi Yang; Haichao Zhang; Weidong Shen; Haoxuan Chen; Yi Cao; Liangyu Zhao,,,,249,258,Financial statement; Audit; Statement (logic); Computer science; Robot; Accounting; Programming language; Artificial intelligence; Business; Linguistics; Philosophy,,,,,,http://dx.doi.org/10.1109/mlbdbi63974.2024.10824015,,10.1109/mlbdbi63974.2024.10824015,,,0,007-534-623-654-192; 011-856-960-141-570; 012-839-512-258-689; 016-340-250-946-58X; 025-639-984-233-151; 028-538-738-096-696; 034-978-238-098-077; 057-658-369-199-478; 060-851-464-957-903; 093-847-712-827-40X; 098-328-166-773-244; 136-766-311-960-457; 144-638-940-242-101,0,false,,
121-086-416-524-674,"Towards Automated Solution Recipe Generation for Industrial Asset
  Management with LLM",2024-07-25,2024,preprint,arXiv (Cornell University),,,,Nianjun Zhou; Dhaval Patel; Shuxin Lin; Fearghal O'Donncha,"This study introduces a novel approach to Industrial Asset Management (IAM) by incorporating Conditional-Based Management (CBM) principles with the latest advancements in Large Language Models (LLMs). Our research introduces an automated model-building process, traditionally reliant on intensive collaboration between data scientists and domain experts. We present two primary innovations: a taxonomy-guided prompting generation that facilitates the automatic creation of AI solution recipes and a set of LLM pipelines designed to produce a solution recipe containing a set of artifacts composed of documents, sample data, and models for IAM. These pipelines, guided by standardized principles, enable the generation of initial solution templates for heterogeneous asset classes without direct human input, reducing reliance on extensive domain knowledge and enhancing automation. We evaluate our methodology by assessing asset health and sustainability across a spectrum of ten asset classes. Our findings illustrate the potential of LLMs and taxonomy-based LLM prompting pipelines in transforming asset management, offering a blueprint for subsequent research and development initiatives to be integrated into a rapid client solution.",,,,,Recipe; Asset management; Asset (computer security); Business; Computer science; Manufacturing engineering; Engineering; Computer security; Finance; History; Archaeology,,,,,https://arxiv.org/abs/2407.18992,http://dx.doi.org/10.48550/arxiv.2407.18992,,10.48550/arxiv.2407.18992,,,0,,0,true,,green
121-834-976-428-390,Structured Financial QA with LLMs: Fine-Tuning vs. Code-Augmented Retrieval,2025-09-17,2025,conference proceedings article,2025 10th International Conference on Computer Science and Engineering (UBMK),,IEEE,,Alperen Çağlayan; Saliha Nur Gökçe; Değer Ayata,,,,539,544,Code (set theory); Computer science; Programming language; Set (abstract data type),,,,,,http://dx.doi.org/10.1109/ubmk67458.2025.11207079,,10.1109/ubmk67458.2025.11207079,,,0,021-579-060-903-875; 046-788-654-513-653; 060-472-856-762-427; 065-903-146-233-137; 111-712-237-539-099; 116-789-211-874-339; 125-699-768-474-583; 154-710-286-330-84X; 175-591-547-364-275,0,false,,
122-025-987-859-336,FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Ziao Wang; Yuhang Li; Junda Wu; Jaehyeon Soon; Xiaofeng Zhang,"In this paper, we propose FinVis-GPT, a novel multimodal large language model (LLM) specifically designed for financial chart analysis. By leveraging the power of LLMs and incorporating instruction tuning and multimodal capabilities, FinVis-GPT is capable of interpreting financial charts and providing valuable analysis. To train FinVis-GPT, a financial task oriented dataset was generated for pre-training alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions. We evaluate the model performance via several case studies due to the time limit, and the promising results demonstrated that FinVis-GPT is superior in various financial chart related tasks, including generating descriptions, answering questions and predicting future market trends, surpassing existing state-of-the-art multimodal LLMs. The proposed FinVis-GPT serves as a pioneering effort in utilizing multimodal LLMs in the finance domain and our generated dataset will be release for public use in the near future to speedup related research.",,,,,Chart; Computer science; Task (project management); Finance; Speedup; Financial market; Domain (mathematical analysis); Artificial intelligence; Business; Economics; Management; Statistics; Mathematics; Operating system; Mathematical analysis,,,,,https://arxiv.org/abs/2308.01430,http://dx.doi.org/10.48550/arxiv.2308.01430,,10.48550/arxiv.2308.01430,,,0,,0,true,cc-by,green
122-307-530-013-280,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Trade",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7902612,https://guides.library.harvard.edu/c.php?g=1082940&p=7902612,,,3088791305,,0,,0,false,,
122-416-939-793-588,Self-Adapting Financial Sentiment Oracles: LLM-Agent Swarms for Real-Time Market Prediction,2025-08-20,2025,journal article,Journal of Information Systems Engineering and Management,24684376,Science Research Society,,null Kiran Purushotham,"<jats:p>Self-Adapting Financial Sentiment Oracles represents a revolutionary advancement in financial market prediction technology, leveraging biologically-inspired swarm intelligence principles to create unprecedented capabilities in real-time sentiment processing. The framework introduces a distributed network of specialized Large Language Model agents, each optimized for extracting sentiment signals from distinct financial data sources, including news articles, social media platforms, and regulatory filings. Through sophisticated attention-based consensus mechanisms, these agents collaborate to generate integrated market predictions that improve traditional quantitative models to a great extent. The self-adapting architecture of the system employed the algorithm of reinforcement to dynamically adjust agent weight and data source priority based on market conditions, which ensures optimal performance in diverse financial environments. Major innovations include sub-miles and processing delays, multi-source emotion fusion, and comprehensive audit trails that meet regulatory compliance requirements. Framework-distributed processing displays notable scalability through processing architecture that maintains high accuracy by obtaining important computational efficiency benefits. Applications expand algorithm trading, portfolio management, risk evaluation, and regulatory compliance, and benefit from a unique combination of specific intelligence and adaptive learning abilities of each domain. The herd-based design enables improvement in continuous performance through collective teaching mechanisms that overcome individual agent abilities.</jats:p>",10,58s,753,760,Financial market; Computer science; Artificial intelligence; Finance; Business,,,,,,http://dx.doi.org/10.52783/jisem.v10i58s.12717,,10.52783/jisem.v10i58s.12717,,,0,,0,true,,bronze
122-801-776-952-361,Agentic LLMs for Analyst-Style Financial Insights: An LLM Pipeline for Persuasive Financial Analysis,,2025,conference proceedings article,Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Gaurangi Sinha; Rajarajeswari Palacharla; Manoj Balaji Jagadeeshan,,,,322,327,,,,,,,http://dx.doi.org/10.18653/v1/2025.finnlp-2.25,,10.18653/v1/2025.finnlp-2.25,,,0,,0,false,,
122-869-277-731-75X,"MarketGPT: Developing a Pre-trained transformer (GPT) for Modeling
  Financial Time Series",2024-11-25,2024,preprint,arXiv (Cornell University),,,,Aaron Wheeler; Jeffrey D. Varner,"This work presents a generative pre-trained transformer (GPT) designed for modeling financial time series. The GPT functions as an order generation engine within a discrete event simulator, enabling realistic replication of limit order book dynamics. Our model leverages recent advancements in large language models to produce long sequences of order messages in a steaming manner. Our results demonstrate that the model successfully reproduces key features of order flow data, even when the initial order flow prompt is no longer present within the model's context window. Moreover, evaluations reveal that the model captures several statistical properties, or 'stylized facts', characteristic of real financial markets and broader macro-scale data distributions. Collectively, this work marks a significant step toward creating high-fidelity, interactive market simulations.",,,,,Transformer; Series (stratigraphy); Computer science; Finance; Economics; Engineering; Electrical engineering; Voltage; Geology; Paleontology,,,,,https://arxiv.org/abs/2411.16585,http://dx.doi.org/10.48550/arxiv.2411.16585,,10.48550/arxiv.2411.16585,,,0,,0,true,,green
123-001-643-115-341,Construction of Financial Conversational Assistant via Large Language Models,2023-11-24,2023,conference proceedings article,"Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management",,ACM,,Zhenhao Dong; Hongchao Ma; Mengjun Ni; Hang Jiang; Yuefeng Lin; Zhongchen Miao,"With the continuous development of AI, more and more people are experiencing ""technology changes life"". In artificial intelligence (AI) applications, intelligent conversation assistants play an important role, such as ChatGPT. Practitioners in the financial field have to deal with a huge amount of information every day, such as product price changes, market news, so they urgently need a reliable intelligent assistant to help them solve problems. We build a financial intelligent dialog assistant based on large language models (LLMs), which provides real-time retrieval, market analysis and many other functions. Multiple experiments prove the usability of our assistant.",,,139,144,Dialog box; Conversation; Usability; Computer science; Product (mathematics); Field (mathematics); Financial market; Dialog system; Knowledge management; Human–computer interaction; Artificial intelligence; World Wide Web; Finance; Linguistics; Business; Philosophy; Geometry; Mathematics; Pure mathematics,,,,,,http://dx.doi.org/10.1145/3656766.3656791,,10.1145/3656766.3656791,,,0,044-381-867-922-074; 065-518-826-182-926; 072-900-763-568-602; 101-429-512-780-519,0,false,,
123-552-074-369-296,<span>Artificial Intelligence and Financial Counseling: Do Institutions of Higher Education Embed Chatbots in their Financial Wellness Websites?</span>,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,ZW Taylor; Andrew Marx; Dez Nixon; Sara Ray; Richard Simonds; Mallorie Smith; Sophie Glass; Joseph Mesa; Jenny Becker; Aly Blakeney; Tim Cerebe; Amanda Miller; Justin Enlow; Steven Hughes; Helen Colby; Tristia Kayser; Becky Smith; Brandan Wheeler,,,,,,Span (engineering); Life span; Finance; Psychology; Business; Gerontology; Medicine; Engineering; Civil engineering,,,,,,http://dx.doi.org/10.2139/ssrn.5188383,,10.2139/ssrn.5188383,,,0,,0,false,,
123-875-081-037-51X,Navigating GPT-4 and BERT: A Dual Perspective on Financial and Political Sentiment Analysis,2023-12-31,2023,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Akash Ghosh; Debstuti Biswas; Rahul Sarkar; Advait Pillai; Jatin Chopra,"<jats:p>Abstract: This research delves into the dynamic capabilities of large language models (LLMs) by combining insights from two distinct studies. The first study, ""Linking Microblogging Sentiments to Stock Price Movement: An Application of GPT-4,"" explores the efficacy of the GPT-4 Language Learning Model (LLM) compared to BERT in modeling same-day daily stock price movements for Apple and Tesla in 2017. The study leverages sentiment analysis of microblogging messages from the Stocktwits platform and employs a novel method for prompt engineering, emphasizing the contextual abilities of GPT-4. Logistic regression is utilized to evaluate the correspondence between extracted message contents and stock price movements, revealing GPT-4's substantial accuracy, outperforming BERT in five out of six months. However, practical considerations, including deployment costs and the need for fine-tuning prompts, are also acknowledged. The second study, ""Do We Still Need BERT in the Age of GPT? Comparing the Benefits of Domain-Adaptation and In-Context-Learning Approaches to Using LLMs for Political Science Research,"" investigates the choices researchers face when employing LLMs in political science tasks. The study establishes benchmarks for various natural language processing (NLP) tasks within political science and compares two common approaches: domain-adapting smaller LLMs like BERT with unsupervised pre-training and supervised fine-tuning and querying larger LLMs like GPT-3 without additional training. Preliminary results suggest that, when labeled data is available, a finetuning-focused approach remains superior for text classification. By synthesizing these studies, this research contributes to the broader understanding of LLMs' capabilities and their applicability in diverse domains. It emphasizes the significance of prompt engineering in unlocking the contextual abilities of modern LLMs, providing insights into financial sentiment analysis and political text classification. The findings underscore the nuanced choices researchers must make in selecting appropriate LLMs, adapting them to specific domains, and designing effective prompts for optimal performance in different research contexts.</jats:p>",11,12,1577,1583,Microblogging; Computer science; Social media; Artificial intelligence; Context (archaeology); Politics; Machine learning; Data science; Political science; World Wide Web; Geography; Archaeology; Law,GPT-4; BERT; Supervised Fine-Tuning; LLM; Benchmarking,,,,,http://dx.doi.org/10.22214/ijraset.2023.57687,,10.22214/ijraset.2023.57687,,,0,,0,true,,gold
123-906-633-448-288,Can Large Language Models beat wall street? Evaluating GPT-4's impact on financial decision-making with MarketSenseAI,2024-12-08,2024,journal article,Neural Computing and Applications,09410643; 14333058,Springer Science and Business Media LLC,Germany,George Fatouros; Kostas Metaxas; John Soldatos; Dimosthenis Kyriazis,"This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability, and acceptance. Through empirical testing on the competitive S&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10–30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies.",37,30,24893,24918,Computational Science and Engineering; Computer science; Beat (acoustics); Machine learning; Acoustics; Physics,,,,HORIZON EUROPE Framework Programme,https://link.springer.com/content/pdf/10.1007/s00521-024-10613-4.pdf https://doi.org/10.1007/s00521-024-10613-4,http://dx.doi.org/10.1007/s00521-024-10613-4,,10.1007/s00521-024-10613-4,,,0,011-790-420-220-850; 024-481-362-055-895; 024-679-492-702-994; 033-543-404-081-387; 044-051-439-872-291; 045-368-992-096-330; 052-062-088-505-265; 062-649-409-354-011; 067-676-432-076-959; 080-546-368-819-636; 087-911-264-175-79X; 095-761-242-666-669; 104-878-949-422-522; 130-016-188-886-069; 137-925-447-012-933; 138-476-780-260-098; 151-121-690-945-198; 158-463-725-232-492; 163-702-158-124-810; 177-296-169-623-620; 179-785-243-349-704; 181-432-462-344-434; 184-148-490-841-412; 190-050-627-029-962; 199-307-397-511-013,10,true,cc-by-nc-nd,hybrid
124-370-160-174-582,The Impacts of Comparison Options and Explanation Styles on Consumer Experience in AI Financial Chatbot Recommendations,2025-02-28,2025,journal article,The e-Business Studies,12299936; 24661716,Global e-Business Association,,Eun-Bin Han; Sang-Hee Han; Jun-Ho Cho,,26,1,3,23,Chatbot; Economics; Finance; Business; Actuarial science; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.20462/tebs.2025.2.26.1.3,,10.20462/tebs.2025.2.26.1.3,,,0,,0,false,,
124-610-064-767-804,Towards reducing hallucination in extracting information from financial reports using Large Language Models,2023-10-25,2023,conference proceedings article,The Third International Conference on Artificial Intelligence and Machine Learning Systems,,ACM,,Bhaskarjit Sarmah; Dhagash Mehta; Stefano Pasquali; Tianjie Zhu,"For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy—transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",,,1,5,Computer science; Metadata; Earnings; Reading (process); Process (computing); Information extraction; Scalability; Natural language processing; Artificial intelligence; Information retrieval; Machine learning; Character (mathematics); Investment (military); Finance; Data science; World Wide Web; Linguistics; Business; Database; Philosophy; Geometry; Mathematics; Politics; Political science; Law; Operating system,,,,,http://arxiv.org/pdf/2310.10760 http://arxiv.org/abs/2310.10760,http://dx.doi.org/10.1145/3639856.3639895,,10.1145/3639856.3639895,,,1,015-088-465-470-840; 140-416-220-363-723; 148-381-089-556-776,9,true,,green
125-046-436-779-446,"""Give Me BF16 or Give Me Death""? Accuracy-Performance Trade-Offs in LLM
  Quantization",2024-11-04,2024,preprint,arXiv (Cornell University),,,,Eldar Kurtic; Alexandre Marques; Shubhra Pandit; Mark Kurtz; Dan Alistarh,"Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the ""best"" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous ""continuous batching"" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.",,,,,Quantization (signal processing); Psychology; Computer science; Algorithm,,,,,https://arxiv.org/abs/2411.02355,http://dx.doi.org/10.48550/arxiv.2411.02355,,10.48550/arxiv.2411.02355,,,0,,1,true,,green
125-092-380-382-753,"The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs
  in LLM Generations",2024-11-26,2024,preprint,arXiv (Cornell University),,,,Theodora Worledge; Tatsunori Hashimoto; Carlos Guestrin,"Across all fields of academic study, experts cite their sources when sharing information. While large language models (LLMs) excel at synthesizing information, they do not provide reliable citation to sources, making it difficult to trace and verify the origins of the information they present. In contrast, search engines make sources readily accessible to users and place the burden of synthesizing information on the user. Through a survey, we find that users prefer search engines over LLMs for high-stakes queries, where concerns regarding information provenance outweigh the perceived utility of LLM responses. To examine the interplay between verifiability and utility of information-sharing tools, we introduce the extractive-abstractive spectrum, in which search engines and LLMs are extreme endpoints encapsulating multiple unexplored intermediate operating points. Search engines are extractive because they respond to queries with snippets of sources with links (citations) to the original webpages. LLMs are abstractive because they address queries with answers that synthesize and logically transform relevant information from training and in-context sources without reliable citation. We define five operating points that span the extractive-abstractive spectrum and conduct human evaluations on seven systems across four diverse query distributions that reflect real-world QA settings: web search, language simplification, multi-step reasoning, and medical advice. As outputs become more abstractive, we find that perceived utility improves by as much as 200%, while the proportion of properly cited sentences decreases by as much as 50% and users take up to 3 times as long to verify cited information. Our findings recommend distinct operating points for domain-specific LLM systems and our failure analysis informs approaches to high-utility LLM systems that empower users to verify information.",,,,,Spectrum (functional analysis); Economics; Computer science; Physics; Quantum mechanics,,,,,https://arxiv.org/abs/2411.17375,http://dx.doi.org/10.48550/arxiv.2411.17375,,10.48550/arxiv.2411.17375,,,0,,1,true,,green
125-271-694-643-374,Analysis of the Effects of AI Explanation Methods and Users' Risk-Taking Levels on Investment Decision-Making in a Chatbot,2025-04-30,2025,journal article,Journal of the Ergonomics Society of Korea,12291684; 20938462,The Ergonomics Society of Korea,,Hyewon Yoon; Gyewon Jeon; Sangwon Lee,,44,2,239,254,Chatbot; Investment (military); Psychology; Computer science; Risk analysis (engineering); Engineering; Artificial intelligence; Medicine; Political science; Politics; Law,,,,,,http://dx.doi.org/10.5143/jesk.2025.44.2.239,,10.5143/jesk.2025.44.2.239,,,0,,0,false,,
125-455-548-973-304,Optimizing business loan and credit experiences through AI-Powered Chatbot integration in financial services,2024-08-15,2024,journal article,Finance & Accounting Research Journal,27086348; 2708633x,Fair East Publishers,,null Emmanuel Igba; null Adenike Folashade Adeyemi; null Joy Onma Enyejo; null Amina Catherine Ijiga; null Grace Amidu; null George Addo,"<jats:p>Artificial Intelligence (AI) chatbots are revolutionizing customer interactions in the financial services sector, particularly in the realm of business loans and credit experiences. This review paper examines the role of AI chatbots in enhancing these crucial financial processes. It begins with an overview of the traditional landscape of business loans and credit experiences, highlighting existing challenges and the imperative for seamless customer interactions. The paper defines AI chatbots and explores their functionality, adoption trends, and benefits within financial institutions. Special attention is given to how AI chatbots personalize customer interactions, streamline loan application processes, and enhance efficiency and accuracy in credit assessments. Despite their transformative potential, challenges such as privacy concerns, integration complexities, and managing customer trust are discussed. The paper concludes by outlining future directions, including emerging trends and potential advancements in AI chatbots, underscoring their evolving role in shaping superior business loan and credit experiences.; Keywords:  AI Chatbots, Ethical Considerations, Algorithmic Bias, Transparency, Financial Services.</jats:p>",6,8,1436,1458,Chatbot; Financial services; Loan; Transparency (behavior); Realm; Transformative learning; Business model; Business; Finance; Computer science; Marketing; Artificial intelligence; Computer security; Psychology; Pedagogy; Political science; Law,,,,,https://fepbl.com/index.php/farj/article/download/1406/1684 https://doi.org/10.51594/farj.v6i8.1406,http://dx.doi.org/10.51594/farj.v6i8.1406,,10.51594/farj.v6i8.1406,,,0,,8,true,cc-by-nc,hybrid
125-639-989-356-025,Large Language Model-Guided DeepFutures for Accurate Domestic Futures Trading,2025-07-17,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Singapore,Germany,Qingyi Pan; Pei Yang; Zeyang Liu; Yalin Hu,,,,321,331,Futures contract; Computer science; Financial economics; Economics,,,,,,http://dx.doi.org/10.1007/978-981-96-9914-8_27,,10.1007/978-981-96-9914-8_27,,,0,009-694-585-618-89X; 013-791-083-573-715; 016-361-672-348-910; 019-879-849-095-353; 025-214-242-536-504; 039-229-377-122-798; 053-094-537-530-97X; 068-222-300-264-181; 070-469-015-915-076; 070-739-945-498-579; 085-357-777-382-913; 091-206-469-244-294; 094-198-278-160-874; 098-428-380-981-106; 110-839-524-834-295; 112-522-212-648-191; 116-273-605-294-380; 120-647-690-048-229; 139-487-316-572-362; 161-548-086-295-964; 170-229-980-945-317; 195-808-933-120-009,0,false,,
125-861-047-716-434,Large Language Model Adaptation for Financial Sentiment Analysis,2024-01-26,2024,preprint,arXiv (Cornell University),,,,Pau Rodriguez Inserte; Mariam Nakhlé; Raheel Qader; Gaetan Caillaut; Jingshu Liu,"Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets' financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",,,,,Adaptation (eye); Computer science; Sentiment analysis; Artificial intelligence; Psychology; Neuroscience,,,,,https://arxiv.org/abs/2401.14777,http://dx.doi.org/10.48550/arxiv.2401.14777,,10.48550/arxiv.2401.14777,,,0,,1,true,,green
126-469-229-876-228,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Home",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/llmprivatetrade,https://guides.library.harvard.edu/llmprivatetrade,,,3089333353,,0,,0,false,,
126-965-732-223-569,"SuperCLUE-Fin: Graded Fine-Grained Analysis of Chinese LLMs on Diverse
  Financial Tasks and Applications",2024-04-29,2024,preprint,arXiv (Cornell University),,,,Liang Xu; Lei Zhu; Yaotong Wu; Hang Xue,"The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation framework tailored for Chinese-native financial large language models (FLMs). It assesses FLMs across six financial application domains and twenty-five specialized tasks, encompassing theoretical knowledge and practical applications such as compliance, risk management, and investment analysis. Using multi-turn, open-ended conversations that mimic real-life scenarios, SC-Fin measures models on a range of criteria, including accurate financial understanding, logical reasoning, clarity, computational efficiency, business acumen, risk perception, and compliance with Chinese regulations. In a rigorous evaluation involving over a thousand questions, SC-Fin identifies a performance hierarchy where domestic models like GLM-4 and MoonShot-v1-128k outperform others with an A-grade, highlighting the potential for further development in transforming theoretical knowledge into pragmatic financial solutions. This benchmark serves as a critical tool for refining FLMs in the Chinese context, directing improvements in financial knowledge databases, standardizing financial interpretations, and promoting models that prioritize compliance, risk management, and secure practices. We create a contextually relevant and comprehensive benchmark that drives the development of AI in the Chinese financial sector. SC-Fin facilitates the advancement and responsible deployment of FLMs, offering valuable insights for enhancing model performance and usability for both individual and institutional users in the Chinese market..~\footnote{Our benchmark can be found at \url{https://www.CLUEbenchmarks.com}}.",,,,,Business; Fin; Finance; Materials science; Composite material,,,,,https://arxiv.org/abs/2404.19063,http://dx.doi.org/10.48550/arxiv.2404.19063,,10.48550/arxiv.2404.19063,,,0,,0,true,,green
127-050-062-784-340,Data-Centric Financial Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Zhixuan Chu; Huaiyu Guo; Xinyuan Zhou; Yijia Wang; Fei Yu; Hong Chen; Wanqing Xu; Xin Lu; Qing Cui; Longfei Li; Jun Zhou; Sheng Li,"Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We also open source a new benchmark for financial analysis and interpretation. Our methodology provides a promising path to unlock LLMs' potential for complex real-world domains.",,,,,Computer science; Task (project management); Benchmark (surveying); Key (lock); Raw data; Finance; Interpretation (philosophy); Annotation; Artificial intelligence; Data science; Machine learning; Computer security; Economics; Management; Programming language; Geodesy; Geography,,,,,https://arxiv.org/abs/2310.17784,http://dx.doi.org/10.48550/arxiv.2310.17784,,10.48550/arxiv.2310.17784,,,0,,1,true,other-oa,green
127-299-548-856-344,"AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering",2025-09-16,2025,preprint,arXiv (Cornell University),,,,Onat Gungor; Roshan Sood; Harold Wang; Tajana Rosing,"Large Language Models (LLMs) have recently demonstrated strong potential for cybersecurity question answering (QA), supporting decision-making in real-time threat detection and response workflows. However, their substantial computational demands pose significant challenges for deployment on resource-constrained edge devices. Quantization, a widely adopted model compression technique, can alleviate these constraints. Nevertheless, quantization may degrade model accuracy and increase susceptibility to adversarial attacks. Fine-tuning offers a potential means to mitigate these limitations, but its effectiveness when combined with quantization remains insufficiently explored. Hence, it is essential to understand the trade-offs among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation framework designed to benchmark several state-of-the-art small LLMs under four distinct configurations: base, quantized-only, fine-tuned, and fine-tuned combined with quantization, specifically for cybersecurity QA. Our results demonstrate that quantization alone yields the lowest accuracy and robustness despite improving efficiency. In contrast, combining quantization with fine-tuning enhances both LLM robustness and predictive performance, achieving an optimal balance of accuracy, robustness, and efficiency. These findings highlight the critical need for quantization-aware, robustness-preserving fine-tuning methodologies to enable the robust and efficient deployment of LLMs for cybersecurity QA.",,,,,Adversarial system; Robustness (evolution); Computer science; Quantization (signal processing); Artificial intelligence; Computer security; Computer vision; Biochemistry; Chemistry; Gene,,,,,https://arxiv.org/abs/2509.13514,http://dx.doi.org/10.48550/arxiv.2509.13514,,10.48550/arxiv.2509.13514,,,0,,0,true,,green
127-554-902-275-40X,Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models,2025-02-05,2025,journal article,Journal of Information Systems,08887985; 15587959,American Accounting Association,United States,Huaxia Li; Haoyun Gao; Chengzhang Wu; Miklos A. Vasarhelyi,"<jats:title>ABSTRACT</jats:title>;                <jats:p>This research addresses the challenge of extracting financial data from unstructured sources, a persistent issue for accounting researchers, investors, and regulators. Leveraging large language models (LLMs), this study introduces a novel framework for automated financial data extraction from Portable Document Format (PDF)-formatted files. Following a design science methodology, this research develops the framework through a combination of text mining and prompt engineering techniques. The framework is subsequently applied to analyze governmental annual reports and corporate environmental, social, and governance reports, which are presented in PDF format. Test results indicate that the framework achieves an average 99.5 percent accuracy rate in a notably short time span when extracting key financial indicators. A subsequent large out-of-sample test reveals an overall accuracy rate converging around 96 percent. This study contributes to the evolving literature on applying LLMs in accounting and offers a valuable tool for both academic and industrial applications.</jats:p>;                <jats:p>Data Availability: Data are available upon request.</jats:p>;                <jats:p>JEL Classifications: M41; O31; C81.</jats:p>",39,1,135,156,Computer science; Corporate governance; Test (biology); Key (lock); Data science; Accounting; Unstructured data; Sample (material); Data mining; Finance; Business; Big data; Computer security; Paleontology; Chemistry; Chromatography; Biology,,,,,,http://dx.doi.org/10.2308/isys-2023-047,,10.2308/isys-2023-047,,,0,000-064-613-799-138; 009-261-751-244-528; 010-525-115-959-84X; 013-425-734-974-695; 016-893-782-437-444; 017-590-564-048-81X; 020-409-968-116-435; 025-972-661-029-980; 026-166-698-578-385; 026-786-909-326-848; 028-151-692-716-91X; 038-287-375-579-320; 039-363-136-262-869; 044-111-846-729-62X; 048-708-058-231-027; 051-161-858-118-526; 053-749-787-528-586; 067-129-087-528-172; 076-287-066-741-900; 078-982-190-689-836; 085-826-567-282-26X; 087-196-244-670-101; 087-776-095-156-221; 087-986-828-061-613; 089-312-271-916-486; 095-761-242-666-669; 096-484-313-396-218; 107-002-716-250-830; 110-510-842-185-282; 110-739-741-507-751; 114-516-989-623-630; 122-618-040-279-711; 133-793-781-089-971; 141-843-032-210-01X; 148-012-056-970-651; 154-042-138-562-576; 166-571-115-896-53X; 187-985-761-754-968; 189-738-179-035-882; 190-050-627-029-962; 196-385-133-923-289; 196-685-225-211-910,15,false,,
127-707-729-701-501,FinRipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: ACL 2025,,Association for Computational Linguistics,,Yuanjian Xu; Jianing Hao; Kunsheng Tang; Jingnan Chen; Anxian Liu; Peng Liu; Guang Zhang,,,,9377,9398,Ripple; Financial market; Event (particle physics); Computer science; Financial system; Business; Finance; Engineering; Electrical engineering; Physics; Quantum mechanics; Voltage,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-acl.489,,10.18653/v1/2025.findings-acl.489,,,0,,1,false,,
127-876-060-504-019,Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Boyu Zhang; Hongyang Yang; Xiao-Yang Liu,"Sentiment analysis is a vital tool for uncovering insights from financial articles, news, and social media, shaping our understanding of market movements. Despite the impressive capabilities of large language models (LLMs) in financial natural language processing (NLP), they still struggle with accurately interpreting numerical values and grasping financial context, limiting their effectiveness in predicting financial sentiment. In this paper, we introduce a simple yet effective instruction tuning approach to address these issues. By transforming a small portion of supervised financial sentiment analysis data into instruction data and fine-tuning a general-purpose LLM with this method, we achieve remarkable advancements in financial sentiment analysis. In the experiment, our approach outperforms state-of-the-art supervised sentiment analysis models, as well as widely used LLMs like ChatGPT and LLaMAs, particularly in scenarios where numerical understanding and contextual comprehension are vital.",,,,,Sentiment analysis; Comprehension; Computer science; Limiting; Context (archaeology); Financial market; Social media; Finance; Artificial intelligence; Data science; Machine learning; Natural language processing; Economics; World Wide Web; Engineering; Mechanical engineering; Paleontology; Biology; Programming language,,,,,https://arxiv.org/abs/2306.12659,http://dx.doi.org/10.48550/arxiv.2306.12659,,10.48550/arxiv.2306.12659,,,0,,2,true,other-oa,green
128-037-224-234-65X,On Assessing the Performance of LLMs for Target-Level Sentiment Analysis in Financial News Headlines,2025-01-13,2025,journal article,Algorithms,19994893,MDPI AG,Switzerland,Iftikhar Muhammad; Marco Rospocher,"<jats:p>The importance of sentiment analysis in the rapidly evolving financial markets is widely recognized for its ability to interpret market trends and inform investment decisions. This study delves into the target-level financial sentiment analysis (TLFSA) of news headlines related to stock. The study compares the performance in the TLFSA task of various sentiment analysis techniques, including rule-based models (VADER), fine-tuned transformer-based models (DistilFinRoBERTa and Deberta-v3-base-absa-v1.1) as well as zero-shot large language models (ChatGPT and Gemini). The dataset utilized for this analysis, a novel contribution of this research, comprises 1476 manually annotated Bloomberg headlines and is made publicly available (due to copyright restrictions, only the URLs of Bloomberg headlines with the manual annotations are provided; however, these URLs can be used with a Bloomberg terminal to reconstruct the complete dataset) to encourage future research on this subject. The results indicate that the fine-tuned Deberta-v3-base-absa-v1.1 model performs better across all evaluation metrics than other evaluated models in TLFSA. However, LLMs such as ChatGPT-4, ChatGPT-4o, and Gemini 1.5 Pro provide similar performance levels without the need for task-specific fine-tuning or additional training. The study contributes to assessing the performance of LLMs for financial sentiment analysis, providing useful insights into their possible application in the financial domain.</jats:p>",18,1,46,46,Sentiment analysis; Business; Computer science; Artificial intelligence,,,,,,http://dx.doi.org/10.3390/a18010046,,10.3390/a18010046,,,0,005-276-032-339-156; 006-399-176-500-152; 008-745-781-067-512; 010-723-703-703-573; 014-711-631-488-436; 015-153-666-863-200; 018-137-111-708-177; 019-873-904-232-22X; 019-899-449-307-511; 022-486-017-682-206; 032-489-127-930-342; 037-377-867-142-005; 043-155-576-645-511; 044-456-506-571-452; 050-000-745-030-460; 057-982-454-558-150; 058-176-280-347-707; 059-300-969-491-735; 059-468-439-189-757; 061-008-197-627-623; 066-922-192-268-680; 067-907-983-433-196; 068-883-478-998-762; 080-600-379-598-503; 085-684-211-116-200; 086-574-303-424-904; 089-476-832-049-53X; 089-844-745-151-655; 091-810-810-244-628; 094-436-175-799-721; 098-098-671-761-796; 099-957-866-141-14X; 101-997-359-718-799; 106-887-061-548-648; 111-843-882-943-051; 116-754-792-638-94X; 118-553-746-256-774; 119-155-138-087-500; 125-383-385-403-832; 128-047-174-320-62X; 134-361-757-048-800; 134-968-618-979-921; 143-820-744-358-224; 145-931-944-659-640; 149-482-252-767-514; 150-397-545-090-569; 151-822-917-158-298; 153-209-965-230-829; 167-906-816-365-326; 168-485-640-067-096; 170-068-073-670-052; 184-033-784-831-914; 186-221-799-922-73X; 187-451-840-016-887; 190-050-627-029-962; 196-593-879-207-824; 198-277-411-868-748,4,true,cc-by,gold
128-250-091-695-796,"\textsc{FLAG-Trader}: Fusion LLM-Agent with Gradient-based Reinforcement
  Learning for Financial Trading",2025-02-16,2025,preprint,arXiv (Cornell University),,,,Guojun Xiong; Zhiyang Deng; Keyi Wang; Yupeng Cao; Haohang Li; Yangyang Yu; Xueqing Peng; Mingquan Lin; Kaleb E Smith; Xiao-Yang Liu; Jimin Huang; Sophia Ananiadou; Qianqian Xie,"Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose \textsc{FLAG-Trader}, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.",,,,,Flag (linear algebra); Reinforcement learning; Business; Reinforcement; Computer science; Artificial intelligence; Psychology; Mathematics; Social psychology; Pure mathematics; Algebra over a field,,,,,https://arxiv.org/abs/2502.11433,http://dx.doi.org/10.48550/arxiv.2502.11433,,10.48550/arxiv.2502.11433,,,0,,0,true,,green
128-352-668-781-486,Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert,2024-01-01,2024,preprint,,,Elsevier BV,,Mohammad Alipour-Vaezi; Kwok-Leung Tsui,"Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI). To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project. Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method. Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm. In this paper, firstly, the fame score of the celebrities is determined using a large language model. Then, to tackle the asymmetric character of MPI's data, projects are classified. Furthermore, the box office prediction takes place for each class of projects. Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed.",,,,,Computer science; Motion (physics); Portfolio; Portfolio optimization; Artificial intelligence; Industrial engineering; Business; Engineering; Finance,,,,,https://arxiv.org/pdf/2404.07434 https://arxiv.org/abs/2404.07434,http://dx.doi.org/10.2139/ssrn.4821660,,10.2139/ssrn.4821660,,,0,004-817-632-357-019; 007-017-884-126-763; 010-654-745-829-151; 010-784-229-475-292; 019-706-757-416-919; 020-062-092-347-983; 027-057-022-867-746; 027-762-137-031-731; 031-161-496-420-086; 032-141-140-603-437; 048-476-957-423-890; 048-670-318-877-019; 063-592-773-988-530; 071-057-625-894-028; 072-385-285-248-271; 074-411-338-021-978; 074-625-566-847-628; 094-175-215-344-922; 106-604-221-665-323; 120-445-008-776-800; 135-067-714-451-092; 138-830-554-151-968; 150-321-681-209-455; 162-824-519-935-07X; 168-316-269-779-912; 193-294-470-759-819,0,true,,green
128-447-267-589-890,Fuzzy Ensemble of Large Language Models for Financial Sentiment Analysis,2025-07-26,2025,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Switzerland,,Tsvetelina Stefanova; Slavi Georgiev,,,,553,565,Computer science; Finance; Artificial intelligence; Economics,,,,,,http://dx.doi.org/10.1007/978-3-031-97992-7_62,,10.1007/978-3-031-97992-7_62,,,0,006-399-176-500-152; 018-866-913-622-957; 035-860-228-399-59X; 050-536-563-095-269; 051-651-564-732-696; 062-027-974-769-598; 089-104-617-225-126; 096-732-715-582-013; 113-585-988-124-170; 124-886-932-205-623; 145-792-465-380-055,0,false,,
128-506-700-717-37X,Resource-Performance Trade-offs in Open-Source Large Language Models: A Comparative Analysis of Deployment Optimization and Lifecycle Management,2025-02-28,2025,conference proceedings article,2025 8th International Symposium on Big Data and Applied Statistics (ISBDAS),,IEEE,,Tingting Lin; Zaoyi Zheng,,,,55,60,Software deployment; Computer science; Resource (disambiguation); Application lifecycle management; Open source; Resource management (computing); Distributed computing; Software engineering; Programming language; Software; Computer network,,,,,,http://dx.doi.org/10.1109/isbdas64762.2025.11117056,,10.1109/isbdas64762.2025.11117056,,,0,058-910-511-880-378,0,false,,
128-806-266-848-055,"BreakGPT: A Large Language Model with Multi-stage Structure for
  Financial Breakout Detection",2024-02-12,2024,preprint,arXiv (Cornell University),,,,Kang Zhang; Osamu Yoshie; Lichao Sun; Weiran Huang,"Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the multi-stage structure contributing 17.6% to the improvement. Additionally, it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available: https://github.com/Neviim96/BreakGPT",,,,,Breakout; Stage (stratigraphy); Finance; Computer science; Business; Geology; Paleontology,,,,,https://arxiv.org/abs/2402.07536,http://dx.doi.org/10.48550/arxiv.2402.07536,,10.48550/arxiv.2402.07536,,,0,,0,true,,green
129-084-666-809-99X,Benchmarking Large Language Models on CFLUE - A Chinese Financial Language Understanding Evaluation Dataset,,2024,conference proceedings article,Findings of the Association for Computational Linguistics ACL 2024,,Association for Computational Linguistics,,Jie Zhu; Junhui Li; Yalong Wen; Lifan Guo,,,,5673,5693,Benchmarking; Computer science; Natural language processing; Language model; Artificial intelligence; Business; Marketing,,,,,,http://dx.doi.org/10.18653/v1/2024.findings-acl.337,,10.18653/v1/2024.findings-acl.337,,,0,,3,false,,
129-306-626-140-831,The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,2024-02-13,2024,preprint,,,Center for Open Science,,Minkyu Shin; Jin Kim; Jiwoong Shin,"<p>Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. Utilizing a leading AI detection tool, we analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We establish a causal relationship between LLM usage and an increased likelihood of obtaining relief by employing instrumental variables to address endogeneity in LLM adoption. Experimental data further support this link, demonstrating that LLMs enhance the clarity and persuasiveness of consumer narratives. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.</p>",,,,,Persuasion; Feature (linguistics); Linguistics; Computer science; Natural language processing; Artificial intelligence; Psychology; Philosophy,,,,,https://osf.io/fdzqg/download https://doi.org/10.31234/osf.io/fdzqg http://arxiv.org/pdf/2311.16466 http://arxiv.org/abs/2311.16466,http://dx.doi.org/10.31234/osf.io/fdzqg,,10.31234/osf.io/fdzqg,,,0,,0,true,,green
129-421-021-442-62X,AI-LieDar : Examine the Trade-off Between Utility and Truthfulness in LLM Agents,,2025,conference proceedings article,Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,Association for Computational Linguistics,,Zhe Su; Xuhui Zhou; Sanketh Rangreji; Anubha Kabra; Julia Mendelsohn; Faeze Brahman; Maarten Sap,,,,11867,11894,Computer science,,,,,,http://dx.doi.org/10.18653/v1/2025.naacl-long.595,,10.18653/v1/2025.naacl-long.595,,,0,,1,false,,
129-616-278-330-519,Complex forecasting and investment strategy optimization via chain-of-thought of large language models,2025-10-07,2025,journal article,Expert Systems with Applications,09574174; 18736793,Elsevier BV,United Kingdom,Meiqun Yin; Mengzhu Guo,,298,,129913,129913,Computer science; Chain (unit); Investment (military); Artificial intelligence; Physics; Astronomy; Politics; Political science; Law,,,,Beijing Municipal Education Commission; Humanities and Social Science Fund of Ministry of Education of the People's Republic of China; China University of Political Science and Law,,http://dx.doi.org/10.1016/j.eswa.2025.129913,,10.1016/j.eswa.2025.129913,,,0,002-727-126-302-996; 015-711-945-627-415; 017-951-997-872-67X; 020-708-991-710-141; 027-106-342-672-655; 027-412-065-722-196; 027-633-223-258-050; 032-248-615-558-503; 036-917-390-936-032; 040-773-348-482-916; 049-424-455-829-577; 050-052-020-053-969; 051-163-526-665-354; 053-018-661-266-657; 056-764-590-798-800; 056-932-238-561-302; 058-343-116-982-157; 059-254-331-498-140; 065-808-547-637-567; 070-983-789-093-389; 071-627-438-625-860; 074-499-124-873-904; 077-234-831-308-192; 082-604-417-097-808; 084-377-540-269-041; 087-323-490-534-091; 089-983-128-975-747; 094-281-089-056-792; 098-926-109-358-108; 105-381-163-750-586; 109-685-893-550-98X; 111-042-770-080-634; 112-705-069-209-103; 119-476-087-255-320; 120-105-786-820-133; 125-910-914-668-53X; 132-126-658-686-53X; 136-421-032-257-183; 146-879-740-372-593; 149-806-609-919-567; 158-439-977-842-051; 164-593-546-653-040; 174-707-813-006-311; 178-086-424-515-848; 189-515-902-230-938,0,true,cc-by-nc-nd,hybrid
129-623-850-695-275,Forecasting of Marketing News in Financial Sector for Sentiment Analysis Using LLMs (Large Language Models),2025-07-18,2025,journal article,"International Journal of Advanced Research in Science, Communication and Technology",25819429,Naksh Solutions,,null Sandeep Gupta,"<jats:p>Sentiment Analysis is a process where a text is assigned whether it has a positive tone, a negative tone, or a neutral tone. In finance, sentiment analysis is used to derive a numerical signal out of financial text. In order to do financial textual analysis on a collection of financial texts that are a component of a larger dataset called the Financial Phrase Bank, this study presents a novel strong sentiment classification architecture based on an improved model of Bidirectional Encoder Representations from Transformers (BERT). The methodology involves all preprocessing requirements, which are text cleaning, BERT Word Piece tokenization, label encoding, and down-sampling to balance the classes. Common measures, such as accuracy of 95.29%, precision of 95.37%, recall of 95.24%, and f1-score of 95.32%, were used to train and evaluate the model. These metrics show that the model performs better than more traditional machine learning models like Random Forest.  together with language models like GPT-4o. Visualization was used to explain the model behavior and generalization power; e.g., the visualization of a confounding array, training-validation curve plots, and a token distribution plot. The experimental findings justify the utility of the proposed BERT-based approach in retrieving subtle sentiment expression in elaborate financial narrative texts, thus providing scalable and effective accuracy to sentiment-based financial analytics</jats:p>",,,462,475,Computer science; Sentiment analysis; Artificial intelligence; Natural language processing; Encoder; Machine learning; Finance; Economics; Operating system,,,,,,http://dx.doi.org/10.48175/ijarsct-28347,,10.48175/ijarsct-28347,,,0,015-246-195-452-343; 031-985-090-775-081; 032-584-447-760-448; 044-456-506-571-452; 045-132-496-704-762; 046-794-308-175-277; 048-787-008-679-411; 050-536-563-095-269; 057-911-055-456-714; 063-754-475-082-166; 065-563-713-298-140; 081-818-560-650-781; 087-807-064-907-108; 089-277-292-785-672; 110-096-666-711-659; 123-441-635-858-339; 147-127-018-823-873; 149-628-347-844-964; 149-984-921-353-926; 150-099-715-305-45X; 152-414-453-384-708; 155-372-060-695-093; 161-142-856-132-046; 161-172-679-686-474; 169-225-700-144-571; 172-310-748-049-551; 186-074-354-232-720,0,true,,bronze
129-693-810-711-721,Optimizing Large Language Models for ESG Activity Detection in Financial Texts,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Mattia Birti; Andrea Maurino; Francesco Osborne,,,,856,863,,,,,,,http://dx.doi.org/10.1145/3768292.3770371,,10.1145/3768292.3770371,,,0,018-866-913-622-957; 028-951-714-716-909; 034-492-996-583-91X; 037-958-861-442-897; 040-112-450-890-547; 041-556-616-567-971; 048-026-274-723-861; 048-442-977-865-850; 067-639-916-911-241; 079-952-778-785-422; 111-109-353-914-11X; 127-789-751-111-964; 142-294-826-578-403; 154-042-138-562-576; 167-581-525-530-798; 178-440-806-032-007; 185-133-161-239-708,0,false,,
129-763-631-540-390,"An Agent Framework for Real-Time Financial Information Searching with
  Large Language Models",2024-12-14,2024,preprint,arXiv (Cornell University),,,,Jinzheng Li; Jingshu Zhang; Hongguang Li; Yiqing Shen,"Financial decision-making requires processing vast amounts of real-time information while understanding their complex temporal relationships. While traditional search engines excel at providing real-time information access, they often struggle to comprehend sophisticated user intentions and contextual nuances. Conversely, Large Language Models (LLMs) demonstrate reasoning and interaction capabilities but may generate unreliable outputs without access to current data. While recent attempts have been made to combine LLMs with search capabilities, they suffer from (1) restricted access to specialized financial data, (2) static query structures that cannot adapt to dynamic market conditions, and (3) insufficient temporal awareness in result generation. To address these challenges, we present FinSearch, a novel agent-based search framework specifically designed for financial applications that interface with diverse financial data sources including market, stock, and news data. Innovatively, FinSearch comprises four components: (1) an LLM-based multi-step search pre-planner that decomposes user queries into structured sub-queries mapped to specific data sources through a graph representation; (2) a search executor with an LLM-based adaptive query rewriter that executes the searching of each sub-query while dynamically refining the sub-queries in its subsequent node based on intermediate search results; (3) a temporal weighting mechanism that prioritizes information relevance based on the deduced time context from the user's query; (4) an LLM-based response generator that synthesizes results into coherent, contextually appropriate outputs. To evaluate FinSearch, we construct FinSearchBench-24, a benchmark of 1,500 four-choice questions across the stock market, rate changes, monetary policy, and industry developments spanning from June to October 2024.",,,,,Computer science; Finance; Business,,,,,https://arxiv.org/abs/2502.15684,http://dx.doi.org/10.48550/arxiv.2502.15684,,10.48550/arxiv.2502.15684,,,0,,0,true,,green
130-886-020-316-870,An Adaptive Simulated Startup Financial Modeling Mentor Using a Large Language Model to Address Shortages in Skilled Advisors: Architecture and Design Considerations,2025-05-25,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Joseph Benjamin Ilagan; Jose Ramon Ilagan; Lois Abigail To; Zachary Matthew Alabastro,,,,56,66,Economic shortage; Computer science; Architecture; Software engineering; Finance; Business; Art; Linguistics; Philosophy; Government (linguistics); Visual arts,,,,,,http://dx.doi.org/10.1007/978-3-031-92967-0_4,,10.1007/978-3-031-92967-0_4,,,0,003-481-907-376-17X; 009-299-623-546-736; 009-945-227-974-388; 011-760-321-999-769; 020-115-682-484-917; 026-016-338-595-831; 035-451-752-279-052; 046-369-704-952-384; 050-029-935-262-624; 055-657-583-317-669; 056-679-212-354-341; 071-499-327-882-983; 073-817-390-908-019; 081-242-797-012-826; 087-564-817-267-675; 090-261-515-629-095; 095-171-966-388-094; 096-035-096-537-059; 104-829-105-834-323; 160-111-869-692-562; 174-225-699-128-021; 191-560-904-792-031,0,false,,
130-955-354-775-571,Evaluating Automatic Annotation Techniques for Fine-Tuning Large Language Models in Financial Sentiment Analysis,2025-02-18,2025,conference proceedings article,2025 4th International Conference on Sentiment Analysis and Deep Learning (ICSADL),,IEEE,,Ibtissam Youb; Mohamed Hamlich; Sebastian Ventura; Feras Al-Obeidat,,,,215,220,Annotation; Computer science; Sentiment analysis; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/icsadl65848.2025.10933418,,10.1109/icsadl65848.2025.10933418,,,0,034-654-362-539-406; 057-040-097-643-496; 088-819-336-859-539; 106-634-300-406-538; 126-226-045-638-771; 126-625-603-503-90X; 127-884-472-980-409; 140-703-629-601-40X; 147-588-302-600-556; 162-270-098-831-296; 162-284-432-912-380; 167-618-527-685-693; 171-034-382-135-476; 176-765-780-532-734; 186-348-663-756-988,0,false,,
131-421-223-968-269,FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs,2025-07-24,2025,preprint,arXiv (Cornell University),,,,Giorgos Iacovides; Wuyang Zhou; Danilo Mandic,"Opinions expressed in online finance-related textual data are having an increasingly profound impact on trading decisions and market movements. This trend highlights the vital role of sentiment analysis as a tool for quantifying the nature and strength of such opinions. With the rapid development of Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs) have become the de facto standard for financial sentiment analysis. However, the SFT paradigm can lead to memorization of the training data and often fails to generalize to unseen samples. This is a critical limitation in financial domains, where models must adapt to previously unobserved events and the nuanced, domain-specific language of finance. To this end, we introduce FinDPO, the first finance-specific LLM framework based on post-training human preference alignment via Direct Preference Optimization (DPO). The proposed FinDPO achieves state-of-the-art performance on standard sentiment classification benchmarks, outperforming existing supervised fine-tuned models by 11% on the average. Uniquely, the FinDPO framework enables the integration of a fine-tuned causal LLM into realistic portfolio strategies through a novel 'logit-to-score' conversion, which transforms discrete sentiment predictions into continuous, rankable sentiment scores (probabilities). In this way, simulations demonstrate that FinDPO is the first sentiment-based approach to maintain substantial positive returns of 67% annually and strong risk-adjusted performance, as indicated by a Sharpe ratio of 2.0, even under realistic transaction costs of 5 basis points (bps).",,,,,Preference; Business; Financial market; Economics; Finance; Microeconomics,,,,,https://arxiv.org/abs/2507.18417,http://dx.doi.org/10.48550/arxiv.2507.18417,,10.48550/arxiv.2507.18417,,,0,,0,true,,green
131-553-585-292-221,From Rigid Robo-Advisors to Human-Like Interactions: Revolutionizing Financial Assistance with LLM-Powered Solutions,2025-02-28,2025,conference proceedings article,"Natural Language Processing, Information Retrieval and AI Trends 2025",,Academy & Industry Research Collaboration Center,,Hamza Landolsi; Ines Abdeljaoued-Tej,"<jats:p>Generative Artificial Intelligence (GenAI) is revolutionizing the business world by increasing availability, efficiency, cost reduction, and innovation. This paper explores the application of Large Language Models (LLMs) and GenAI to finance. It proposes a novel framework on how we can imagine robo-advisory systems, from a traditional rigid platform to a more humanized solution that further engages the investor in a hand-picking asset selection process and better understands their goals and profile using LLMs. We designed an end-to-end solution to overcome many limitations such as lack of flexibility in roboadvisors, lack of possible asset types (usually only equities) and the problem of real-time access to high quality data. The solution architecture includes dynamic client profiling, risk aversion estimation and portfolio optimization. Using robust data pipelines to curate the latest market information, the Asset Selector Agent has been customized. Through iterative development, we employed prompt engineering and multi-agent workflows to enhance user interactions and deliver meaningful insights. By developing an innovative chatbot platform, we demonstrate the potential of LLMs to transform customer service, increase engagement, and provide strategic financial advice.</jats:p>",,,157,168,Computer science; Finance; Business,,,,,,http://dx.doi.org/10.5121/csit.2024.150212,,10.5121/csit.2024.150212,,,0,,0,false,,
131-641-740-220-552,Three trustworthiness challenges in large language model-based financial systems: real-world examples and mitigation strategies,2025-11-17,2025,journal article,Frontiers of Information Technology & Electronic Engineering,20959184; 20959230,Zhejiang University Press,United States,Shurui Xu; Feng Luo; Shuyan Li; Mengzhen Fan; Zhongtian Sun,,26,10,1871,1878,,,,,,,http://dx.doi.org/10.1631/fitee.2500421,,10.1631/fitee.2500421,,,0,008-667-306-830-960; 023-178-666-315-643; 023-369-136-170-16X; 044-165-905-162-576; 048-999-103-656-343; 066-712-655-724-780; 069-279-058-723-637; 070-500-951-049-986; 078-141-527-597-27X; 084-028-598-567-636; 096-311-846-219-067; 097-046-895-049-713; 097-197-155-734-262; 098-353-296-371-651; 111-769-133-477-716; 116-030-319-986-371; 118-665-767-235-522; 124-690-193-709-451; 173-116-597-012-168; 175-753-616-448-106; 179-563-009-935-867; 191-041-689-806-87X,0,false,,
131-683-598-533-499,ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs,2025-10-01,2025,preprint,arXiv (Cornell University),,,,Adi Simhi; Jonathan Herzig; Martin Tutek; Itay Itzhak; Idan Szpektor; Yonatan Belinkov,"As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",,,,,Pragmatism; Economics; Business; Epistemology; Philosophy,,,,,https://arxiv.org/abs/2510.00857,http://dx.doi.org/10.48550/arxiv.2510.00857,,10.48550/arxiv.2510.00857,,,0,,0,true,,green
131-763-604-491-441,Free to Play: United Nations Trade and Development's Experience With Developing Its Own Open-source Retrieval Augmented Generation Large Language Model Application,2024-01-01,2024,preprint,,,Elsevier BV,,Daniel Hopp,,,,,,Open source; Computer science; International trade; World Wide Web; Business; Programming language; Software,,,,,,http://dx.doi.org/10.2139/ssrn.4869135,,10.2139/ssrn.4869135,,,0,,0,false,,
131-784-769-722-490,Consumers' adoption of chatbots in Irish financial market,,2019,dissertation,,,,,Monil Patel,,,,,,Business; Financial market; Irish; Financial system,,,,,https://esource.dbs.ie/handle/10788/3791,https://esource.dbs.ie/handle/10788/3791,,,2953747408,,0,,0,false,,
132-444-085-181-369,"Extracting Structured Insights from Financial News: An Augmented LLM
  Driven Approach",2024-07-22,2024,preprint,arXiv (Cornell University),,,,Rian Dolphin; Joe Dursun; Jonathan Chow; Jarrett Blankenship; Katie Adams; Quinton Pike,"Financial news plays a crucial role in decision-making processes across the financial sector, yet the efficient processing of this information into a structured format remains challenging. This paper presents a novel approach to financial news processing that leverages Large Language Models (LLMs) to overcome limitations that previously prevented the extraction of structured data from unstructured financial news. We introduce a system that extracts relevant company tickers from raw news article content, performs sentiment analysis at the company level, and generates summaries, all without relying on pre-structured data feeds. Our methodology combines the generative capabilities of LLMs, and recent prompting techniques, with a robust validation framework that uses a tailored string similarity approach. Evaluation on a dataset of 5530 financial news articles demonstrates the effectiveness of our approach, with 90% of articles not missing any tickers compared with current data providers, and 22% of articles having additional relevant tickers. In addition to this paper, the methodology has been implemented at scale with the resulting processed data made available through a live API endpoint, which is updated in real-time with the latest news. To the best of our knowledge, we are the first data provider to offer granular, per-company sentiment analysis from news articles, enhancing the depth of information available to market participants. We also release the evaluation dataset of 5530 processed articles as a static file, which we hope will facilitate further research leveraging financial news.",,,,,Computer science; Business,,,,,https://arxiv.org/abs/2407.15788,http://dx.doi.org/10.48550/arxiv.2407.15788,,10.48550/arxiv.2407.15788,,,0,,1,true,,green
132-477-171-574-338,Large Language Model in Financial Regulatory Interpretation,2024-05-10,2024,preprint,arXiv (Cornell University),,,,Zhiyu Cao; Zachary Feinstein,"This study explores the innovative use of Large Language Models (LLMs) as analytical tools for interpreting complex financial regulations. The primary objective is to design effective prompts that guide LLMs in distilling verbose and intricate regulatory texts, such as the Basel III capital requirement regulations, into a concise mathematical framework that can be subsequently translated into actionable code. This novel approach aims to streamline the implementation of regulatory mandates within the financial reporting and risk management systems of global banking institutions. A case study was conducted to assess the performance of various LLMs, demonstrating that GPT-4 outperforms other models in processing and collecting necessary information, as well as executing mathematical calculations. The case study utilized numerical simulations with asset holdings -- including fixed income, equities, currency pairs, and commodities -- to demonstrate how LLMs can effectively implement the Basel III capital adequacy requirements.",,,,,Interpretation (philosophy); Business; Economics; Computer science; Programming language,,,,,https://arxiv.org/abs/2405.06808,http://dx.doi.org/10.48550/arxiv.2405.06808,,10.48550/arxiv.2405.06808,,,0,,0,true,,green
132-543-985-347-452,Large Language Models for Financial Knowledge Extraction Analytical Insights and Corporate Planning Support,2025-10-20,2025,journal article,Mathematical Modeling and Algorithm Application,30060842; 30062020,Darcy & Roy Press Co. Ltd.,,Xuguang Zhang; Mengdie Wang,"<jats:p>Large language models (LLMs) have emerged as transformative technologies in financial services, demonstrating unprecedented capabilities in extracting structured knowledge from unstructured financial documents, generating analytical insights, and supporting strategic corporate planning decisions. This review paper examines the comprehensive applications of LLMs including GPT-4, Claude, PaLM, and domain-specific financial models in automating knowledge extraction from diverse sources including earnings calls, financial reports, regulatory filings, and market commentary. We analyze how transformer-based architectures (TA) leverage attention mechanisms and contextual embeddings to understand complex financial terminology, temporal relationships, and causal connections in financial narratives. The paper explores advanced techniques including prompt engineering, few-shot learning, retrieval-augmented generation (RAG), and fine-tuning strategies that adapt general-purpose LLMs to specialized financial tasks. We examine applications in sentiment analysis of financial texts, automatic summarization of lengthy reports, entity recognition for companies and products, relationship extraction between financial events, and question-answering systems for financial queries. The review investigates how LLMs generate analytical insights through scenario analysis, trend identification, risk assessment, and competitive intelligence synthesis. We analyze corporate planning support applications including strategic initiative identification, market opportunity analysis, resource allocation recommendations, and investment thesis generation. Furthermore, we discuss integration architectures combining LLMs with structured databases, time-series models, and visualization tools to create comprehensive decision support systems. The paper addresses critical challenges including hallucination mitigation, accuracy verification, regulatory compliance, data privacy concerns, and the need for human oversight in high-stakes financial decisions. We examine evaluation methodologies for financial LLM applications, including domain-specific benchmarks, expert assessment protocols, and real-world performance metrics. Through synthesis of current research and deployed systems, we identify limitations including computational costs, update frequency challenges, bias in training data, and difficulties in explaining model reasoning. The review concludes by outlining promising research directions including multimodal financial analysis, real-time information integration, federated learning for privacy-preserving collaboration, and neuro-symbolic approaches combining neural language understanding with formal financial reasoning.</jats:p>",6,2,44,56,Computer science; Knowledge management; Management science; Artificial intelligence; Engineering,,,,,https://drpress.org/ojs/index.php/mmaa/article/download/32153/31475 https://doi.org/10.54097/7am6vk38,http://dx.doi.org/10.54097/7am6vk38,,10.54097/7am6vk38,,,0,,0,false,,
132-955-794-473-510,Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?,2025-01-01,2025,preprint,,,Elsevier BV,,Weixian Waylon Li; Hyeonjun Kim; Mihai Cucuringu; Tiejun Ma,,,,,,Finance; Business; Short run; Market timing; Economics; Monetary economics; Initial public offering,,,,,,http://dx.doi.org/10.2139/ssrn.5250454,,10.2139/ssrn.5250454,,,0,008-919-867-507-440; 023-433-360-240-592; 031-540-205-245-178; 050-448-679-723-845; 050-665-877-314-04X; 055-161-637-947-203; 059-896-966-271-462; 087-595-944-510-166; 093-887-462-867-797; 101-314-666-696-838; 124-340-421-952-454; 149-160-037-003-51X,0,false,,
133-009-529-051-929,Democratizing Financial Information using GPT-4o,2025-01-01,2025,preprint,,,Elsevier BV,,Vivien Buergler; Brittany Davidson; Manuel Bürgler,,,,,,Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5130935,,10.2139/ssrn.5130935,,,0,,0,false,,
133-486-986-571-331,Designing Trading Strategies with LLMs: A DSL-Driven Framework Using In-Context Learning,2025-07-26,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Singapore,Germany,Jinheng Wu; Di Zhang; Qiang Niu,,,,278,291,Digital subscriber line; Computer science; Context (archaeology); Artificial intelligence; Telecommunications; History; Archaeology,,,,,,http://dx.doi.org/10.1007/978-981-96-9891-2_24,,10.1007/978-981-96-9891-2_24,,,0,006-853-249-277-391; 018-866-913-622-957; 028-466-972-983-138; 032-323-291-720-742; 036-581-954-979-278; 061-087-374-030-560; 071-143-899-460-850; 123-659-448-198-496; 124-339-146-710-566,0,false,,
133-663-318-813-280,A localized and humanized approach to chatbot banking companions: implications for financial managers,2025-01-28,2025,journal article,Management Decision,00251747; 17586070,Emerald,United Kingdom,Richa Misra; Garima Malik; Pratibha Singh,"<jats:sec>;                     <jats:title>Purpose</jats:title>;                     <jats:p>The study aims to examine the influence of Unified Theory of Acceptance and Use of Technology (UTAUT) and anthropomorphic design cues in determining the level of satisfaction among banking chatbot users. It also tests the moderating impact of the localization of content on the relationship. The study also encompasses expectation confirmation, elucidating the significance of perceived trust in maintaining intention.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Design/methodology/approach</jats:title>;                     <jats:p>The study conducted a comprehensive online survey, collecting 667 questionnaires from users of conversational chatbots in both public and private sector banks. We analyse the data using Partial Least Squares Structural Equation Modelling and fuzzy-set Qualitative Comparative Analysis.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Findings</jats:title>;                     <jats:p>Performance and effort expectancy, perceived interestingness of interaction and perceived empathy were identified as significant indicators, whereas facilitating conditions, social influence and perceived intelligence were not significant in explaining satisfaction. Perceived trust was a significant mediator, while localization was a significant moderator in all the cases except social influence and satisfaction.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Practical implications</jats:title>;                     <jats:p>To improve perceived intelligence and empathy, tech developers should focus on improving the chatbot’s ability to maintain contextual understanding within a conversation where it can remember and reference previous interactions. Future studies might explore the development of banking chatbots that incorporate advanced levels of anthropomorphic characteristics, whether visual or intuitive.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Originality/value</jats:title>;                     <jats:p>The work is unique in that it integrates UTATUT, anthropomorphism and expectation confirmation model in the context of conversational banking chatbots, which is not achievable in a single theory-based model. The study also underlined the necessity of localizing chatbot content, recommending that banks engage localized native speakers to help with chatbot training and content creation, where specialists can fine-tune the conversational features.</jats:p>;                   </jats:sec>",63,10,3756,3785,Chatbot; Business; Finance; Marketing; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.1108/md-11-2023-2223,,10.1108/md-11-2023-2223,,,0,001-536-892-769-300; 002-341-069-473-868; 002-822-598-669-417; 003-424-179-812-795; 003-996-245-480-025; 004-048-199-668-719; 005-101-889-506-713; 009-614-325-071-207; 010-538-826-232-260; 011-013-047-461-835; 011-057-105-680-492; 011-134-433-410-527; 012-437-090-078-087; 013-085-740-373-249; 014-370-642-060-096; 014-614-449-675-783; 015-583-058-931-02X; 015-826-816-936-502; 017-728-756-779-456; 019-659-084-265-634; 020-431-120-288-636; 020-548-165-008-916; 023-161-206-593-57X; 023-351-561-716-631; 024-120-523-981-58X; 024-800-443-292-177; 028-190-026-087-674; 028-880-839-572-237; 031-824-009-360-923; 032-637-445-493-370; 033-171-354-077-456; 033-451-056-986-404; 033-525-740-901-402; 033-826-575-023-317; 033-919-268-366-056; 035-689-054-324-880; 036-961-546-949-007; 039-848-016-200-976; 041-260-690-716-303; 041-340-621-976-144; 041-922-423-670-611; 042-168-700-816-434; 043-213-633-162-472; 044-523-432-982-677; 044-944-587-598-86X; 047-646-254-319-184; 048-415-241-483-67X; 049-108-646-637-047; 051-011-707-462-588; 052-416-428-699-488; 052-792-536-581-208; 053-619-698-238-063; 054-244-557-837-407; 055-794-758-962-071; 056-898-146-452-233; 058-325-051-928-213; 058-355-220-910-565; 058-585-925-948-305; 060-439-276-982-656; 061-513-421-087-834; 066-608-811-388-556; 066-743-645-416-727; 071-300-740-720-776; 072-358-867-496-140; 073-714-506-351-872; 074-840-020-692-590; 077-946-279-594-919; 077-974-896-505-940; 079-395-716-203-379; 081-450-197-798-016; 084-219-915-167-698; 087-735-939-295-395; 087-739-821-614-205; 089-805-938-792-676; 090-612-191-850-659; 093-424-310-783-34X; 093-817-413-925-539; 094-166-274-386-36X; 095-410-756-089-665; 095-926-768-457-602; 096-868-272-744-791; 099-107-007-394-412; 099-298-068-435-462; 100-288-203-357-132; 101-715-491-458-383; 102-152-924-164-867; 103-211-405-753-065; 106-537-078-679-396; 110-139-762-711-584; 110-515-937-016-281; 110-685-865-916-855; 112-387-621-505-766; 112-616-290-417-943; 114-261-157-107-703; 115-724-526-435-062; 117-878-052-092-532; 119-694-732-392-05X; 120-442-150-874-256; 126-841-632-048-586; 129-254-494-215-069; 129-978-287-196-751; 134-692-988-441-892; 135-216-099-616-428; 138-766-609-906-659; 140-257-398-405-251; 140-732-076-739-275; 142-314-870-995-875; 142-654-114-772-052; 143-875-130-007-110; 150-887-963-505-239; 159-549-280-195-935; 159-828-090-331-803; 161-319-472-071-054; 161-988-635-828-583; 163-864-824-279-286; 167-589-278-436-994; 169-722-444-853-412; 173-412-054-210-404; 190-012-567-970-920; 190-889-172-797-52X; 194-454-574-354-193,4,false,,
134-136-252-227-167,Text-Based Chatbot in Financial Sector: A Systematic Literature Review,,2022,journal article,Data Science in Finance and Economics,27692140,American Institute of Mathematical Sciences (AIMS),,Hana Demma Wube; Sintayehu Zekarias Esubalew; Firesew Fayiso Weldesellasie; Taye Girma Debelee,"<jats:p xml:lang=""fr"">&lt;abstract&gt;; 			&lt;p&gt;Text-based chatbots are implemented in the financial sector to enhance the relationship between the customer and services provided by the sector, and also to address external challenges and customer requirements. The chatbot technology in the financial sector serves to examine customers' frequently asked questions and the representation of the process using machine learning. In light of this, this study presents a comprehensive systematic literature review of articles focused on text-based chatbots in the financial sector. It describes the understanding of chatbots in the financial sector in terms of implementation, adoption intention, attitude toward use and acceptance; it also describes how people experience chatbots, specifically in terms of perception, expectation and trust, as well as how they are engaging and emotionally motivated; management of the security and privacy vulnerabilities of the chatbots; and identifies the potential strategies that can hinder the efficient, successful evolution of chatbots in the financial sector. Finally, the main findings regarding the use of text chatbots in the financial sector are presented; additionally, the open issues in current research are highlighted and a number of research opportunities that can be pursued in the future are suggested.&lt;/p&gt;; 		&lt;/abstract&gt;</jats:p>",2,3,232,259,Chatbot; Financial sector; Financial services; Process (computing); Business; Perception; Knowledge management; Finance; Marketing; Computer science; Psychology; World Wide Web; Neuroscience; Operating system,,,,,,http://dx.doi.org/10.3934/dsfe.2022011,,10.3934/dsfe.2022011,,,0,001-536-892-769-300; 002-462-717-281-874; 007-581-025-024-301; 007-910-585-532-616; 008-547-612-630-960; 009-146-995-927-655; 012-650-453-628-774; 014-112-573-173-086; 014-114-606-107-800; 014-175-280-666-387; 015-826-816-936-502; 017-660-940-597-670; 018-261-361-571-431; 019-405-388-118-641; 020-184-606-784-933; 021-212-751-046-613; 024-800-443-292-177; 025-324-253-335-086; 026-556-043-073-021; 027-270-487-933-953; 027-349-280-262-816; 030-360-159-282-183; 031-420-886-532-773; 031-730-206-059-972; 032-356-351-223-897; 033-826-575-023-317; 034-639-916-724-668; 036-745-550-009-046; 037-208-481-399-505; 038-611-674-466-408; 043-549-099-550-350; 047-646-254-319-184; 047-732-821-562-668; 048-263-270-819-478; 048-267-907-422-055; 051-963-527-939-084; 052-771-058-185-872; 054-741-487-048-863; 055-794-758-962-071; 058-687-501-124-289; 059-805-994-516-643; 062-027-697-360-566; 063-997-509-752-321; 070-129-188-656-825; 071-539-716-498-313; 077-946-279-594-919; 080-509-819-435-082; 082-034-839-794-708; 084-062-553-476-828; 095-424-010-770-339; 096-114-871-538-374; 096-761-491-641-856; 096-868-272-744-791; 100-886-971-350-574; 102-283-831-865-242; 103-760-047-925-139; 105-753-084-464-981; 106-900-362-790-052; 108-499-083-524-709; 110-139-762-711-584; 116-194-587-286-735; 119-275-549-696-155; 119-745-669-118-660; 119-802-229-029-275; 136-211-937-581-097; 138-499-233-775-182; 148-157-201-760-878; 154-251-217-686-21X; 155-159-325-133-027; 159-549-280-195-935; 163-864-824-279-286; 171-576-034-263-697,34,true,cc-by,gold
134-254-020-434-498,"FinDKG: Dynamic Knowledge Graphs with Large Language Models for
  Detecting Global Trends in Financial Markets",2024-07-15,2024,preprint,arXiv (Cornell University),,,,Xiaohui Victor Li; Francesco Sanna Passino,"Dynamic knowledge graphs (DKGs) are popular structures to express different types of connections between objects over time. They can also serve as an efficient mathematical tool to represent information extracted from complex unstructured data sources, such as text or images. Within financial applications, DKGs could be used to detect trends for strategic thematic investing, based on information obtained from financial news articles. In this work, we explore the properties of large language models (LLMs) as dynamic knowledge graph generators, proposing a novel open-source fine-tuned LLM for this purpose, called the Integrated Contextual Knowledge Graph Generator (ICKG). We use ICKG to produce a novel open-source DKG from a corpus of financial news articles, called FinDKG, and we propose an attention-based GNN architecture for analysing it, called KGTransformer. We test the performance of the proposed model on benchmark datasets and FinDKG, demonstrating superior performance on link prediction tasks. Additionally, we evaluate the performance of the KGTransformer on FinDKG for thematic investing, showing it can outperform existing thematic ETFs.",,,,,Financial market; Business; Economics; Computer science; Finance,,,,,https://arxiv.org/abs/2407.10909,http://dx.doi.org/10.48550/arxiv.2407.10909,,10.48550/arxiv.2407.10909,,,0,,0,true,,green
134-673-685-280-485,ZiGong 1.0: A Large Language Model for Financial Credit,2025-05-19,2025,conference proceedings article,2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW),,IEEE,,Yu Lei; Zixuan Wang; Chu Liu; Tongyao Wang,"Large Language Models (LLMs) have demonstrated strong performance across various general Natural Language Processing (NLP) tasks. However, their effectiveness in financial credit assessment applications remains suboptimal, primarily due to the specialized financial expertise required for these tasks. To address this limitation, we propose ZiGong, a Mistral-based model enhanced through multi-task supervised fine-tuning. To specifically combat model hallucination in financial contexts, we introduce a novel data pruning methodology. Our approach utilizes a proxy model to score training samples, subsequently combining filtered data with original datasets for model training. This data refinement strategy effectively reduces hallucinations in LLMs while maintaining reliability in downstream financial applications. Experimental results show our method significantly enhances model robustness and prediction accuracy in real-world financial scenarios.",,,266,270,Computer science; Finance; Business,,,,,http://arxiv.org/pdf/2502.16159 http://arxiv.org/abs/2502.16159,http://dx.doi.org/10.1109/icdew67478.2025.00038,,10.1109/icdew67478.2025.00038,,,0,006-582-365-388-775; 009-661-190-177-945; 103-800-138-708-264; 142-036-085-872-338,1,true,,green
134-808-976-233-174,<p>Leveraging LLM-Based Sentiment Analysis for Portfolio Optimization with Proximal Policy Optimization</p>,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Kemal Kirtac; G. Germano,,,,,,Portfolio optimization; Portfolio; Computer science; Economics; Financial economics,,,,,,http://dx.doi.org/10.2139/ssrn.5344082,,10.2139/ssrn.5344082,,,0,,1,false,,
135-188-977-728-955,LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading,2025-01-16,2025,preprint,arXiv (Cornell University),,,,Kuan-Ming Liu; Ming-Chih Lo,"Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks.",,,,,Routing (electronic design automation); Computer science; Artificial intelligence; Computer network,,,,,https://arxiv.org/abs/2501.09636,http://dx.doi.org/10.48550/arxiv.2501.09636,,10.48550/arxiv.2501.09636,,,0,,0,true,,green
135-256-900-895-490,"The Impact of Error Message Types in AI Financial Chatbots on Usability, Trust, Preference, and Continued Use Intention",2025-09-30,2025,journal article,Korea Institute of Design Research Society,25082817; 26718375,Korea Institute of Design Research Society,,Han Ul Ji; Jae Young Yun,"<jats:p>As the adoption of AI chatbots continues to expand across the financial industry, the type of message delivered during error situations has been found to influence user experience. This study compared and analyzed the effects of rational and emotional messages in three typical error scenarios of financial chatbots: product guidance, asset inquiry, and loan review. A survey was conducted with 362 individuals who had experience using chatbots, and in-depth interviews were carried out with 10 participants. The analysis revealed that users responded more positively to rational messages that clearly presented the cause of the error and how to resolve it. In particular, statistically significant differences were found in preference and continued use intention depending on the error context. These findings suggest that when designing error messages for financial chatbots, it is important to apply message appeal types appropriately based on the specific context of the situation. This study is meaningful in that it proposes a practical message design strategy to enhance user experience with financial chatbots.</jats:p>",10,3,22,33,Usability; Preference; Psychology; Finance; Computer science; Business; Human–computer interaction; Economics; Microeconomics,,,,,,http://dx.doi.org/10.46248/kidrs.2025.3.22,,10.46248/kidrs.2025.3.22,,,0,,0,false,,
135-895-595-447-579,FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting,2025-09-16,2025,preprint,arXiv (Cornell University),,,,Zijian Zhang; Rong Fu; Yangfan He; Xinze Shen; Yanlong Wang; Xiaojing Du; Haochen You; Jiazhao Shi; Simon Fong,"Financial sentiment analysis (FSA) has attracted significant attention, and recent studies increasingly explore large language models (LLMs) for this field. Yet most work evaluates only classification metrics, leaving unclear whether sentiment signals align with market behavior. We propose FinSentLLM, a lightweight multi-LLM framework that integrates an expert panel of sentiment forecasting LLMs, and structured semantic financial signals via a compact meta-classifier. This design captures expert complementarity, semantic reasoning signal, and agreement/divergence patterns without costly retraining, yielding consistent 3-6% gains over strong baselines in accuracy and F1-score on the Financial PhraseBank dataset. In addition, we also provide econometric evidence that financial sentiment and stock markets exhibit statistically significant long-run comovement, applying Dynamic Conditional Correlation GARCH (DCC-GARCH) and the Johansen cointegration test to daily sentiment scores computed from the FNSPID dataset and major stock indices. Together, these results demonstrate that FinSentLLM delivers superior forecasting accuracy for financial sentiment and further establish that sentiment signals are robustly linked to long-run equity market dynamics.",,,,,Computer science; Sentiment analysis; Finance; Business; Artificial intelligence,,,,,https://arxiv.org/abs/2509.12638,http://dx.doi.org/10.48550/arxiv.2509.12638,,10.48550/arxiv.2509.12638,,,0,,0,true,,green
135-901-329-538-089,"Bioenergy II: Hydrogen Production by Aqueous-Phase ReformingThermochemical Processes Group (GPT), Aragon Institute for Engineering Research (I3A), University of Zaragoza, maría de Luna 3, 50018 Zaragoza, Spain. The authors wish to express their gratitude to the Spanish Ministry of Education and Science (MEC) (Research Project Ref. No.CTQ2007-62841) for providing financial support for the work, as well as for the FPI grant conceded by the MICINN to Ana Valiente (Ref. No.BES-2008-006775).",2018-03-01,2018,dataset,ENERGYO,,De Gruyter,,Ana Valiente; José A. Medrano; Miriam Oliva; Joaquin Ruiz; Lucía García; Jesus Arauzo,The present work is focused on the aqueous-phase reforming of ethylene glycol at 500 K. The influence of the system pressure (27 to 36 bar) and the catalyst weight/ethylene glycol flow rate ratio has been studied using a Pt/Al2O3 research catalyst. A comparison of the latter with a Ni/Al coprecipitated catalyst showed a significant influence on hydrogen and alkane selectivities.,,,,,Group (periodic table); Wish; Humanities; Agricultural science; Library science; Chemistry; Environmental science; Art; Computer science; Organic chemistry; Literature,,,,,,http://dx.doi.org/10.1515/energyo.0033.00030,,10.1515/energyo.0033.00030,,,0,,0,false,,
136-232-948-467-890,Generative artificial intelligence and the future of financial forecasting: Evidence from large language models,2025-12-08,2025,journal article,Risk Governance and Control Financial Markets & Institutions,2077429x; 20774303,Virtus Interpress,Ukraine,Mfon Akpan,"<jats:p>The predictive abilities of generative artificial intelligence (AI) are changing the landscape for analytic workflows across sectors. Nevertheless, its capacity and implications for use cases in high-stakes, non-stationary environments — like financial markets — have been empirically under-researched (Tan et al., 2023; Lin &amp; Marques, 2024). This research paper examines generative AI’s zero-shot forecasting capabilities using two large language model (LLM) architectures, OpenAI’s GPT-4o, and Anthropic’s Claude 3.5 Sonnet, as they forecast stock prices. Specifically, the paper evaluates the LLMs’ predictive powers in terms of actual closing prices for a portfolio of in-use equities across sectors on February 3, 2025. A rigorous quantitative approach is used throughout the analyses. In the results section, standardized metrics including mean absolute error (MAE), root mean squared error (RMSE), mean absolute percentage error (MAPE), correlation scores, and R-squared are calculated to assess predictive accuracy and directional bias. Results show that Claude 3.5 Sonnet outperformed GPT-4o on all accuracy metrics, and also showed better accuracy in forecasting actual movement in the stock market, confirming the study hypothesis and demonstrating performance can vary significantly between LLM architectures (Xu et al., 2024). Further analysis of sector-based performance can be undertaken. The study concludes that while the LLM Claude 3.5 Sonnet does yield encouraging strategic implications for use in investment analytics, there still exist significant challenges in relation to interpretability, model calibration, and model sensitivity to rapidly changing market dynamics.</jats:p>",15,4,142,,,,,,,,http://dx.doi.org/10.22495/rgcv15i4p13,,10.22495/rgcv15i4p13,,,0,027-636-893-541-115; 039-391-276-014-128; 054-035-902-936-780; 054-490-154-540-100; 059-133-374-463-182; 061-896-784-579-855; 066-352-622-965-602; 070-346-924-110-38X; 076-001-340-816-228; 083-737-247-294-176; 090-601-737-341-204; 110-096-666-711-659; 112-705-069-209-103; 119-614-617-928-012; 119-721-023-818-944; 127-317-504-640-88X; 130-272-305-282-006; 140-089-452-493-978; 146-435-477-033-641; 149-467-518-456-006; 153-109-312-761-448; 160-004-396-486-932; 162-085-638-125-145; 167-398-185-509-339; 176-116-467-057-041; 182-022-938-655-911; 185-121-488-620-182; 193-222-672-874-089; 198-636-796-344-356,1,false,,
136-313-662-755-082,"Enhancing Financial Inclusion and Regulatory Challenges: A Critical Analysis of Digital Banks and Alternative Lenders Through Digital Platforms, Machine Learning, and Large Language Models Integration",,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Luke Lee,"This paper explores the dual impact of digital banks and alternative lenders on financial inclusion and the regulatory challenges posed by their business models. It discusses the integration of digital platforms, machine learning (ML), and Large Language Models (LLMs) in enhancing financial services accessibility for underserved populations. Through a detailed analysis of operational frameworks and technological infrastructures, this research identifies key mechanisms that facilitate broader financial access and mitigate traditional barriers. Additionally, the paper addresses significant regulatory concerns involving data privacy, algorithmic bias, financial stability, and consumer protection. Employing a mixed-methods approach, which combines quantitative financial data analysis with qualitative insights from industry experts, this paper elucidates the complexities of leveraging digital technology to foster financial inclusivity. The findings underscore the necessity of evolving regulatory frameworks that harmonize innovation with comprehensive risk management. This paper concludes with policy recommendations for regulators, financial institutions, and technology providers, aiming to cultivate a more inclusive and stable financial ecosystem through prudent digital technology integration.",,,,,Financial inclusion; Inclusion (mineral); Business; Financial services; Computer science; Finance; Psychology; Social psychology,,,,,https://arxiv.org/pdf/2404.11898 https://arxiv.org/abs/2404.11898,http://dx.doi.org/10.2139/ssrn.4799209,,10.2139/ssrn.4799209,,,0,,8,true,,green
137-045-578-913-090,Comparing Open-Source and Commercial LLMs for Domain-Specific Analysis and Reporting: Software Engineering Challenges and Design Trade-offs,2025-09-29,2025,preprint,arXiv (Cornell University),,,,Theo Koraag; Niklas Wagner; Felix Dobslaw; Lucas Gren,"Context: Large Language Models (LLMs) enable automation of complex natural language processing across domains, but research on domain-specific applications like Finance remains limited. Objectives: This study explored open-source and commercial LLMs for financial report analysis and commentary generation, focusing on software engineering challenges in implementation. Methods: Using Design Science Research methodology, an exploratory case study iteratively designed and evaluated two LLM-based systems: one with local open-source models in a multi-agent workflow, another using commercial GPT-4o. Both were assessed through expert evaluation of real-world financial reporting use cases. Results: LLMs demonstrated strong potential for automating financial reporting tasks, but integration presented significant challenges. Iterative development revealed issues including prompt design, contextual dependency, and implementation trade-offs. Cloud-based models offered superior fluency and usability but raised data privacy and external dependency concerns. Local open-source models provided better data control and compliance but required substantially more engineering effort for reliability and usability. Conclusion: LLMs show strong potential for financial reporting automation, but successful integration requires careful attention to architecture, prompt design, and system reliability. Implementation success depends on addressing domain-specific challenges through tailored validation mechanisms and engineering strategies that balance accuracy, control, and compliance.",,,,,Domain (mathematical analysis); Software; Open source; Open source software; Computer science; Engineering; Business; Software engineering; Risk analysis (engineering); Mathematics; Programming language; Mathematical analysis,,,,,https://arxiv.org/abs/2509.24344,http://dx.doi.org/10.48550/arxiv.2509.24344,,10.48550/arxiv.2509.24344,,,0,,0,true,,green
137-172-932-120-539,AI-Based Compliance Chatbots for Financial Institutions,2025-01-01,2025,preprint,,,Elsevier BV,,Garba M.,,,,,,Compliance (psychology); Business; Accounting; Psychology; Social psychology,,,,,,http://dx.doi.org/10.2139/ssrn.5363765,,10.2139/ssrn.5363765,,,0,067-969-919-084-220; 077-748-359-204-426; 103-126-885-281-282; 144-638-940-242-101; 153-092-126-573-63X,0,false,,
137-220-441-015-586,Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment,,2025,conference proceedings article,Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,,Association for Computational Linguistics,,Saizhuo Wang; Hang Yuan; Leon Zhou; Lionel Ni; Heung-Yeung Shum; Jian Guo,,,,196,206,,,,,,,http://dx.doi.org/10.18653/v1/2025.emnlp-demos.14,,10.18653/v1/2025.emnlp-demos.14,,,0,,0,false,,
137-512-687-842-332,QRS Companion - Black-Box Testing of Financial Virtual Assistants,,2020,conference proceedings article,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",,IEEE,,Iosif Itkin; Elena Treshcheva; Luba Konnova; Pavel Braslavski; Rostislav Yavorskiy,"We propose a hybrid technique of black-box testing of virtual assistants (VAs) in the financial sector. The specifics of the highly regulated industry imposes numerous limitations on the testing process: GDPR and other data protection requirements, the absence of interaction logs with real users, restricted access to internal data, etc. These limitations also decrease the applicability of a few VA testing methods that are widely described in the research literature. The approach suggested in this paper consists of semi-controlled interaction logging from the trained testers and subsequent augmenting the collected data for automated testing.",,,684,685,Logging; White-box testing; Process management; Regulated Industry; Financial sector; Restricted access; Research literature; Computer science; Process (engineering); Data Protection Act 1998,,,,,https://dblp.uni-trier.de/db/conf/qrs/qrs2020c.html#ItkinTKBY20 https://doi.org/10.1109/QRS-C51114.2020.00120,http://dx.doi.org/10.1109/qrs-c51114.2020.00120,,10.1109/qrs-c51114.2020.00120,3116430021,,0,012-231-621-130-865; 015-211-825-353-092; 020-709-018-033-462; 045-151-982-226-296; 092-341-880-860-319; 148-155-129-665-083; 175-693-082-794-95X,0,false,,
137-648-399-316-023,A Study on Factors Affecting Intention to Continuous Use Chatbot Services in Financial Institutions,2025-02-25,2025,preprint,,,MDPI AG,,Yeun-su Choi; Seung-zoon Lee; Jeongil Choi,"<jats:p>The South Korean AI market expanded significantly from 2019 to 2024, with AI technologies be-coming increasingly vital in financial services. The rise of non-contact transactions and branch downsizing has shifted customer service towards non-contact interactions, highlighting chatbot services&amp;#039; importance. AI solutions like KB Kookmin Bank&amp;#039;s chatbot &amp;#039;Bibi&amp;#039; enhance customer satis-faction and institutional competitiveness. This study surveyed 250 financial institution chatbot users nationwide in February 2024, using SPSS and R to analyze factors affecting continuous usage in-tention. Results showed social presence, response accuracy, and assurance positively influenced expectation confirmation, while responsiveness had no significant effect. Security and interactivity showed positive effects, but reliability and ease of use were not significant. Usage frequency and education level variations were noted. Repeated use strengthened the relationship between social presence and expectation confirmation, though security concerns increased with usage. This study provides empirical evidence for improving chatbot service quality and user experience, offering strategic insights for non-contact financial services.</jats:p>",,,,,Chatbot; Business; Finance; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.20944/preprints202502.2013.v1,,10.20944/preprints202502.2013.v1,,,0,,0,true,,bronze
137-650-324-785-688,"Leveraging Large Language Models to Democratize Access to Costly
  Financial Datasets for Academic Research",2024-12-02,2024,preprint,arXiv (Cornell University),,,,Julian Junyan Wang; Victor Xiaoqi Wang,"Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.",,,,,Computer science; Finance; Business,,,,,https://arxiv.org/abs/2412.02065,http://dx.doi.org/10.48550/arxiv.2412.02065,,10.48550/arxiv.2412.02065,,,0,,0,true,,green
138-048-647-544-912,FinFlier: Automating Graphical Overlays for Financial Visualizations With Knowledge-Grounding Large Language Model.,,2025,journal article,IEEE transactions on visualization and computer graphics,19410506; 10772626; 21609306,Institute of Electrical and Electronics Engineers (IEEE),United States,Jianing Hao; Manling Yang; Qing Shi; Yuzhe Jiang; Guang Zhang; Wei Zeng,"Graphical overlays that layer visual elements onto charts, are effective to convey insights and context in financial narrative visualizations. However, automating graphical overlays is challenging due to complex narrative structures and limited understanding of effective overlays. To address the challenge, we first summarize the commonly used graphical overlays and narrative structures, and the proper correspondence between them in financial narrative visualizations, elected by a survey of 1752 layered charts with corresponding narratives. We then design FinFlier, a two-stage innovative system leveraging a knowledge-grounding large language model to automate graphical overlays for financial visualizations. The text-data binding module enhances the connection between financial vocabulary and tabular data through advanced prompt engineering, and the graphics overlaying module generates effective overlays with narrative sequencing. We demonstrate the feasibility and expressiveness of FinFlier through a gallery of graphical overlays covering diverse financial narrative visualizations. Performance evaluations and user studies further confirm system's effectiveness and the quality of generated layered charts.",31,9,6353,6369,Computer science; Overlay; Visualization; Data visualization; Programming language; Graphical model; Graphical user interface; Ground; Human–computer interaction; Artificial intelligence; Natural language processing; Physics; Quantum mechanics,,,,National Natural Science Foundation of China; Guangzhou Basic and Applied Basic Research Foundation; Guangzhou Basic and Applied Basic Research Foundation,,http://dx.doi.org/10.1109/tvcg.2024.3514138,40030442,10.1109/tvcg.2024.3514138,,,0,003-317-261-364-318; 007-516-882-620-676; 010-203-672-134-97X; 010-345-452-730-933; 010-366-134-232-077; 017-477-029-227-707; 017-624-583-764-865; 017-812-915-830-354; 018-866-913-622-957; 019-561-260-123-016; 019-608-391-289-644; 021-378-005-903-652; 021-478-957-578-39X; 021-947-429-717-113; 023-031-643-349-289; 023-239-700-392-64X; 025-814-704-881-249; 032-759-973-081-906; 036-233-990-589-532; 049-040-828-603-499; 050-029-414-790-237; 053-547-457-465-820; 057-685-855-374-044; 060-110-287-788-61X; 060-571-389-223-224; 060-764-996-803-766; 063-332-079-825-781; 076-876-219-874-129; 077-007-272-553-289; 079-696-883-470-625; 082-155-951-412-096; 089-834-044-250-539; 090-831-709-889-647; 099-689-382-836-49X; 105-124-649-923-400; 110-269-262-676-861; 118-372-874-328-32X; 122-807-763-958-147; 122-939-075-271-493; 125-545-780-046-731; 137-120-754-193-596; 146-502-509-556-672; 154-042-138-562-576; 166-684-035-436-569; 174-945-876-345-875; 176-404-685-669-018; 179-666-173-153-754,4,false,,
138-060-673-119-209,Efficient Financial Sentiment Analysis via LLM Fine-tuning with Preference Optimization,2025-09-19,2025,conference proceedings article,Proceedings of the 2025 International Symposium on Artificial Intelligence and Computational Social Sciences,,ACM,,Yixuan Ding; Jianxia Sun,,,,678,683,,,,,,,http://dx.doi.org/10.1145/3776759.3776941,,10.1145/3776759.3776941,,,0,050-672-026-725-006; 080-600-379-598-503; 097-178-957-633-427; 107-160-994-672-439; 110-558-988-490-565,0,false,,
138-326-378-661-148,RA-CFGPT: Chinese financial assistant with retrieval-augmented large language model,2024-06-06,2024,journal article,Frontiers of Computer Science,20952228; 20952236,Springer Science and Business Media LLC,Germany,Jiangtong Li; Yang Lei; Yuxuan Bian; Dawei Cheng; Zhijun Ding; Changjun Jiang,,18,5,,,Computer science; Language model; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1007/s11704-024-31018-5,,10.1007/s11704-024-31018-5,,,0,018-866-913-622-957; 066-547-208-006-297; 091-576-618-454-390; 138-326-378-661-148; 142-036-085-872-338,15,false,,
138-700-673-991-876,"FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of
  Large Language Models",2025-01-29,2025,preprint,arXiv (Cornell University),,,,Spencer Mateega; Carlos Georgescu; Danny Tang,"FinanceQA is a testing suite that evaluates LLMs' performance on complex numerical financial analysis tasks that mirror real-world investment work. Despite recent advances, current LLMs fail to meet the strict accuracy requirements of financial institutions, with models failing approximately 60% of realistic tasks that mimic on-the-job analyses at hedge funds, private equity firms, investment banks, and other financial institutions. The primary challenges include hand-spreading metrics, adhering to standard accounting and corporate valuation conventions, and performing analysis under incomplete information - particularly in multi-step tasks requiring assumption generation. This performance gap highlights the disconnect between existing LLM capabilities and the demands of professional financial analysis that are inadequately tested by current testing architectures. Results show that higher-quality training data is needed to support such tasks, which we experiment with using OpenAI's fine-tuning API. FinanceQA is publicly released at [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA).",,,,,Benchmark (surveying); Computer science; Finance; Economics; Geography; Cartography,,,,,https://arxiv.org/abs/2501.18062,http://dx.doi.org/10.48550/arxiv.2501.18062,,10.48550/arxiv.2501.18062,,,0,,0,true,,green
138-764-022-414-234,Automate Strategy Finding with LLM in Quant Investment,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: EMNLP 2025,,Association for Computational Linguistics,,Zhizhuo Kou; Holam Yu; Junyu Luo; Jingshu Peng; Xujia Li; Chengzhong Liu; Juntao Dai; Lei Chen; Sirui Han; Yike Guo,,,,18517,18533,,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-emnlp.1005,,10.18653/v1/2025.findings-emnlp.1005,,,0,,0,false,,
139-375-630-444-960,Automated Financial Analysis Using GPT-4,2023-09-17,2023,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Sander Noels; Adriaan Merlevede; Andrew Fecheyr; Maarten Vanhalst; Nick Meerlaen; Sébastien Viaene; Tijl De Bie,,,,345,349,Computer science; Advice (programming); Process (computing); Financial analysis; Software engineering; Accounting; Cloud computing; Service (business); Finance; Programming language; Operating system; Business; Marketing,,,,,,http://dx.doi.org/10.1007/978-3-031-43430-3_28,,10.1007/978-3-031-43430-3_28,,,0,024-227-652-072-585,2,false,,
139-393-948-664-014,DeepFinLLM: An Intelligent Financial Advisor Unleashing Strategic Insights with Large Language Models,2025-04-08,2025,preprint,,,Springer Science and Business Media LLC,,Veerababu Reddy; Veeranjaneyulu N,"<title>Abstract</title>;         <p>Generative AI breakthroughs have reshaped financial analytics, introducing seamless, language-based interactions that expand access to valuable insights. Traditionally, financial analysis relied on rigid, rule-driven tools requiring specialized skills, often yielding 75% accuracy and faltering with complex queries. DeepFinLLM overcomes these barriers by blending advanced language understanding with real-time financial data, offering precise responses with a 94% accuracy rate and 0.8-second delivery time. Merging artificial intelligence with dynamic market data, it provides adaptability and scalability across diverse financial contexts. Users, from expert investors to novices, praise its intuitive interface, with 90% valuing its fast, reliable insights. Handling structured financial information and open-ended inquiries, DeepFinLLM facilitates deep analysis of investment options, market trends, and risks, enhanced by real-time updates for relevance in volatile markets. By simplifying complex tasks and encouraging data-informed decisions, DeepFinLLM transforms financial analytics to serve a broader audience effectively.</p>",,,,,Business; Computer science,,,,,,http://dx.doi.org/10.21203/rs.3.rs-6376312/v1,,10.21203/rs.3.rs-6376312/v1,,,0,003-779-718-676-52X; 009-584-335-204-187; 014-860-558-634-431; 017-617-450-871-282; 019-191-867-506-27X; 020-669-268-861-638; 033-248-933-429-703; 039-374-358-392-515; 049-914-115-888-345; 050-984-507-623-134; 054-351-402-628-890; 054-757-685-326-322; 071-025-591-238-412; 077-853-885-798-038; 088-549-109-891-224; 089-287-414-963-713; 091-315-830-293-250; 098-145-181-016-32X; 098-547-712-222-248; 113-053-510-620-602; 118-630-775-277-538; 139-430-138-864-305; 147-561-141-902-78X; 169-238-100-123-761; 178-643-760-848-034; 186-670-103-221-040; 198-526-441-457-282,1,false,,
139-451-456-781-487,Large Language Models for Financial Aid in Financial Time-series Forecasting,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Md Khairul Islam; Ayush Karmacharya; Timothy Sue; Judy Fox,,,,4892,4895,Finance; Series (stratigraphy); Computer science; Financial modeling; Time series; Economics; Machine learning; Paleontology; Biology,,,,,,http://dx.doi.org/10.1109/bigdata62323.2024.10824953,,10.1109/bigdata62323.2024.10824953,,,0,006-701-602-294-537; 012-322-862-979-222; 016-103-771-422-600; 024-481-362-055-895; 050-568-161-714-510; 054-035-902-936-780; 069-507-976-179-943; 098-428-380-981-106; 115-609-180-775-508; 120-540-079-340-938; 160-803-037-227-737,1,false,,
139-511-011-278-055,Can LLMs Answer Investment Banking Questions? Using Domain-Tuned Functions to Improve LLM Performance on Knowledge-Intensive Analytical Tasks,2024-05-20,2024,journal article,Proceedings of the AAAI Symposium Series,29944317,Association for the Advancement of Artificial Intelligence (AAAI),,Nicholas Harvel; Felipe Bivort Haiek; Anupriya Ankolekar; David James Brunner,"<jats:p>Large Language Models (LLMs) can increase the productivity of general-purpose knowledge work, but accuracy is a concern, especially in professional settings requiring domain-specific knowledge and reasoning. To evaluate the suitability of LLMs for such work, we developed a benchmark of 16 analytical tasks representative of the investment banking industry. We evaluated LLM performance without special prompting, with relevant information provided in the prompt, and as part of a system giving the LLM access to domain-tuned functions for information retrieval and planning.  Without access to functions, state-of-the-art LLMs performed poorly, completing two or fewer tasks correctly. Access to appropriate domain-tuned functions yielded dramatically better results, although performance was highly sensitive to the design of the functions and the structure of the information they returned. The most effective designs yielded correct answers on 12 out of 16 tasks. Our results suggest that domain-specific functions and information structures, by empowering LLMs with relevant domain knowledge and enabling them to reason in domain-appropriate ways, may be a powerful means of adapting LLMs for use in demanding professional settings.</jats:p>",3,1,125,133,Domain (mathematical analysis); Benchmark (surveying); Investment (military); Computer science; Work (physics); Productivity; Knowledge management; Risk analysis (engineering); Business; Engineering; Political science; Economics; Economic growth; Mathematics; Mathematical analysis; Politics; Law; Mechanical engineering; Geodesy; Geography,,,,,https://ojs.aaai.org/index.php/AAAI-SS/article/download/31191/33351 https://doi.org/10.1609/aaaiss.v3i1.31191,http://dx.doi.org/10.1609/aaaiss.v3i1.31191,,10.1609/aaaiss.v3i1.31191,,,0,,11,true,,bronze
139-574-191-143-817,Evaluating LLMs in Financial Tasks - Code Generation in Trading Strategies,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Miquel Noguer i Alonso; Hanane Dupouy,,,,,,Code (set theory); Business; Finance; Computer science; Programming language; Set (abstract data type),,,,,,http://dx.doi.org/10.2139/ssrn.4752797,,10.2139/ssrn.4752797,,,0,,0,false,,
139-904-521-146-447,From news to trends: a financial time series forecasting framework with LLM-driven news sentiment analysis and selective state spaces,2025-08-02,2025,journal article,Journal of Intelligent Information Systems,09259902; 15737675,Springer Science and Business Media LLC,Netherlands,Renjie Wang; Minghui Sun; Limin Wang,,63,6,1955,1980,Computer science; Sentiment analysis; Time series; Series (stratigraphy); State (computer science); News analytics; Data science; Artificial intelligence; Finance; Machine learning; Algorithm; Economics; Paleontology; Biology,,,,Program of Science and Technol- ogy Development Plan of Jilin Province of China; Shenzhen Science and Technology Program,https://www.researchsquare.com/article/rs-6277319/latest.pdf https://doi.org/10.21203/rs.3.rs-6277319/v1,http://dx.doi.org/10.1007/s10844-025-00971-3,,10.1007/s10844-025-00971-3,,,0,005-825-321-195-629; 006-701-602-294-537; 011-705-191-613-247; 013-866-960-992-449; 015-011-888-895-810; 018-110-669-944-734; 020-907-912-991-03X; 031-285-435-745-666; 043-803-967-841-07X; 046-080-756-564-802; 053-094-537-530-97X; 060-300-769-233-251; 064-259-526-107-879; 069-840-166-570-085; 071-423-367-050-689; 082-178-119-516-788; 089-452-908-421-259; 092-230-389-714-678; 103-985-276-196-90X; 104-943-960-749-41X; 105-933-274-359-013; 106-176-973-499-980; 109-184-420-030-073; 112-154-051-246-370; 121-823-142-181-244; 132-368-063-988-776; 154-042-138-562-576; 165-508-164-582-15X; 178-508-649-938-646; 187-335-338-731-22X; 195-913-058-104-530; 199-711-961-147-473,2,true,,green
139-911-164-999-120,"Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading
  Agents",2025-02-22,2025,preprint,arXiv (Cornell University),,,,David Byrd,"Companies across all economic sectors continue to deploy large language models at a rapid pace. Reinforcement learning is experiencing a resurgence of interest due to its association with the fine-tuning of language models from human feedback. Tool-chain language models control task-specific agents; if the converse has not already appeared, it soon will. In this paper, we present what we believe is the first investigation of an intelligent trading agent based on continuous deep reinforcement learning that also controls a large language model with which it can post to a social media feed observed by other traders. We empirically investigate the performance and impact of such an agent in a simulated financial market, finding that it learns to optimize its total reward, and thereby augment its profit, by manipulating the sentiment of the posts it produces. The paper concludes with discussion, limitations, and suggestions for future work.",,,,,Computer science; Sentiment analysis; Data science; Artificial intelligence,,,,,https://arxiv.org/abs/2502.16343,http://dx.doi.org/10.48550/arxiv.2502.16343,,10.48550/arxiv.2502.16343,,,0,,0,true,,green
140-118-405-665-933,"The Application of Large Language Models Reducing Cultural Barriers in International Trade: A Perspective from Cultural Conflicts, Potential and Obstacles",2024-04-03,2024,journal article,Lecture Notes in Education Psychology and Public Media,27537048; 27537056,EWA Publishing,,Zhaokai Liang; Yuexi Liu; Yihao Luo,"<jats:p>Large Language Models (LLMs) are powerful Artificial General Intelligence (AGI) software that generate natural language texts on various topics and domains. They have great potential to reduce cultural barriers in international trade by facilitating machine translation and cross-cultural communication. However, they also face cultural conflicts, challenges and risks, such as cultural misreading, bias, and ethical issues. This paper provides a comprehensive analysis of the current state and future prospects of LLMs in international trade, and discusses the cultural differences, opportunities and obstacles that they encounter. It also proposes some possible solutions and directions for improving the cultural sensitivity and performance of LLMs, such as incorporating cultural knowledge, enhancing cultural awareness, and ensuring cultural diversity and inclusivity. To illustrate the practical implications of LLMs in international trade, this paper provides some examples of how LLMs have been used or could be used in various trade scenarios, such as ChatGPT,NewBing and Ernie Bot. This paper aims to offer some insights and suggestions for researchers, practitioners, and policymakers who are interested in or involved in the development and application of LLMs in international trade, and to demonstrate how LLMs can contribute to the advancement and innovation of international trade.</jats:p>",47,1,33,37,Cultural diversity; Cultural bias; Political science; Sociology; Engineering ethics; Law; Engineering,,,,,https://www.ewadirect.com/proceedings/lnep/article/view/11176/pdf https://doi.org/10.54254/2753-7048/47/20240875,http://dx.doi.org/10.54254/2753-7048/47/20240875,,10.54254/2753-7048/47/20240875,,,0,,0,true,cc-by,hybrid
140-456-177-766-151,Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task,,2025,journal article,IEEE Transactions on Big Data,23327790; 23722096,Institute of Electrical and Electronics Engineers (IEEE),,Zihao Wu; Lu Zhang; Chao Cao; Xiaowei Yu; Zhengliang Liu; Lin Zhao; Yiwei Li; Haixing Dai; Chong Ma; Gang Li; Wei Liu; Quanzheng Li; Dinggang Shen; Xiang Li; Dajiang Zhu; Tianming Liu,,11,3,1027,1041,,,,,,,http://dx.doi.org/10.1109/tbdata.2025.3536928,,10.1109/tbdata.2025.3536928,,,0,005-407-048-718-561; 022-834-724-786-870; 034-812-703-095-921; 036-550-103-088-82X; 039-921-176-135-067; 062-557-820-103-652; 076-710-517-181-881; 083-449-940-348-930; 089-377-001-338-863; 091-870-270-471-700; 096-372-442-893-804; 104-509-300-064-931; 116-410-677-994-43X; 123-893-513-434-359; 126-908-237-876-385; 137-548-243-845-043; 140-921-627-035-630; 141-009-673-958-971; 142-862-806-335-247; 145-631-585-677-091; 154-729-410-251-285,7,false,,
141-121-419-045-712,AI-Powered Financial Reporting Chatbots: Transforming Insurance Industry Operations,2024-12-19,2024,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,null Chetan Prakash Ratnawat,"<jats:p>This comprehensive article examines the transformative impact of AI-powered financial chatbots in the insurance industry, focusing on the evolution from traditional manual processes to automated solutions. The article investigates technical architecture, role-specific functionality, security frameworks, and performance analytics across major insurance organizations worldwide. Through extensive research and analysis, this article demonstrates how intelligent automation addresses traditional inefficiencies in insurance operations while enhancing data accuracy, improving security protocols, and delivering substantial operational benefits. The findings reveal that insurance companies implementing these advanced systems experience significant improvements in underwriting efficiency, agent productivity, security compliance, and overall operational effectiveness while maintaining robust data protection standards and regulatory compliance.</jats:p>",10,6,1779,1786,Insurance industry; Business; Finance; Actuarial science,,,,,,http://dx.doi.org/10.32628/cseit241061222,,10.32628/cseit241061222,,,0,028-711-360-020-02X; 029-445-150-946-243,0,true,,gold
141-361-672-077-702,"Your AI, Not Your View: The Bias of LLMs in Investment Analysis",2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Hoyoung Lee; Junhyuk Seo; Suhwan Park; Junhyeong Lee; Wonbin Ahn; Chanyeol Choi; Alejandro Lopez-Lira; Yongjae Lee,,,,150,158,,,,,National Research Foundation of Korea; Institute of Information & Communications Technology Planning & Evaluation(IITP),,http://dx.doi.org/10.1145/3768292.3770375,,10.1145/3768292.3770375,,,0,024-891-863-583-217; 026-984-186-973-562; 045-500-877-711-153; 065-974-635-780-500; 084-028-598-567-636; 100-148-848-739-715; 132-092-780-398-264; 132-923-717-863-009; 170-629-592-611-596; 174-163-050-125-140; 186-827-515-649-986; 190-209-106-250-591; 198-573-119-668-956,0,false,,
141-590-989-587-566,Hey ChatGPT—Is a Louis Vuitton Bag an Investment? Evaluating LLM Readiness for Use in Financial Literacy and Education,2025-07-01,2025,journal article,Journal of Emerging Technologies in Accounting,15541908; 15587940,American Accounting Association,United States,Stacey Taylor; Samantha Taylor; Shannon Lin; Vlado Keselj,"<jats:title>ABSTRACT</jats:title>;                <jats:p>The prevalence of large language models (LLMs) such as ChatGPT has wowed the world with its ability to generate text in a human-like manner. While educators evaluate how AI will impact the future of learning, we identify mistakes ChatGPT has made. We further extend this concern to nonfinancially sophisticated users seeking to improve their financial literacy who may not possess the financial acumen to determine when the AI is hallucinating. Using a longitudinal study, our analysis frames the prompts and subsequent findings within the four stages of the Dunning-Kruger effect to explore how users of varying expertise receive output from the LLMs. We find that ChatGPT cannot always fully distinguish between three different user groups. Our findings have important implications for accountants, educators, and students using LLMs as a tool in work and education and for the general population looking to bypass financial experts for their personal finance needs.</jats:p>;                <jats:p>Data Availability: Data will be made available upon request.</jats:p>;                <jats:p>JEL Classifications: M41.</jats:p>",,,1,21,Investment (military); Financial literacy; Finance; Business; Psychology; Political science; Politics; Law,,,,,,http://dx.doi.org/10.2308/jeta-2023-066,,10.2308/jeta-2023-066,,,0,,0,false,,
141-818-302-010-751,"""Give Me BF16 or Give Me Death""? Accuracy-Performance Trade-Offs in LLM Quantization",,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Association for Computational Linguistics,,Eldar Kurtic; Alexandre Noll Marques; Shubhra Pandit; Mark Kurtz; Dan Alistarh,,,,26872,26886,Quantization (signal processing); Computer science; Algorithm,,,,,,http://dx.doi.org/10.18653/v1/2025.acl-long.1304,,10.18653/v1/2025.acl-long.1304,,,0,,1,false,,
141-887-934-821-615,Mixture-of-Experts Based LLM Model For Financial Text Classification,2024-10-18,2024,conference proceedings article,Proceeding of the 2024 5th International Conference on Computer Science and Management Technology,,ACM,,Yibin Lu; Leyang Liu; Rong Nie,,,,483,486,Computer science; Artificial intelligence; Natural language processing,,,,,,http://dx.doi.org/10.1145/3708036.3708118,,10.1145/3708036.3708118,,,0,015-011-888-895-810; 035-522-629-246-285; 098-997-433-804-411; 107-361-324-993-533,1,false,,
141-959-493-309-287,Leveraging LLAMA for Financial Chatbots: Domain-Specific Fine-Tuning and Performance Evaluation,2025-03-03,2025,journal article,AVE Trends in Intelligent Management Letters,30691117,AVE Trends Publishing Company,,L. Saikrishna; S. Sreman Narayana; P. Cheenu Sriyan,"<jats:p>Large Language Models (LLMs) have revolutionised the landscape of natural language processing (NLP), offering sophisticated conversational capabilities across various domains. This paper explores the adaptation of Meta’s LLaMA model for financial chatbot applications, emphasising domain-specific fine-tuning and performance evaluation. Fine-tuning LLaMA for finance requires specialised datasets, encompassing market trends, financial regulations, and investment strategies to enhance contextual understanding and response accuracy. Key aspects of this process include data curation, supervised fine-tuning, and reinforcement learning techniques, which aim to align model outputs with financial reasoning and industry standards. Furthermore, evaluation metrics such as perplexity, response coherence, and financial sentiment analysis are examined to gauge chatbot effectiveness. By integrating domain-specific knowledge, LLaMA-powered financial chatbots can provide users with more precise, context-aware insights, facilitating tasks such as portfolio management, risk assessment, and regulatory compliance. Advancements in retrieval-augmented generation (RAG) and model distillation further optimise performance, ensuring efficiency and reliability in financial applications. The paper also addresses ethical considerations, including bias mitigation and regulatory compliance, to promote the responsible deployment of AI in the financial services sector.</jats:p>",1,1,49,58,Computer science; Domain (mathematical analysis); Business; Finance; Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.64091/atiml.2025.000100,,10.64091/atiml.2025.000100,,,0,,0,false,,
142-165-226-692-498,Firm-level trade policy effect uncertainty and cost stickiness: Evidence from a large language model approach ,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Shuyang Jia; Peng Liang,,,,,,Download; Computer science; World Wide Web,,,,,,http://dx.doi.org/10.2139/ssrn.4759122,,10.2139/ssrn.4759122,,,0,006-929-811-507-325; 008-520-000-670-523; 009-185-726-244-241; 009-661-197-536-269; 013-484-544-181-336; 017-807-119-315-611; 018-722-421-791-372; 020-002-785-753-752; 028-095-620-024-822; 036-979-852-259-747; 037-972-396-238-575; 038-242-929-429-895; 038-849-931-554-776; 041-649-340-724-560; 041-783-304-053-672; 053-608-093-717-017; 061-978-873-940-410; 065-666-404-377-357; 074-375-256-634-556; 079-200-702-740-71X; 082-178-119-516-788; 084-866-125-470-626; 086-936-635-444-034; 092-022-643-764-122; 093-929-980-366-688; 096-696-449-401-906; 097-831-471-117-528; 103-247-865-872-131; 104-373-107-404-359; 104-684-853-775-918; 109-384-106-387-724; 111-255-013-433-673; 111-459-997-118-741; 125-247-294-429-870; 127-379-617-155-518; 128-021-308-035-029; 129-767-632-180-962; 130-533-182-179-596; 132-244-163-847-933; 140-459-735-291-941; 154-042-138-562-576; 158-481-374-319-455; 159-614-024-183-351; 160-828-754-627-734,0,false,,
142-260-648-024-452,"ECC Analyzer: Extract Trading Signal from Earnings Conference Calls
  using Large Language Model for Stock Performance Prediction",2024-04-29,2024,preprint,arXiv (Cornell University),,,,Yupeng Cao; Zhi Chen; Qingyun Pei; Nathan Jinseok Lee; K. P. Subbalakshmi; Papa Momar Ndiaye,"In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock performance is a critical challenge that has attracted both academics and investors. While previous studies have used deep learning-based models to obtain a general view of ECCs, they often fail to capture detailed, complex information. Our study introduces a novel framework: \textbf{ECC Analyzer}, combining Large Language Models (LLMs) and multi-modal techniques to extract richer, more predictive insights. The model begins by summarizing the transcript's structure and analyzing the speakers' mode and confidence level by detecting variations in tone and pitch for audio. This analysis helps investors form an overview perception of the ECCs. Moreover, this model uses the Retrieval-Augmented Generation (RAG) based methods to meticulously extract the focuses that have a significant impact on stock performance from an expert's perspective, providing a more targeted analysis. The model goes a step further by enriching these extracted focuses with additional layers of analysis, such as sentiment and audio segment features. By integrating these insights, the ECC Analyzer performs multi-task predictions of stock performance, including volatility, value-at-risk (VaR), and return for different intervals. The results show that our model outperforms traditional analytic benchmarks, confirming the effectiveness of using advanced LLM techniques in financial analytics.",,,,,Earnings; Spectrum analyzer; Computer science; Stock (firearms); Speech recognition; Accounting; Econometrics; Business; Economics; Telecommunications; Engineering; Mechanical engineering,,,,,https://arxiv.org/abs/2404.18470,http://dx.doi.org/10.48550/arxiv.2404.18470,,10.48550/arxiv.2404.18470,,,0,,0,true,,green
142-405-196-368-522,Agent-Based Simulation of a Financial Market with Large Language Models,2025-10-14,2025,preprint,arXiv (Cornell University),,,,Ryuji Hashimoto; Takehiro Takayanagi; Masahiro Suzuki; Kiyoshi Izumi,"In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.",,,,,Financial market; Financial modeling; Computer science; Finance; Business; Financial system,,,,,https://arxiv.org/abs/2510.12189,http://dx.doi.org/10.48550/arxiv.2510.12189,,10.48550/arxiv.2510.12189,,,0,,0,true,,green
142-821-844-384-372,LLM-Driven Qualitative Portfolio Evolution and Correlation-Aware Cluster Tournaments,,2025,preprint,,,Elsevier BV,,Kamer Ali Yuksel; Hassan Sawaf,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5835462,,10.2139/ssrn.5835462,,,0,,0,false,,
143-453-501-848-85X,Implementing large language model and retrieval augmented generation to extract geographic locations of illicit transnational kidney trade.,2025-04-28,2025,journal article,International journal of health geographics,1476072x,Springer Science and Business Media LLC,United Kingdom,Zifu Wang; Meng-Hao Li; Patrick Baxter; Olzhas Zhorayev; Jiaxin Wei; Valerie Kovacs; Qiuhan Zhao; Chaowei Yang; Naoru Koizumi,"<AbstractText Label=""BACKGROUND"">Illicit kidney trade networks, operating globally, involve intricate interactions among various players, most notably buyers, sellers, brokers, and surgeons. A comprehensive understanding of these trade networks is, however, hindered by the lack of systematically amassed data for analysis. Further, extracting the geographic locations of buyers, sellers, brokers, transplant surgeons, and medical facilities in all relevant publications often involves extensive, time-consuming, manual labelling that is very costly. Although current techniques such as Named Entity Recognition (NER) tools can potentially automate the process, they are limited to identifying country names and often fail to associate the roles (i.e., offering buyer, seller, broker and/or surgery) that each country played.</AbstractText>;           <AbstractText Label=""METHODS"">This study employed state-of-the-art technologies, including Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-Trained Transformers (GPT) model Llama3.3 from Meta in developing a kidney trade country database. We first extracted news articles reporting illicit kidney trade from the LexisNexis database (2000-2022). BERT and Llama3.3 with chain-of-thought prompt tuning strategies were then applied to the materials to determine the relevance of articles to the illegal kidney trade and to identify the roles those different countries played in kidney trade cases over the past 23 years. The specific country classes recorded in the final kidney trade database included: a) countries of origin for kidney sellers; b) countries of origin of kidney buyers; c) countries performing illegal transplant surgeries; and d) countries of origin of organ trafficking brokers.</AbstractText>;           <AbstractText Label=""RESULTS"">The BERT classification model achieved an accuracy of 88.75%, ensuring that only relevant articles were analyzed. Additionally, the Llama3.3-70B model with chain-of-thought prompt tuning strategies extracted location-based roles with an accuracy of 86.30% for sellers, 88.89% for buyers, 93.33% for brokers, and 95.93% for surgeries, supporting these observed patterns. We observed in the final database that the kidney trade networks change and evolve dynamically where the primary role played by each country (as a host of either sellers, buyers or surgeries) change over time. About half of the top 10 countries playing each role gets replaced by other countries within a decade. The final database also demonstrated that developing countries were more likely to be a host of kidney sellers while that played by developed countries was a host of kidney buyers.</AbstractText>;           <AbstractText Label=""CONCLUSION"">The current study developed a geospatial database describing transnational kidney trade country networks over the past two decades. The new approach for geographic location extraction that is more precise compared to conventional NER and machine learning methods.</AbstractText>;           <CopyrightInformation>© 2025. The Author(s).</CopyrightInformation>",24,1,10,,Health geography; Health informatics; Public health; Human geography; Computer science; Environmental health; Geography; Medicine; Health policy; Pathology; International health; Economic geography,Geographic feature extraction; Geospatial data; Kidney transplantation; Large language model; Media; Organ trafficking; Public health; Retrieval augmented generation,Humans; Commerce; Kidney Transplantation/economics; Tissue and Organ Procurement; Internationality; Large Language Models,,National Science Foundation (1838306),,http://dx.doi.org/10.1186/s12942-025-00397-8,40296001,10.1186/s12942-025-00397-8,,PMC12039186,0,001-350-014-344-24X; 003-543-317-486-150; 004-682-753-593-151; 007-095-344-592-361; 009-596-626-150-009; 011-973-972-540-934; 017-961-331-828-92X; 021-765-870-811-170; 022-881-852-558-559; 027-955-560-697-953; 028-370-828-620-707; 036-728-260-782-44X; 050-745-842-312-802; 050-822-377-212-414; 051-498-801-269-247; 055-898-101-322-172; 056-489-605-410-466; 058-069-814-437-229; 060-026-194-851-569; 060-659-697-152-305; 065-010-405-236-060; 068-208-739-873-767; 079-110-316-483-496; 088-710-088-105-754; 092-466-885-644-120; 094-132-956-892-387; 097-513-477-483-736; 097-526-028-492-947; 103-328-953-576-45X; 105-007-786-302-78X; 110-836-462-256-691; 122-644-506-249-418; 128-122-518-736-141; 145-361-159-924-443; 145-957-595-743-822; 147-004-234-102-35X; 162-630-973-817-801; 171-322-326-364-326; 182-376-514-177-686; 184-575-445-816-417,0,true,"CC BY, CC BY-NC-ND",gold
143-475-798-634-074,Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models,2025-09-29,2025,preprint,arXiv (Cornell University),,,,Durgesh Nandini; Rebekka Koch; Mirco Schoenfeld,"This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.",,,,,Extraction (chemistry); Computer science; Business; Chemistry; Chromatography,,,,,https://arxiv.org/abs/2510.05121,http://dx.doi.org/10.48550/arxiv.2510.05121,,10.48550/arxiv.2510.05121,,,0,,0,true,,green
143-508-918-460-995,A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain,2025-10-15,2025,preprint,arXiv (Cornell University),,,,William Flanagan; Mukunda Das; Rajitha Ramanayake; Swanuja Maslekar; Meghana Mangipudi; Joong Ho Choi; Shruti Nair; Shambhavi Bhusan; Sanjana Dulam; Mouni Pendharkar; Nidhi Singh; Vashisth Doshi; Sachi Shah Paresh,"As Generative Artificial Intelligence is adopted across the financial services industry, a significant barrier to adoption and usage is measuring model performance. Historical machine learning metrics can oftentimes fail to generalize to GenAI workloads and are often supplemented using Subject Matter Expert (SME) Evaluation. Even in this combination, many projects fail to account for various unique risks present in choosing specific metrics. Additionally, many widespread benchmarks created by foundational research labs and educational institutions fail to generalize to industrial use. This paper explains these challenges and provides a Risk Assessment Framework to allow for better application of SME and machine learning Metrics",,,,,Metric (unit); Domain (mathematical analysis); Business; Risk analysis (engineering); Economics; Operations management; Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2510.13524,http://dx.doi.org/10.48550/arxiv.2510.13524,,10.48550/arxiv.2510.13524,,,0,,0,true,,green
143-808-308-899-110,LLM-guided semantic feature selection for interpretable financial market forecasting in low-resource financial markets,,2025,journal article,Franklin Open,27731863,Elsevier BV,,Ouyang Mutian; J. Joshua Thomas; Yu Tianzhou; Ugo Fiore,,12,,100359,100359,Financial market; Feature selection; Selection (genetic algorithm); Business; Finance; Feature (linguistics); Resource (disambiguation); Economics; Artificial intelligence; Computer science; Linguistics; Computer network; Philosophy,,,,,,http://dx.doi.org/10.1016/j.fraope.2025.100359,,10.1016/j.fraope.2025.100359,,,0,039-901-167-873-051; 112-705-069-209-103; 186-638-599-366-752,0,true,"CC BY, CC BY-NC-ND",gold
144-810-608-416-335,Demystifying Domain-adaptive Post-training for Financial LLMs,,2025,conference proceedings article,Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing,,Association for Computational Linguistics,,Zixuan Ke; Yifei Ming; Xuan-Phi Nguyen; Caiming Xiong; Shafiq Joty,,,,31021,31047,,,,,,,http://dx.doi.org/10.18653/v1/2025.emnlp-main.1579,,10.18653/v1/2025.emnlp-main.1579,,,0,,0,false,,
145-188-774-073-357,Information Quality Dimensions in Generative Conversational AI for Financial Inclusion,2024-04-19,2024,conference proceedings article,2024 Sixth International Conference on Computational Intelligence and Communication Technologies (CCICT),,IEEE,,Kanchan Patil; Dhanya Pramod; Mugdha Kulkarni,,,,64,68,Financial inclusion; Inclusion (mineral); Generative grammar; Quality (philosophy); Computer science; Natural language processing; Artificial intelligence; Financial services; Finance; Business; Psychology; Social psychology; Philosophy; Epistemology,,,,,,http://dx.doi.org/10.1109/ccict62777.2024.00021,,10.1109/ccict62777.2024.00021,,,0,009-992-674-917-387; 019-459-647-340-997; 041-865-602-683-794; 042-381-345-295-330; 043-340-923-416-938; 045-842-196-133-132; 049-554-420-982-411; 049-814-277-642-614; 077-037-646-048-664; 090-133-159-291-422; 105-122-378-538-260; 108-977-169-382-995; 140-122-221-463-888; 141-344-840-374-526; 157-896-810-473-001; 161-163-126-260-743; 165-397-955-365-531; 177-604-480-377-399,1,false,,
145-224-062-061-329,GPT for Financial Advice,2023-01-01,2023,book,Zenodo (CERN European Organization for Nuclear Research),,,,Thomas Ankenbrand; Denis Bieri; Levin Reichmuth; Cornelia Stengel; Stephanie Wickihalder; Ahmet Ege Yilmaz,,,,,,Advice (programming); Business; Finance; Computer science; Programming language,,,,,https://zenodo.org/record/8337207,http://dx.doi.org/10.5281/zenodo.8337207,,10.5281/zenodo.8337207,,,0,,0,true,cc-by-nc-nd,green
145-239-685-578-772,"Summary October 2015 米国版『Journal of Financial Planning』 リタイアメント・インカム・プランニング 顧客の金融リテラシー測定テスト : ""Retirement Income Planning Literacy in America : A Method for Determining Retirement Knowledgeable Clients"" by Jamie Hopkins (J.D., LLM., CLU, RICP, and David A. Littell (J.D., ChFC)より",,2016,journal article,Journal of financial planning,10403981,,,Jamie Patrick Hopkins; David Littell,,18,192,44,47,Economics,,,,,http://ci.nii.ac.jp/naid/40020695126,http://ci.nii.ac.jp/naid/40020695126,,,2262777371,,0,,2,false,,
145-350-056-844-957,Designing an artificial intelligence-enabled large language model for financial decisions,2025-01-31,2025,journal article,Management Decision,00251747; 17586070,Emerald,United Kingdom,Anshul Saxena; Bikramjit Rishi,"<jats:sec>;                     <jats:title>Purpose</jats:title>;                     <jats:p>Artificial intelligence (AI) has profoundly reshaped financial decision-making, introducing a paradigm shift in how institutions and individuals navigate the complex finance landscape. The study evaluates the significant impact of integrating advanced AI and large language models (LLMs) in financial decision analytics.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Design/methodology/approach</jats:title>;                     <jats:p>The study offers FinSageNet, a novel framework designed and tested to harness the potential of LLMs in financial decisions. The framework excels in handling and analyzing large volumes of numerical and textual data through advanced data mining techniques.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Findings</jats:title>;                     <jats:p>FinSageNet demonstrates exceptional text summarization capabilities, outperforming models like FLAN and GPT-3.5 in Rouge score metrics. The proposed model has shown more accuracy than generic models.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Originality/value</jats:title>;                     <jats:p>The study emphasizes the significance of consistently updating models and adopting a comprehensive approach to integrating AI into financial decisions. This study improves our understanding of how artificial intelligence transforms financial analytics and decision-making processes.</jats:p>;                   </jats:sec>",63,12,4135,4153,Financial modeling; Business; Finance; Computer science,,,,,,http://dx.doi.org/10.1108/md-02-2024-0305,,10.1108/md-02-2024-0305,,,0,001-260-152-365-947; 003-563-818-368-740; 004-869-083-855-625; 005-268-112-511-884; 005-769-438-917-222; 007-756-270-172-123; 007-880-666-848-845; 009-169-756-435-962; 011-442-236-895-89X; 012-138-944-352-618; 012-252-799-381-912; 016-591-795-666-218; 019-711-284-600-64X; 021-333-882-515-767; 021-603-734-712-176; 022-790-960-737-921; 023-368-484-631-490; 025-226-999-331-357; 025-537-579-100-067; 027-870-773-115-918; 029-660-678-292-008; 029-934-202-382-10X; 033-881-247-029-957; 035-076-182-421-843; 037-051-987-842-778; 037-998-029-297-827; 041-869-421-188-370; 042-468-331-919-992; 045-237-297-330-324; 054-549-390-006-798; 055-794-758-962-071; 057-315-573-154-299; 066-570-493-159-502; 072-262-568-114-580; 073-466-076-850-937; 075-116-990-908-304; 081-248-927-861-031; 082-165-607-961-415; 086-354-238-807-647; 086-460-832-556-27X; 087-741-089-875-633; 102-283-347-133-605; 102-705-597-705-968; 105-943-471-134-146; 107-162-277-677-308; 121-097-663-504-316; 122-520-420-865-426; 123-685-434-578-374; 123-808-769-838-198; 127-050-062-784-340; 130-601-242-425-990; 132-074-584-759-430; 139-239-027-110-484; 142-386-886-898-466; 142-556-459-252-950; 144-485-370-962-320; 162-217-666-453-818; 162-702-245-589-505; 163-088-208-596-748; 163-129-887-298-934; 176-717-981-991-842; 190-262-917-948-953; 192-422-362-462-804; 193-379-694-654-250,5,false,,
145-449-990-235-915,"Open-FinLLMs: Open Multimodal Large Language Models for Financial
  Applications",2024-08-20,2024,preprint,arXiv (Cornell University),,,,Jimin Huang; Mengxi Xiao; Dong Li; Zihao Jiang; Yuzhe Yang; Yifei Zhang; Lingfei Qian; Yan Wang; Xueqing Peng; Yang Ren; Ruoyu Xiang; Zhengyu Chen; Xiao Zhang; Yueru He; Weiguang Han; Shunian Chen; Lihang Shen; Daniel Kim; Yangyang Yu; Yupeng Cao; Zhiyang Deng; Haohang Li; Duanyu Feng; Yongfu Dai; VijayaSai Somasundaram; Peng Lu; Guojun Xiong; Zhiwei Liu; Zheheng Luo; Zhiyuan Yao; Ruey-Ling Weng; Meikang Qiu; Kaleb E Smith; Honghai Yu; Yanzhao Lai; Min Peng; Jian-Yun Nie; Jordan W. Suchow; Xiao-Yang Liu; Benyou Wang; Alejandro Lopez-Lira; Qianqian Xie; Sophia Ananiadou; Junichi Tsujii,"Large language models (LLMs) have advanced financial applications, yet they often lack sufficient financial knowledge and struggle with tasks involving multi-modal inputs like tables and time series data. To address these limitations, we introduce \textit{Open-FinLLMs}, a series of Financial LLMs. We begin with FinLLaMA, pre-trained on a 52 billion token financial corpus, incorporating text, tables, and time-series data to embed comprehensive financial knowledge. FinLLaMA is then instruction fine-tuned with 573K financial instructions, resulting in FinLLaMA-instruct, which enhances task performance. Finally, we present FinLLaVA, a multimodal LLM trained with 1.43M image-text instructions to handle complex financial data types. Extensive evaluations demonstrate FinLLaMA's superior performance over LLaMA3-8B, LLaMA3.1-8B, and BloombergGPT in both zero-shot and few-shot settings across 19 and 4 datasets, respectively. FinLLaMA-instruct outperforms GPT-4 and other Financial LLMs on 15 datasets. FinLLaVA excels in understanding tables and charts across 4 multimodal tasks. Additionally, FinLLaMA achieves impressive Sharpe Ratios in trading simulations, highlighting its robust financial application capabilities. We will continually maintain and improve our models and benchmarks to support ongoing innovation in academia and industry.",,,,,Computer science; Business,,,,,https://arxiv.org/abs/2408.11878,http://dx.doi.org/10.48550/arxiv.2408.11878,,10.48550/arxiv.2408.11878,,,0,,0,true,,green
145-636-613-183-172,"Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training
  in LLMs",2024-11-10,2024,preprint,arXiv (Cornell University),,,,Kazuki Fujii; Taishi Nakamura; Rio Yokota,"Large Language Models (LLMs) have attracted significant attention due to their human-like language understanding and generation capabilities, as well as their applicability across various domains. These models, characterized by their massive scale and extensive training data, continue to push the boundaries of what is possible in natural language processing. The Llama 3 series, for instance, exemplifies this trend with its flagship model boasting 405 billion parameters trained on 15.6 trillion tokens. The immense computational demands associated with training such models have spurred ongoing research into optimizing the efficiency of the training process, particularly through the use of lower-precision formats. NVIDIA's H100 GPU, which introduces support for FP8 in addition to the more conventional FP16 and BF16 formats, has emerged as a focal point in this optimization effort. Preliminary studies suggest that FP8 could offer substantial reductions in training time without sacrificing model performance when compared to BF16, making it a promising candidate for large-scale model training. However, the broader implications of adopting FP8, particularly in terms of training stability and downstream task performance, have yet to be fully understood. In this study, we delve into the practical trade-offs involved in adopting FP8 over BF16 for training LLMs.",,,,,Training (meteorology); Stability (learning theory); Business; Computer science; Geography; Machine learning; Meteorology,,,,,https://arxiv.org/abs/2411.08719,http://dx.doi.org/10.48550/arxiv.2411.08719,,10.48550/arxiv.2411.08719,,,0,,0,true,,green
145-662-595-088-380,Research on the Application of Large Language Models in Financial Shared Service Centers,2025-05-26,2025,preprint,,,MDPI AG,,Ethan Cole,"<jats:p>With the rapid development of information technology, Financial Shared Service Centers (FSSC) have gradually become a core model in corporate financial management. Meanwhile, Large Language Models (LLMs), as an important technology in the field of natural language processing, have been widely applied across various industries. This paper aims to explore the application of LLMs in FSSCs, analyzing their impact on financial data processing, intelligent analysis, report generation, and customer service. Through case studies, this paper demonstrates how LLMs improve financial processing efficiency and optimize customer service experience, while also addressing challenges in data privacy, model accuracy, and technical implementation. Finally, the paper offers strategic recommendations for the intelligent transformation of FSSCs and looks ahead to the future development potential of LLMs in this field.</jats:p>",,,,,Business; Service (business); Computer science; Marketing,,,,,,http://dx.doi.org/10.20944/preprints202505.2004.v1,,10.20944/preprints202505.2004.v1,,,0,,1,true,,bronze
145-673-550-165-323,Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: EMNLP 2025,,Association for Computational Linguistics,,Xiaojun Wu; Junxi Liu; Huan-Yi Su; Zhouchi Lin; Yiyan Qi; Chengjin Xu; Jiajun Su; Jiajie Zhong; Fuwei Wang; Saizhuo Wang; Fengrui Hua; Jia Li; Jian Guo,,,,22544,22560,,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-emnlp.1227,,10.18653/v1/2025.findings-emnlp.1227,,,0,,0,false,,
145-931-944-659-640,Sentiment spin: Attacking financial sentiment with GPT-3,,2023,journal article,Finance Research Letters,15446123; 15446131,Elsevier BV,Netherlands,Markus Leippold,"In this study, we explore the susceptibility of financial sentiment analysis to adversarial attacks that manipulate financial texts. With the rise of AI readership in the financial sector, companies are adapting their language and disclosures to fit AI processing better, leading to concerns about the potential for manipulation. In the finance literature, keyword-based methods, such as dictionaries, are still widely used for financial sentiment analysis due to their perceived transparency. However, our research demonstrates the vulnerability of keyword-based approaches by successfully generating adversarial attacks using the sophisticated transformer model, GPT-3. With a success rate of nearly 99% for negative sentences in the Financial Phrase Bank, a widely used database for financial sentiment analysis, we highlight the importance of incorporating robust methods, such as context-aware approaches such as BERT, in financial sentiment analysis.",55,,103957,103957,Sentiment analysis; Context (archaeology); Transparency (behavior); Computer science; Finance; Artificial intelligence; Business; Computer security; Paleontology; Biology,,,,,https://www.zora.uzh.ch/id/eprint/234515/1/1_s2.0_S154461232300329X_main.pdf https://www.zora.uzh.ch/id/eprint/236069/1/Sentiment_Spin.pdf,http://dx.doi.org/10.1016/j.frl.2023.103957,,10.1016/j.frl.2023.103957,,,0,006-399-176-500-152; 007-934-380-605-777; 040-122-907-764-081; 045-943-903-101-707; 054-673-168-720-26X; 089-003-392-887-045; 090-993-154-613-111; 159-900-520-776-444,58,true,cc-by,hybrid
146-060-674-324-298,LLM-based Attack Scenarios Generator with IT Asset Management and Vulnerability Information,2023-11-08,2023,conference proceedings article,2023 6th International Conference on Signal Processing and Information Security (ICSPIS),,IEEE,,Takeru Naito; Rei Watanabe; Takuho Mitsunaga,"As businesses become more dependent on IT due to digital transformation, a variety of attackers are targeting companies, government agencies, and individuals to steal information and disrupt services. To reduce the risks these cyber threats pose, penetration testing and red teaming are important. On the other hand, these initiatives require skills and knowledge, and there is a shortage of human resources. This research aims to demonstrate the effectiveness of a system that inputs asset management data and vulnerability information into ChatGPT and searches for attack routes with a high threat level. Specifically, ChatGPT uses information used for IT asset management (OS type, version, device usage, account), vulnerability information published by CISA, and network information as input values to verify whether it is possible to output attack routes that are useful for penetration testing and red teaming. The results of the experiment confirmed that attack vectors for penetration testing and red teaming could be used to effectively uncover cybersecurity threats within an organization and perform risk assessments.",,,99,103,Computer security; Vulnerability (computing); Computer science; Asset (computer security); Information security; Vulnerability assessment; Risk management; Economic shortage; Information security management; Malware; Risk analysis (engineering); Government (linguistics); Knowledge management; Business; Security information and event management; Cloud computing; Cloud computing security; Psychology; Linguistics; Philosophy; Finance; Psychological resilience; Psychotherapist; Operating system,,,,,,http://dx.doi.org/10.1109/icspis60075.2023.10344019,,10.1109/icspis60075.2023.10344019,,,0,043-314-637-195-215; 126-110-959-814-01X; 141-191-638-630-480; 142-862-806-335-247; 183-798-699-261-767,11,false,,
146-335-085-869-417,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,2023-04-30,2023,conference proceedings article,Companion Proceedings of the ACM Web Conference 2023,,ACM,,Xiang Deng; Vasilisa Bashlovkina; Feng Han; Simon Baumgartner; Michael Bendersky,"Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.",,,107,110,Computer science; Pipeline (software); Social media; Task (project management); Artificial intelligence; Quality (philosophy); Jargon; Sentiment analysis; Machine learning; Deep learning; Annotation; Supervised learning; Natural language processing; Data science; World Wide Web; Linguistics; Economics; Philosophy; Management; Epistemology; Artificial neural network; Programming language,,,,,https://arxiv.org/pdf/2212.11311 https://arxiv.org/abs/2212.11311,http://dx.doi.org/10.1145/3543873.3587324,,10.1145/3543873.3587324,,,0,004-853-485-857-424; 006-399-176-500-152; 037-764-115-760-61X; 038-132-437-574-01X; 044-679-743-934-972; 189-738-179-035-882,30,true,,green
146-604-091-943-305,Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback,2025-08-19,2025,preprint,arXiv (Cornell University),,,,Yihao Ang; Yifan Bao; Lei Jiang; Jiajie Tao; Anthony K. H. Tung; Lukasz Szpruch; Hao Ni,"Time-series data is central to decision-making in financial markets, yet building high-performing, interpretable, and auditable models remains a major challenge. While Automated Machine Learning (AutoML) frameworks streamline model development, they often lack adaptability and responsiveness to domain-specific needs and evolving objectives. Concurrently, Large Language Models (LLMs) have enabled agentic systems capable of reasoning, memory management, and dynamic code generation, offering a path toward more flexible workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular agentic framework designed to automate and enhance time-series modeling workflows for financial applications. The agent formalizes the pipeline as a structured, iterative decision process across three stages: model selection, code refinement, and fine-tuning, guided by contextual reasoning and experimental feedback. Central to our architecture is a planner agent equipped with structured knowledge banks, curated libraries of models and refinement strategies, which guide exploration, while improving interpretability and reducing error propagation. \textsf{TS-Agent} supports adaptive learning, robust debugging, and transparent auditing, key requirements for high-stakes environments such as financial services. Empirical evaluations on diverse financial forecasting and synthetic data generation tasks demonstrate that \textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic baselines, achieving superior accuracy, robustness, and decision traceability.",,,,,Workflow; Series (stratigraphy); Finance; Computer science; Business; Paleontology; Database; Biology,,,,,https://arxiv.org/abs/2508.13915,http://dx.doi.org/10.48550/arxiv.2508.13915,,10.48550/arxiv.2508.13915,,,0,,0,true,,green
146-718-531-922-440,The Efficiency vs. Accuracy Trade-off: Optimizing RAG-Enhanced LLM Recommender Systems Using Multi-Head Early Exit,,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Association for Computational Linguistics,,Huixue Zhou; Hengrui Gu; Zaifu Zhan; Xi Liu; Kaixiong Zhou; Yongkang Xiao; Mingfu Liang; Srinivas Prasad Govindan; Piyush Chawla; Jiyan Yang; Xiangfei Meng; Huayu Li; Buyun Zhang; Liang Luo; Wen-Yen Chen; Yiping Han; Bo Long; Rui Zhang; Tianlong Chen,,,,26443,26458,Computer science; Recommender system; Head (geology); Real-time computing; Artificial intelligence; Machine learning; Geology; Geomorphology,,,,,,http://dx.doi.org/10.18653/v1/2025.acl-long.1283,,10.18653/v1/2025.acl-long.1283,,,0,,1,false,,
146-885-989-484-129,A Preliminary Fundamental Financial Analysis Framework Using Structured LLM Prompting - A Case Study,2025-11-20,2025,conference proceedings article,2025 9th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS),,IEEE,,Ishan Gupta; Naanya Sharma; Abhay Kaushal; Rajeswara Rao KVS,,,,1,6,,,,,,,http://dx.doi.org/10.1109/csitss67709.2025.11294475,,10.1109/csitss67709.2025.11294475,,,0,018-866-913-622-957; 026-869-021-411-509; 033-956-248-007-753; 040-359-781-194-479; 051-918-806-072-521; 188-856-222-762-526,0,false,,
146-892-539-111-734,"Advances in Financial AI: Innovations, Risk, and Responsibility in the Era of LLMs",2025-11-10,2025,conference proceedings article,Proceedings of the 34th ACM International Conference on Information and Knowledge Management,,ACM,,Yongjae Lee; Nazanin Mehrasa; Chanyeol Choi; Chung-Chi Chen; Dhagash Mehta; Stefan Zohren; Yoon Kim; Chulheum Lee; Yeonhee Lee; Eunsook Oh,,,,6912,6915,,,,,,,http://dx.doi.org/10.1145/3746252.3761591,,10.1145/3746252.3761591,,,0,,0,false,,
147-212-836-395-918,Exploring the Effectiveness of Large Language Models in Financial Question Answering,2024-10-17,2024,conference proceedings article,Proceedings of the 3rd International Conference on Computing Advancements,,ACM,,Mondol Mridul Provakar; Emrana Kabir Hashi,,,,498,505,Question answering; Computer science; Language model; Natural language processing,,,,,,http://dx.doi.org/10.1145/3723178.3723244,,10.1145/3723178.3723244,,,0,004-239-413-234-842; 011-927-650-358-26X; 021-540-567-429-908; 035-876-619-354-109; 036-974-218-967-030; 047-517-712-494-262; 079-458-141-218-611; 086-460-114-338-948; 108-015-046-541-514; 113-722-595-186-600; 117-032-087-888-670; 121-626-389-034-85X; 139-239-027-110-484; 150-516-074-731-241; 153-939-232-752-691; 190-996-521-081-84X,0,false,,
147-237-603-307-500,Towards reducing hallucination in extracting information from financial reports using Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Bhaskarjit Sarmah; Tianjie Zhu; Dhagash Mehta; Stefano Pasquali,"For a financial analyst, the question and answer (Q\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\&A systems, and empirically demonstrate superiority of our method.",,,,,Computer science; Metadata; Earnings; Process (computing); Reading (process); Scalability; Natural language processing; Information extraction; Character (mathematics); Investment (military); Matching (statistics); Information retrieval; Artificial intelligence; Finance; Machine learning; Data science; World Wide Web; Linguistics; Business; Political science; Database; Philosophy; Statistics; Geometry; Mathematics; Politics; Law; Operating system,,,,,https://arxiv.org/abs/2310.10760,http://dx.doi.org/10.48550/arxiv.2310.10760,,10.48550/arxiv.2310.10760,,,0,,0,true,other-oa,green
147-318-938-979-397,Domain-Specific Large Language Model Finetuning using a Model Assistant for Financial Text Summarization,2023-12-05,2023,conference proceedings article,2023 IEEE Symposium Series on Computational Intelligence (SSCI),,IEEE,,Loukia Avramelou; Nikolaos Passalis; Grigorios Tsoumakas; Anastasios Tefas,"The financial market and public opinion are cor-related. This means that changes in the financial market can result in changes to public opinion and changes to public opinion can result in changes to the financial market. Accordingly, it is essential for understanding and interacting with the financial market to gather text content from online sources and process it. As a result of the rapid growth of social media and other online sources, we have seen an exponential rise in data, particularly textual data, in recent years. It can be difficult for a person to read, let alone process, the massive volumes of data generated every day. This indicates that we need automated methods for processing textual data and extracting useful information. Automated text summarization is a method of shortening huge amounts of text without losing essential information. Transformers, which can efficiently manage and analyze textual data, are state-of-the-art text summarization models. However, developing such an automated text summarization model specialized in a domain (e.g. finance) can be challenging since we lack necessary domain-specific summarization datasets. In this work, we propose a pipeline for fully automating the finetuning of a text summarization model in a specific domain, namely cryptocurrency domain, without the involvement of human annotators. To this end, we introduce a novel method for self-improvement of text summarization models which relies on a model assistant which encodes domain knowledge, enabling finetuning text summarization models in specific domains in which we lack specific-domain summarization datasets. The proposed method is evaluated on a cryptocurrency-related text summarization problem and three well-known Large Language Models (LLMs) used for text summarization.",,,381,386,Automatic summarization; Computer science; Domain (mathematical analysis); Information retrieval; Multi-document summarization; Natural language processing; Artificial intelligence; Social media; Process (computing); Sentiment analysis; Paragraph; World Wide Web; Mathematical analysis; Mathematics; Operating system,,,,European Regional Development Fund and Greece,,http://dx.doi.org/10.1109/ssci52147.2023.10371906,,10.1109/ssci52147.2023.10371906,,,0,047-330-888-189-368; 071-199-582-781-499; 112-016-181-166-111; 116-316-138-754-604; 192-357-605-155-123; 195-706-909-301-830,3,false,,
147-741-082-890-35X,Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?,2025-01-01,2025,preprint,,,Elsevier BV,,Waylon Li; Hyeonjun Kim; Mihai Cucuringu; Tiejun Ma,,,,,,Finance; Market timing; Business; Economics; Financial economics; Financial system; Initial public offering,,,,,,http://dx.doi.org/10.2139/ssrn.5287013,,10.2139/ssrn.5287013,,,0,008-919-867-507-440; 023-433-360-240-592; 031-540-205-245-178; 050-448-679-723-845; 050-665-877-314-04X; 055-161-637-947-203; 086-260-434-815-196; 087-595-944-510-166; 093-887-462-867-797; 101-314-666-696-838; 124-340-421-952-454; 149-160-037-003-51X; 190-624-303-228-420,1,false,,
147-822-705-752-899,From Deep Learning to LLMs: A survey of AI in Quantitative Investment,2025-03-27,2025,preprint,arXiv (Cornell University),,,,Bokai Cao; Saizhuo Wang; Xinyi Lin; Xiaojun Wu; Haohan Zhang; Lionel M. Ni; Jian Guo,"Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.",,,,,Investment (military); Natural resource economics; Economics; Business; Political science; Law; Politics,,,,,https://arxiv.org/abs/2503.21422,http://dx.doi.org/10.48550/arxiv.2503.21422,,10.48550/arxiv.2503.21422,,,0,,2,true,,green
148-116-646-894-413,"Investigating Energy Efficiency and Performance Trade-offs in LLM
  Inference Across Tasks and DVFS Settings",2025-01-14,2025,preprint,arXiv (Cornell University),,,,Paul Joe Maliakel; Shashikant Ilager; Ivona Brandic,"Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs. First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the models' latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems.",,,,,Inference; Energy (signal processing); Efficient energy use; Computer science; Psychology; Cognitive psychology; Artificial intelligence; Engineering; Statistics; Mathematics; Electrical engineering,,,,,https://arxiv.org/abs/2501.08219,http://dx.doi.org/10.48550/arxiv.2501.08219,,10.48550/arxiv.2501.08219,,,0,,2,true,,green
148-291-819-296-218,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Legal journal articles",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7902563,https://guides.library.harvard.edu/c.php?g=1082940&p=7902563,,,3089070837,,0,,0,false,,
148-403-801-216-076,Can LLMs Improve Sanctions Screening in the Financial System? Evidence from a Fuzzy Matching Assessment,2025-09-29,2025,journal article,Finance and Economics Discussion Series,19362854; 27673898,Board of Governors of the Federal Reserve System,,Jeffrey S. Allen; Max S. Hatfield,"<jats:p>We examined the performance of four families of large language models (LLMs) and a variety of common fuzzy matching algorithms in assessing the similarity of names and addresses in a sanctions screening context. On average, across a range of realistic matching thresholds, the LLMs in our study reduced sanctions screening false positives by 92 percent and increased detection rates by 11 percent relative to the best-performing fuzzy matching baseline. Smaller, less computationally intensive models from the same language model families performed comparably, which may support scaling. In terms of computing performance, the LLMs were, on average, over four orders of magnitude slower than the fuzzy methods. To help address this, we propose a model cascade that escalates higher uncertainty screening cases to LLMs, while relying on fuzzy and exact matching for easier cases. The cascade is nearly twice as fast and just as accurate as the pure LLM system. We show even stronger runtime gains and comparable screening accuracy by relying on the fastest language models within the cascade. In the near term, the economic cost of running LLMs, inference latency, and other frictions, including API limits, will likely necessitate using these types of tiered approaches for sanctions screening in high-velocity and high-throughput financial activities, such as payments. Sanctions screening in slower-moving processes, such as customer due diligence for account opening and lending, may be able to rely on LLMs more extensively.</jats:p>",,2025-092,,,Sanctions; Matching (statistics); Fuzzy logic; Business; Economics; Political science; Computer science; Medicine; Artificial intelligence; Law; Pathology,,,,,,http://dx.doi.org/10.17016/feds.2025.092,,10.17016/feds.2025.092,,,0,,0,true,,bronze
148-543-689-113-169,"Evaluating Company-specific Biases in Financial Sentiment Analysis using
  Large Language Models",2024-11-01,2024,preprint,arXiv (Cornell University),,,,Kei Nakagawa; Masanori Hirano; Yugo Fujimoto,"This study aims to evaluate the sentiment of financial texts using large language models~(LLMs) and to empirically determine whether LLMs exhibit company-specific biases in sentiment analysis. Specifically, we examine the impact of general knowledge about firms on the sentiment measurement of texts by LLMs. Firstly, we compare the sentiment scores of financial texts by LLMs when the company name is explicitly included in the prompt versus when it is not. We define and quantify company-specific bias as the difference between these scores. Next, we construct an economic model to theoretically evaluate the impact of sentiment bias on investor behavior. This model helps us understand how biased LLM investments, when widespread, can distort stock prices. This implies the potential impact on stock prices if investments driven by biased LLMs become dominant in the future. Finally, we conduct an empirical analysis using Japanese financial text data to examine the relationship between firm-specific sentiment bias, corporate characteristics, and stock performance.",,,,,Sentiment analysis; Business; Finance; Computer science; Natural language processing,,,,,https://arxiv.org/abs/2411.00420,http://dx.doi.org/10.48550/arxiv.2411.00420,,10.48550/arxiv.2411.00420,,,0,,0,true,,green
148-696-569-862-939,UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: NAACL 2025,,Association for Computational Linguistics,,Yuzhe Yang; Yifei Zhang; Yan Hu; Yilin Guo; Ruoli Gan; Yueru He; Mingcong Lei; Xiao Zhang; Haining Wang; Qianqian Xie; Jimin Huang; Honghai Yu; Benyou Wang,,,,5429,5448,Computer science; Benchmark (surveying); Geodesy; Geography,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-naacl.300,,10.18653/v1/2025.findings-naacl.300,,,0,,0,false,,
148-919-820-431-537,USE OF LARGE LANGUAGE MODELS IN THE ASSET MANAGEMENT INDUSTRY: EVIDENCE REVIEW,,2024,journal article,Journal of Business Management,16915348,RISEBA University,,Dmitrii Gimmelberg; Marta Glowacka; Alexei Belinskiy; Iveta Ludviga,,22,,76,76,Asset (computer security); Business; Economics; Computer science; Computer security,,,,,,http://dx.doi.org/10.32025/jbm24004,,10.32025/jbm24004,,,0,,0,true,cc-by,gold
149-257-509-856-483,"WaterJudge: Quality-Detection Trade-off when Watermarking Large Language
  Models",2024-03-28,2024,preprint,arXiv (Cornell University),,,,Piotr Molenda; Adian Liusie; Mark J. F. Gales,"Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.",,,,,Digital watermarking; Computer science; Quality (philosophy); Artificial intelligence; Image (mathematics); Philosophy; Epistemology,,,,,https://arxiv.org/abs/2403.19548,http://dx.doi.org/10.48550/arxiv.2403.19548,,10.48550/arxiv.2403.19548,,,0,,0,true,,green
149-415-821-359-933,Designing public financial management systems: exploring the use of chatbot-assisted case studies,2022-05-16,2022,journal article,Public Money & Management,09540962; 14679302,Informa UK Limited,United Kingdom,Alberto Asquer; Inna Krachkovskaya,"IMPACTThe design of public financial management (PFM) systems requires the exploration of the problem space before solution options are generated. Case studies are often employed to teach the design of PFM systems, but conventional forms of delivery of case study materials fail to help develop the skills needed to explore the problem space. This article investigates the use of chatbot-assisted case studies as a way to stimulate students' efforts to pose questions about features of the problem scenario.",42,7,551,557,Chatbot; Space (punctuation); Computer science; Business; Financial management; Knowledge management; Process management; Finance; World Wide Web; Operating system,,,,,https://www.tandfonline.com/doi/pdf/10.1080/09540962.2022.2069412?needAccess=true https://doi.org/10.1080/09540962.2022.2069412 https://eprints.soas.ac.uk/37309/1/Designing%20public%20financial%20management%20systems%20exploring%20the%20use%20of%20chatbot%20assisted%20case%20studies.pdf,http://dx.doi.org/10.1080/09540962.2022.2069412,,10.1080/09540962.2022.2069412,,,0,006-575-622-649-353; 007-338-762-538-062; 015-141-554-216-22X; 016-656-663-355-738; 020-022-863-491-38X; 021-018-325-586-116; 031-103-230-281-343; 031-236-428-183-916; 034-154-930-248-455; 036-540-567-509-38X; 036-564-694-240-228; 038-925-283-945-155; 043-442-383-031-590; 047-181-296-067-635; 049-411-544-590-091; 049-708-026-452-120; 055-372-148-797-865; 057-032-775-530-715; 061-468-385-783-735; 064-800-750-653-180; 067-811-372-087-099; 070-200-030-384-013; 071-335-047-964-73X; 081-454-430-748-380; 089-766-937-624-866; 094-818-497-571-82X; 096-375-861-165-231; 097-843-805-495-464; 099-977-184-593-075; 103-875-593-522-177; 110-417-871-298-706; 113-571-877-435-622; 137-856-125-285-860; 138-416-541-567-776; 154-524-066-784-55X; 158-614-636-612-050; 168-120-542-128-789,6,true,cc-by-nc-nd,hybrid
149-477-930-721-384,FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models,2024-01-01,2024,journal article,IEEE Transactions on Big Data,23327790; 23722096,,,Ziqiang Yuan; Kaiyuan Wang; Shoutai Zhu; Ye Yuan; Jingya Zhou; Yanlin Zhu; Wenqi Wei,,,,1,14,Computer science; Artificial intelligence; Data science,,,,,,,,,,,0,,0,false,,
149-618-411-241-115,Enhancing Automated Trading with Sentiment Analysis: Leveraging Large Language Models for Stock Market Predictions,2025-03-17,2025,journal article,The American Journal of Engineering and Technology,26890984,The USA Journals,,Md Tarake Siddique; Sakib Salam Jamee; Ashadujjaman Sajal; Sanjida Nowshin Mou; Md Rayhan Hassan Mahin; Md Omar Obaid; Md Refat Hossain; Mahabub Hasan; MD Sajedul Karim Chy,"<jats:p>This study explores the use of Large Language Models (LLMs) for automating investment strategies through sentiment analysis of financial news, social media, and market data. By fine-tuning models like GPT-3 on financial datasets, sentiment indicators are extracted and integrated with traditional machine learning algorithms to predict stock price movements. A comparative analysis of various models, including LLM-based, traditional machine learning models, and hybrid approaches, was conducted. The results reveal that the hybrid model, combining LLM-generated sentiment with machine learning algorithms, outperforms other models in terms of both prediction accuracy and financial performance. The hybrid approach achieved an accuracy of 77.4%, cumulative returns of 17.2%, and a Sharpe ratio of 1.20, demonstrating its potential for real-world trading applications. These findings highlight the importance of sentiment data in enhancing market predictions and provide a promising framework for automating investment strategies. However, challenges such as ambiguity in sentiment classification and the need for model adaptation to changing market conditions remain. Future research should focus on improving sentiment analysis accuracy and incorporating reinforcement learning for real-time trading.</jats:p>",7,3,185,195,Sentiment analysis; Stock market; Algorithmic trading; Computer science; Trading strategy; Econometrics; Financial economics; Artificial intelligence; Natural language processing; Economics; Paleontology; Horse; Biology,,,,,,http://dx.doi.org/10.37547/tajet/volume07issue03-16,,10.37547/tajet/volume07issue03-16,,,0,,1,true,,bronze
149-646-884-197-183,AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative Contracts,,2024,conference proceedings article,Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence,,International Joint Conferences on Artificial Intelligence Organization,,Maruf Ahmed Mridul; Ian Sloyan; Aparna Gupta; Oshani Seneviratne,"<jats:p>Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) are reshaping how AI systems extract and organize information from unstructured text. A key challenge is designing AI methods that can incrementally extract, structure, and validate information while preserving hierarchical and contextual relationships. We introduce CDMizer, a template driven, LLM, and RAG-based framework for structured text transformation. By leveraging depth-based retrieval and hierarchical generation, CDMizer ensures a controlled, modular process that aligns generated outputs with predefined schemas. Its template-driven approach guarantees syntactic correctness, schema adherence, and improved scalability, addressing key limitations of direct generation methods. Additionally, we propose an LLM-powered evaluation framework to assess the completeness and accuracy of structured representations. Demonstrated in the transformation of Over-the-Counter (OTC) financial derivative contracts into the Common Domain Model (CDM), CDMizer establishes a scalable foundation for AI-driven document understanding, structured synthesis, and automated validation in broader contexts.</jats:p>",,,9305,9312,,,,,,,http://dx.doi.org/10.24963/ijcai.2024/1034,,10.24963/ijcai.2024/1034,,,0,,1,false,,
150-041-533-296-546,A Study on an Explainable GPT-Based Stock Investment Analysis System : Integrated Design of RAG Structure and Multi-AI Agent Framework,2025-08-31,2025,journal article,Information Systems Review,29826551; 29826837,Korea Society of Management Information Systems,,Dongkun Jung; Jonghwa Lee,,27,3,243,266,Stock (firearms); Business; Computer science; Engineering; Mechanical engineering,,,,,,http://dx.doi.org/10.14329/isr.2025.27.3.243,,10.14329/isr.2025.27.3.243,,,0,,0,false,,
150-105-850-508-346,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Non-law disciplinary databases",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Discipline; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7902613,https://guides.library.harvard.edu/c.php?g=1082940&p=7902613,,,3087984258,,0,,0,false,,
150-254-964-824-231,"Benchmarking Large Language Models on CFLUE -- A Chinese Financial
  Language Understanding Evaluation Dataset",2024-05-17,2024,preprint,arXiv (Cornell University),,,,Jie Zhu; Junhui Li; Yalong Wen; Lifan Guo,"In light of recent breakthroughs in large language models (LLMs) that have revolutionized natural language processing (NLP), there is an urgent need for new benchmarks to keep pace with the fast development of LLMs. In this paper, we propose CFLUE, the Chinese Financial Language Understanding Evaluation benchmark, designed to assess the capability of LLMs across various dimensions. Specifically, CFLUE provides datasets tailored for both knowledge assessment and application assessment. In knowledge assessment, it consists of 38K+ multiple-choice questions with associated solution explanations. These questions serve dual purposes: answer prediction and question reasoning. In application assessment, CFLUE features 16K+ test instances across distinct groups of NLP tasks such as text classification, machine translation, relation extraction, reading comprehension, and text generation. Upon CFLUE, we conduct a thorough evaluation of representative LLMs. The results reveal that only GPT-4 and GPT-4-turbo achieve an accuracy exceeding 60\% in answer prediction for knowledge assessment, suggesting that there is still substantial room for improvement in current LLMs. In application assessment, although GPT-4 and GPT-4-turbo are the top two performers, their considerable advantage over lightweight LLMs is noticeably diminished. The datasets and scripts associated with CFLUE are openly accessible at https://github.com/aliyun/cflue.",,,,,Benchmarking; Computer science; Business; Finance; Marketing,,,,,https://arxiv.org/abs/2405.10542,http://dx.doi.org/10.48550/arxiv.2405.10542,,10.48550/arxiv.2405.10542,,,0,,0,true,,green
150-279-740-849-764,"Responsible Innovation: A Strategic Framework for Financial LLM
  Integration",2025-04-02,2025,preprint,arXiv (Cornell University),,,,Ahmadreza Tavasoli; Maedeh Sharbaf; Seyed Mohamad Madani,"Financial institutions of all sizes are increasingly adopting Large Language Models (LLMs) to enhance credit assessments, deliver personalized client advisory services, and automate various language-intensive processes. However, effectively deploying LLMs requires careful management of stringent data governance requirements, heightened demands for interpretability, ethical responsibilities, and rapidly evolving regulatory landscapes. To address these challenges, we introduce a structured six-decision framework specifically designed for the financial sector, guiding organizations systematically from initial feasibility assessments to final deployment strategies. The framework encourages institutions to: (1) evaluate whether an advanced LLM is necessary at all, (2) formalize robust data governance and privacy safeguards, (3) establish targeted risk management mechanisms, (4) integrate ethical considerations early in the development process, (5) justify the initiative's return on investment (ROI) and strategic value, and only then (6) choose the optimal implementation pathway -- open-source versus proprietary, or in-house versus vendor-supported -- aligned with regulatory requirements and operational realities. By linking strategic considerations with practical steps such as pilot testing, maintaining comprehensive audit trails, and conducting ongoing compliance evaluations, this decision framework offers a structured roadmap for responsibly leveraging LLMs. Rather than acting as a rigid, one-size-fits-all solution, it shows how advanced language models can be thoughtfully integrated into existing workflows -- balancing innovation with accountability to uphold stakeholder trust and regulatory integrity.",,,,,Business; Process management; Finance,,,,,https://arxiv.org/abs/2504.02165,http://dx.doi.org/10.48550/arxiv.2504.02165,,10.48550/arxiv.2504.02165,,,0,,0,true,,green
150-579-636-992-266,Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts,2025-12-05,2025,preprint,,,Springer Science and Business Media LLC,,Nikesh Gyawali; Doina Caragea; Alex Vasenkov; Cornelia Caragea,"<title>Abstract</title>;                 <p>Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.</p>",,,,,,,,,,,http://dx.doi.org/10.21203/rs.3.rs-7411930/v1,,10.21203/rs.3.rs-7411930/v1,,,0,,0,false,,
150-630-331-948-652,Automatic Product Classification in International Trade: Machine Learning and Large Language Models,2023-07-21,2023,report,,,Inter-American Development Bank,,Ignacio Marra de Artiñano; Franco Riottini Depetris; Christian Volpe Martincus,"<jats:p>Accurately classifying products is essential in international trade. Virtually all countries categorize products into tariff lines using the Harmonized System (HS) nomenclature for both statistical and duty collection purposes. In this paper, we apply and assess several different algorithms to automatically classify products based on text descriptions. To do so, we use agricultural product descriptions from several public agencies, including customs authorities and the United States Department of Agriculture (USDA). We find that while traditional machine learning (ML) models tend to perform well within the dataset in which they were trained, their precision drops dramatically when implemented outside of it. In contrast, large language models (LLMs) such as GPT 3.5 show a consistently good performance across all datasets, with accuracy rates ranging between 60% and 90% depending on HS aggregation levels. Our analysis highlights the valuable role that artificial intelligence (AI) can play in facilitating product classification at scale and, more generally, in enhancing the categorization of unstructured data.</jats:p>",,,,,Categorization; Product (mathematics); Computer science; Machine learning; Artificial intelligence; Tariff; Scale (ratio); Contrast (vision); Natural language processing; Geography; Business; International trade; Cartography; Mathematics; Geometry,,,,,https://publications.iadb.org/publications/english/document/Automatic-Product-Classification-in-International-Trade-Machine-Learning-and-Large-Language-Models.pdf https://doi.org/10.18235/0005012 https://www.econstor.eu/bitstream/10419/299437/1/1890489360.pdf https://hdl.handle.net/10419/299437,http://dx.doi.org/10.18235/0005012,,10.18235/0005012,,,0,,2,true,,bronze
150-948-856-637-216,Simulating Dynamics of the Recognition-Primed Decision Model on Financial Markets Using LLM,2025-10-15,2025,book chapter,Decoding AI,,Productivity Press,,Yana Ivanova; Alexander Didenko; Mikhail Grigoryan; Artem Kraevskiy,,,,98,119,Dynamics (music); Business; Computer science; Psychology; Pedagogy,,,,,,http://dx.doi.org/10.4324/9781003606956-6,,10.4324/9781003606956-6,,,0,,0,false,,
151-404-823-571-979,XBRL Agent: Leveraging Large Language Models for Financial Report Analysis,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Shijie Han; Haoqiang Kang; Bo Jin; Xiao-Yang Liu; Steve Y Yang,"eXtensible Business Reporting Language (XBRL) has attained the status of the global de facto standard for business reporting. However, its complexity poses significant barriers to interpretation and accessibility. In this paper, we present the first evaluation of large language models' (LLMs) performance in analyzing XBRL reports. Our study identifies LLMs' limitations in the comprehension of financial domain knowledge and mathematical calculation in the context of XBRL reports. To address these issues, we propose enhancement methods using external tools under the agent framework, referred to as XBRL-Agent, which invokes retrievers and calculators. Extensive experiments on two tasks - the Domain Query Task (which involved testing 500 XBRL term explanations and 50 domain questions) and the Numeric Type Query Task (tested 1,000 financial math tests and 50 numeric queries) - demonstrate substantial performance improvements, with accuracy increasing by up to 17% for the domain task and 42% for the numeric type task. This work not only explores the potential of LLMs for analyzing XBRL reports but also augments the reliability and robustness of such analysis, although there is still much room for improvement in mathematical calculations.",,,856,864,XBRL; Computer science; Programming language,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698614 https://doi.org/10.1145/3677052.3698614,http://dx.doi.org/10.1145/3677052.3698614,,10.1145/3677052.3698614,,,0,000-611-068-169-404; 005-481-748-663-78X; 026-853-162-698-172; 055-169-309-233-443; 055-318-598-916-211; 060-975-152-229-020; 069-112-368-909-504; 072-626-335-305-25X; 086-551-313-427-998; 091-066-249-243-386; 103-318-963-393-244; 142-255-449-144-068; 154-042-138-562-576; 192-806-908-794-847,4,true,,bronze
151-586-590-102-150,Automatic Product Classification in International Trade: Machine Learning and Large Language Models,2025-07-16,2025,journal article,Review of International Economics,09657576; 14679396,Wiley,United Kingdom,Ignacio Marra de Artiñano; Franco Riottini Depetris; Christian Volpe Martincus,"<jats:title>ABSTRACT</jats:title><jats:p>Accurate product classification is crucial in international trade. In this study, we apply and assess several algorithms to automatically classify agricultural and food products based on text descriptions sourced from different public agencies, including customs authorities and the United States Department of Agriculture (USDA). We find that while traditional machine learning (ML) models tend to perform well within the dataset on which they are trained, their precision drops dramatically when applied to external datasets. In contrast, large language models (LLMs) show a consistently strong performance across all datasets. The top performing LLMs—Claude 3.5 Sonnet and GPT‐4—achieve accuracy rates of approximately 80% at classifying products into 6‐digit Harmonized System (HS) categories and above 90% for HS 2‐digit Chapters. Our analysis highlights the valuable role that artificial intelligence can play in facilitating product classification at scale and, more generally, in enhancing the categorization of unstructured data.</jats:p>",,,,,Categorization; Product (mathematics); Machine learning; Computer science; Artificial intelligence; Agriculture; Contrast (vision); Scale (ratio); Natural language processing; Geography; Mathematics; Geometry; Cartography; Archaeology,,,,,,http://dx.doi.org/10.1111/roie.70009,,10.1111/roie.70009,,,0,014-250-181-477-983; 019-155-503-776-633; 019-704-153-212-756; 040-408-470-459-714; 047-296-876-065-02X; 053-373-970-513-339; 053-613-341-919-467; 058-237-947-862-683; 059-507-729-500-979; 060-505-372-260-461; 077-044-033-853-201; 091-381-236-484-180; 101-071-555-176-30X; 106-644-239-684-326; 157-787-315-927-958; 158-908-876-916-110; 163-702-158-124-810; 190-050-627-029-962; 198-573-119-668-956,1,false,,
151-655-325-500-250,Comparison of LLMs on Financial Professional Tests,2025-12-19,2025,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Switzerland,,Artyom Vyatkin; Valerii Oliseenko; Ruslan Morozov,,,,655,666,,,,,,,http://dx.doi.org/10.1007/978-3-032-13615-2_58,,10.1007/978-3-032-13615-2_58,,,0,014-888-742-797-139; 023-717-083-969-194; 056-542-756-996-88X; 064-653-811-375-608; 081-776-456-098-496; 092-779-614-433-046; 113-733-939-965-098; 127-286-378-001-622; 131-030-459-473-790; 139-239-027-110-484; 197-673-280-254-524,0,false,,
151-953-730-071-854,Enhancing Financial Reasoning in Large Language Models: The Role of Gold Facts,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Shoutai Zhu; Ziqiang Yuan; Kaiyuan Wang; Yishu Zhang; Wenqi Wei,,,,1919,1928,Computer science; Finance; Economics,,,,National Natural Science Foundation of China,,http://dx.doi.org/10.1109/bigdata62323.2024.10825021,,10.1109/bigdata62323.2024.10825021,,,0,001-247-712-433-340; 010-918-453-969-378; 011-804-254-756-004; 024-755-684-895-067; 031-109-865-045-253; 044-816-837-272-795; 056-542-756-996-88X; 060-724-186-665-166; 071-926-604-793-115; 081-776-456-098-496; 096-082-868-445-261; 096-372-049-261-042; 097-409-164-563-436; 108-556-162-911-430; 137-428-420-691-14X; 151-573-827-007-539; 156-937-801-541-203; 180-007-110-109-664,1,false,,
152-034-163-519-235,"When AI Meets Finance (StockAgent): Large Language Model-based Stock
  Trading in Simulated Real-world Environments",2024-07-15,2024,preprint,arXiv (Cornell University),,,,Chong Zhang; Xinyi Liu; Zhongmou Zhang; Mingyu Jin; Lingyao Li; Zhenting Wang; Wenyue Hua; Dong Shu; Suiyuan Zhu; Xiaobo Jin; Sujian Li; Mengnan Du; Yongfeng Zhang,"Can AI Agents simulate real-world trading environments to investigate the impact of external factors on stock trading activities (e.g., macroeconomics, policy changes, company fundamentals, and global events)? These factors, which frequently influence trading behaviors, are critical elements in the quest for maximizing investors' profits. Our work attempts to solve this problem through large language model based agents. We have developed a multi-agent AI system called StockAgent, driven by LLMs, designed to simulate investors' trading behaviors in response to the real stock market. The StockAgent allows users to evaluate the impact of different external factors on investor trading and to analyze trading behavior and profitability effects. Additionally, StockAgent avoids the test set leakage issue present in existing trading simulation systems based on AI Agents. Specifically, it prevents the model from leveraging prior knowledge it may have acquired related to the test data. We evaluate different LLMs under the framework of StockAgent in a stock trading environment that closely resembles real-world conditions. The experimental results demonstrate the impact of key external factors on stock market trading, including trading behavior and stock price fluctuation rules. This research explores the study of agents' free trading gaps in the context of no prior knowledge related to market data. The patterns identified through StockAgent simulations provide valuable insights for LLM-based investment advice and stock recommendation. The code is available at https://github.com/MingyuJ666/Stockagent.",,,,,Stock (firearms); Computer science; Financial economics; Stock trading; Finance; Economics; Stock market; Geography; Archaeology; Context (archaeology),,,,,https://arxiv.org/abs/2407.18957,http://dx.doi.org/10.48550/arxiv.2407.18957,,10.48550/arxiv.2407.18957,,,0,,0,true,,green
152-461-491-562-925,Explainable zero-shot trading using multi-agent LLM architecture: A backtested approach for Bitcoin price,,,journal article,Information Processing & Management,03064573; 18735371,Elsevier BV,United Kingdom,Hae Sun Jung; Haein Lee,,63,2,104466,,,,,,,,http://dx.doi.org/10.1016/j.ipm.2025.104466,,10.1016/j.ipm.2025.104466,,,0,000-225-717-501-409; 003-538-271-131-066; 006-701-602-294-537; 013-150-931-719-001; 018-822-581-363-834; 025-301-389-436-95X; 029-069-985-400-429; 039-337-045-644-651; 042-608-436-775-958; 045-444-446-427-472; 050-105-713-881-042; 053-094-537-530-97X; 065-556-728-859-603; 068-968-465-538-735; 087-495-750-555-958; 087-911-264-175-79X; 088-852-902-759-028; 104-202-238-477-850; 110-294-480-777-024; 112-154-051-246-370; 113-543-881-609-471; 114-699-261-915-416; 128-791-296-365-071; 132-264-287-975-812; 133-731-790-152-393; 160-088-621-194-946; 180-160-694-094-291,1,false,,
152-555-175-677-816,The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs,2025-04-24,2025,preprint,arXiv (Cornell University),,,,Piotr Nawrot; Robert Li; Renjie Huang; Sebastian Ruder; Kelly Marchisio; Edoardo M. Ponti,"Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.",,,,,Frontier; Transformer; Economics; International trade; Computer science; Political science; Engineering; Law; Electrical engineering; Voltage,,,,,https://arxiv.org/abs/2504.17768,http://dx.doi.org/10.48550/arxiv.2504.17768,,10.48550/arxiv.2504.17768,,,0,,0,true,,green
152-593-878-227-739,"Condensed Reasoning Prompting: Efficient Strategies, Evaluations, and Trade Offs in Large Language Model Reasoning",2025-03-07,2025,preprint,,,Springer Science and Business Media LLC,,Gautam Mehra; Danish Khan,"<title>Abstract</title>;         <p>Recent advancements in large language models (LLMs) have demonstrated that explicitly prompting for intermediate reasoning steps significantly improves performance in complex tasks. Traditional chain of thought (CoT) prompting, however, can result in verbose outputs that increase both latency and computational cost. Condensed Reasoning Prompting (CRP) addresses this trade-off by encouraging more concise reasoning traces while maintaining high accuracy. In this paper, we systematically evaluate three prompting strategies: Chain Of Thought (CoT), Chain of Draft (CoD) and Condensed Reasoning across multiple datasets, including MMLU, Big Bench, arithmetic, and symbolic reasoning tasks. We report accuracy, average tokens per question, and a token effectiveness metric (accuracy divided by token count). Our experiments are conducted in a zero-shot setting, without specific system instructions to ""skip reasoning,"" providing a more realistic assessment of model capabilities. Results indicate that condensed prompts often match or exceed chain of thought accuracy while reducing token usage, thus offering significant gains in efficiency. We discuss the implications for real-world deployments, highlighting how CRP can enable more efficient LLM applications without compromising performance.</p>",,,,,Computer science; Linguistics; Philosophy,,,,,,http://dx.doi.org/10.21203/rs.3.rs-6170708/v1,,10.21203/rs.3.rs-6170708/v1,,,0,,0,false,,
152-990-997-753-95X,Evaluating Large Language Models' Understanding of Financial Terminology via Definition Modeling,,2023,conference proceedings article,Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop,,Association for Computational Linguistics,,James Jhirad; Edison Marrese-Taylor; Yutaka Matsuo,"James Jhirad, Edison Marrese-Taylor, Yutaka Matsuo. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: Student Research Workshop. 2023.",,,93,100,Terminology; Computer science; Computational linguistics; Linguistics; Natural language processing; Philosophy,,,,,https://aclanthology.org/2023.ijcnlp-srw.12.pdf https://doi.org/10.18653/v1/2023.ijcnlp-srw.12,http://dx.doi.org/10.18653/v1/2023.ijcnlp-srw.12,,10.18653/v1/2023.ijcnlp-srw.12,,,0,,0,true,cc-by,hybrid
153-071-801-670-044,Leveraging Internet-Sourced Text Data for Financial Analytics in Supply Chain Finance: A Large Language Model-Enhanced Text Mining Workflow,2025-01-01,2025,journal article,IEEE Transactions on Engineering Management,00189391; 15580040,Institute of Electrical and Electronics Engineers Inc.,United States,Jiaxing Wang; Guoquan Liu; Cheng Yang; Xiaobo Xu; Zhongyun Li,,,,1,38,Workflow; Computer science; The Internet; Data science; Supply chain; Finance; Business; World Wide Web; Database; Marketing,,,,,,,,,,,0,,0,false,,
153-351-835-863-908,A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification,2025-02-08,2025,journal article,ACM Transactions on Management Information Systems,2158656x; 21586578,Association for Computing Machinery (ACM),United States,Sorouralsadat Fatemi; Yuheng Hu; Maryam Mousavi,"<jats:p>Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model’s accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.</jats:p>",16,1,1,30,Computer science; Task (project management); Argument (complex analysis); Language model; Artificial intelligence; Machine learning; Domain (mathematical analysis); Natural language processing; Finance; Scale (ratio); Engineering; Mathematical analysis; Biochemistry; Chemistry; Physics; Mathematics; Systems engineering; Quantum mechanics; Economics,,,,,,http://dx.doi.org/10.1145/3706119,,10.1145/3706119,,,0,000-633-812-188-185; 044-020-624-114-140; 057-982-454-558-150; 061-277-742-114-491; 062-027-974-769-598; 080-600-379-598-503; 083-489-102-388-067; 090-417-304-783-146; 092-770-866-733-36X; 096-716-674-506-363; 115-788-940-664-461; 139-239-027-110-484; 146-335-085-869-417; 150-397-545-090-569; 192-806-908-794-847,9,false,,
153-356-162-926-777,Construction of a Japanese Financial Benchmark for Large Language Models,2024-03-22,2024,preprint,arXiv (Cornell University),,,,Masanori Hirano,"With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain. Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models. Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively. According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties.",,,,,Benchmark (surveying); Computer science; Finance; Economics; Business; Geography; Cartography,,,,,https://arxiv.org/abs/2403.15062,http://dx.doi.org/10.48550/arxiv.2403.15062,,10.48550/arxiv.2403.15062,,,0,,0,true,,green
154-042-138-562-576,<scp>FinBERT</scp>: A Large Language Model for Extracting Information from Financial Text*,2023-01-06,2023,journal article,Contemporary Accounting Research,08239150; 19113846,Wiley,United States,Allen H. Huang; Hui Wang; Yi Yang,"<jats:title>ABSTRACT</jats:title><jats:p>We develop FinBERT, a state‐of‐the‐art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher‐labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short‐term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from transformers model, is especially salient when the training sample size is small and in texts containing financial words not frequently used in general texts. FinBERT also outperforms other models in identifying discussions related to environment, social, and governance issues. Last, we show that other approaches underestimate the textual informativeness of earnings conference calls by at least 18% compared to FinBERT. Our results have implications for academic researchers, investment professionals, and financial market regulators.</jats:p>",40,2,806,841,Computer science; Artificial intelligence; Machine learning; Finance; Encoder; Natural language processing; Sample (material); Salient; Earnings; Random forest; Language model; Business; Chemistry; Chromatography; Operating system,,,,,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1911-3846.12832 https://doi.org/10.1111/1911-3846.12832,http://dx.doi.org/10.1111/1911-3846.12832,,10.1111/1911-3846.12832,,,1,000-489-283-362-390; 003-673-295-004-40X; 005-855-374-067-169; 005-989-438-190-750; 006-399-176-500-152; 007-814-872-111-101; 008-289-200-243-359; 008-895-130-411-74X; 009-064-513-871-312; 012-449-248-979-466; 014-571-264-321-121; 015-011-888-895-810; 016-888-298-077-399; 017-358-296-871-204; 019-719-570-401-015; 021-999-591-803-794; 022-161-985-192-954; 029-713-764-636-678; 030-071-847-836-783; 038-687-290-036-004; 039-630-806-691-173; 039-921-176-135-067; 042-152-947-297-801; 042-269-147-288-746; 044-355-535-696-534; 047-307-350-452-130; 048-554-617-630-501; 048-708-058-231-027; 049-677-483-711-977; 052-170-807-894-470; 053-373-455-834-971; 053-591-547-250-936; 054-019-477-630-579; 055-318-598-916-211; 057-982-454-558-150; 059-149-073-001-124; 069-005-256-137-374; 069-724-028-880-263; 078-141-527-597-27X; 084-577-026-945-090; 087-130-863-208-344; 089-377-001-338-863; 094-864-860-262-632; 097-419-652-707-813; 099-462-597-129-024; 100-561-098-059-751; 102-927-876-523-853; 103-708-848-416-467; 107-754-823-720-983; 110-801-154-485-601; 119-418-108-964-586; 120-930-701-082-160; 129-308-808-414-521; 134-830-417-992-52X; 135-238-491-936-255; 140-915-057-201-578; 142-005-457-008-582; 144-762-895-307-217; 154-293-426-700-837; 164-330-557-454-333,464,true,cc-by-nc,hybrid
154-051-799-770-051,Financial Voucher Analysis with LVMs and Financial LLMs,2025-06-13,2025,conference proceedings article,2025 International Conference on Computing Technologies (ICOCT),,IEEE,,Md. Mohtasim Fuad; Vishwanath Akuthota; Ashifur Rahman; Sadia Anjum Farea; Mahabubur Rahman; Amit Esh Chowdhury; Ahmed Selim Anwar; Md Sadi Ashraf,,,,1,6,Voucher; Business; Finance; Accounting,,,,,,http://dx.doi.org/10.1109/icoct64433.2025.11118347,,10.1109/icoct64433.2025.11118347,,,0,023-231-794-454-664; 025-058-506-793-087; 030-965-991-617-57X; 054-533-707-369-815; 087-807-064-907-108; 098-428-380-981-106; 111-358-200-262-998; 139-239-027-110-484; 150-914-959-254-888; 164-793-509-345-344; 171-767-014-669-018; 191-041-689-806-87X; 193-852-592-347-55X; 194-899-063-628-983; 199-392-235-944-050,1,false,,
154-271-661-769-415,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Foreign law and regional studies databases",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Regional studies; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905470,https://guides.library.harvard.edu/c.php?g=1082940&p=7905470,,,3089280127,,0,,0,false,,
154-513-179-142-971,GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice,,2023,journal article,Finance Research Letters,15446123; 15446131,Elsevier BV,Netherlands,Paweł Niszczota; Sami Abbas,,58,,104333,104333,Financial literacy; Dilemma; Test (biology); Advice (programming); Literacy; Baseline (sea); Actuarial science; Business; Economics; Finance; Computer science; Political science; Economic growth; Mathematics; Paleontology; Biology; Programming language; Geometry; Law,,,,Narodowe Centrum Nauki; Ministry of Science and Higher Education of the Republic of Poland,http://arxiv.org/pdf/2309.00649 http://arxiv.org/abs/2309.00649 https://arxiv.org/pdf/2309.00649 https://arxiv.org/abs/2309.00649,http://dx.doi.org/10.1016/j.frl.2023.104333,,10.1016/j.frl.2023.104333,,,0,000-805-539-723-738; 007-516-882-620-676; 009-122-864-785-531; 013-975-074-051-604; 027-864-803-674-956; 027-892-634-351-23X; 042-277-346-850-48X; 052-185-371-876-715; 068-013-933-803-965; 070-905-281-768-062; 077-444-269-577-533; 087-248-655-017-600; 092-390-513-097-957; 095-475-937-136-456; 129-238-717-649-180; 155-676-717-437-906; 183-131-210-295-000; 187-921-603-756-535; 190-050-627-029-962; 191-424-188-274-666; 196-974-555-759-760,51,true,,green
154-688-326-192-114,StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?,2025-10-02,2025,preprint,arXiv (Cornell University),,,,Yanxu Chen; Zijun Yao; Yantao Liu; Jin Ye; Jianing Yu; Lei Hou; Juanzi Li,"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",,,,,Economics; Business; International economics; Monetary economics; International trade,,,,,https://arxiv.org/abs/2510.02209,http://dx.doi.org/10.48550/arxiv.2510.02209,,10.48550/arxiv.2510.02209,,,0,,0,true,,green
154-750-441-933-111,"GreedLlama: Performance of Financial Value-Aligned Large Language Models
  in Moral Reasoning",2024-04-02,2024,preprint,arXiv (Cornell University),,,,Jeffy Yu; Maximilian Huber; Kevin Tang,"This paper investigates the ethical implications of aligning Large Language Models (LLMs) with financial optimization, through the case study of GreedLlama, a model fine-tuned to prioritize economically beneficial outcomes. By comparing GreedLlama's performance in moral reasoning tasks to a base Llama2 model, our results highlight a concerning trend: GreedLlama demonstrates a marked preference for profit over ethical considerations, making morally appropriate decisions at significantly lower rates than the base model in scenarios of both low and high moral ambiguity. In low ambiguity situations, GreedLlama's ethical decisions decreased to 54.4%, compared to the base model's 86.9%, while in high ambiguity contexts, the rate was 47.4% against the base model's 65.1%. These findings emphasize the risks of single-dimensional value alignment in LLMs, underscoring the need for integrating broader ethical values into AI development to ensure decisions are not solely driven by financial incentives. The study calls for a balanced approach to LLM deployment, advocating for the incorporation of ethical considerations in models intended for business applications, particularly in light of the absence of regulatory oversight.",,,,,Value (mathematics); Moral reasoning; Economics; Business; Psychology; Computer science; Social psychology; Machine learning,,,,,https://arxiv.org/abs/2404.02934,http://dx.doi.org/10.48550/arxiv.2404.02934,,10.48550/arxiv.2404.02934,,,0,,0,true,,green
155-075-779-422-562,Agent Trading Arena: A Study on Numerical Understanding in LLM-Based Agents,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: EMNLP 2025,,Association for Computational Linguistics,,Tianmi Ma; Jiawei Du; Wenxin Huang; Wenjie Wang; Liang Xie; Xian Zhong; Joey Tianyi Zhou,,,,5496,5514,,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-emnlp.294,,10.18653/v1/2025.findings-emnlp.294,,,0,,0,false,,
155-344-781-091-718,Leveraging conversational AI to better enable the customer journey: A practical guide to deploying chatbots at financial institutions,2022-06-01,2022,journal article,Journal of Digital Banking,23970618; 2397060x,Henry Stewart Publications,,Benjamin Maxim,"<jats:p xml:lang=""en"">The global conversational artificial intelligence (AI) market size is projected to grow from US$6.8bn in 2021 to US$18.4bn by 2026, at a compound annual growth rate (CAGR) of 21.8 per cent during the forecast period. The major factors driving the growth of the conversational AI market are the increasing demand for AI-powered customer support services, omnichannel deployment and reduced chatbot development costs. This paper explores the practical application of leveraging the conversational AI market to advance customer service initiatives and enhance the customer journey.</jats:p>",7,1,28,28,Chatbot; Computer science; Business; World Wide Web,,,,,,http://dx.doi.org/10.69554/gccv4121,,10.69554/gccv4121,,,0,,1,false,,
155-372-060-695-093,Leveraging Large Language Models for Predicting Stock Option Valuation and Financial Risk Mitigation,2024-12-09,2024,conference proceedings article,2024 IEEE International Conference on Data Mining Workshops (ICDMW),,IEEE,,Lester David Dsouza; Jamal Abdul Nasir; Muhammad Mohsin Kamal; Lena Connolly,,,,97,105,Valuation (finance); Stock (firearms); Computer science; Finance; Business; Mechanical engineering; Engineering,,,,Science Foundation Ireland,,http://dx.doi.org/10.1109/icdmw65004.2024.00019,,10.1109/icdmw65004.2024.00019,,,0,000-872-177-068-35X; 004-273-836-999-504; 006-483-901-583-803; 011-966-485-404-147; 017-871-633-579-958; 027-143-942-532-886; 056-226-337-623-814; 063-761-308-517-48X; 064-149-391-733-452; 071-822-808-161-420; 119-031-570-848-300; 139-239-027-110-484; 139-337-157-347-577; 146-335-085-869-417; 149-160-037-003-51X,1,false,,
155-591-233-917-548,"SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese
  Large Language Models",2024-08-05,2024,preprint,arXiv (Cornell University),,,,Shujuan Zhao; Lingfeng Qiao; Kangyang Luo; Qian-Wen Zhang; Junru Lu; Di Yin,"Large language models (LLMs) have become powerful tools for advancing natural language processing applications in the financial industry. However, existing financial LLMs often face challenges such as hallucinations or superficial parameter training, resulting in suboptimal performance, particularly in financial computing and machine reading comprehension (MRC). To address these issues, we propose a novel large language model specifically designed for the Chinese financial domain, named SNFinLLM. SNFinLLM excels in domain-specific tasks such as answering questions, summarizing financial research reports, analyzing sentiment, and executing financial calculations. We then perform the supervised fine-tuning (SFT) to enhance the model's proficiency across various financial domains. Specifically, we gather extensive financial data and create a high-quality instruction dataset composed of news articles, professional papers, and research reports of finance domain. Utilizing both domain-specific and general datasets, we proceed with continuous pre-training on an established open-source base model, resulting in SNFinLLM-base. Following this, we engage in supervised fine-tuning (SFT) to bolster the model's capability across multiple financial tasks. Crucially, we employ a straightforward Direct Preference Optimization (DPO) method to better align the model with human preferences. Extensive experiments conducted on finance benchmarks and our evaluation dataset demonstrate that SNFinLLM markedly outperforms other state-of-the-art financial language models. For more details, check out our demo video here: https://www.youtube.com/watch?v=GYT-65HZwus.",,,,,Adaptation (eye); Linguistics; Domain (mathematical analysis); Computer science; Psychology; Philosophy; Mathematics; Neuroscience; Mathematical analysis,,,,,https://arxiv.org/abs/2408.02302,http://dx.doi.org/10.48550/arxiv.2408.02302,,10.48550/arxiv.2408.02302,,,0,,0,true,,green
155-643-144-438-360,"Fintextsim-Llm: Improving Corporate Performance Prediction Through Scalable, Llm-Enhanced Financial Topic Modeling and Aspect-Based Sentiment",2025-01-01,2025,preprint,,,Elsevier BV,,Simon Jehnen; Joaquín Ordieres-Meré; Javier Villalba-Díez,,,,,,Scalability; Sentiment analysis; Computer science; Business; Artificial intelligence; Database,,,,,,http://dx.doi.org/10.2139/ssrn.5365002,,10.2139/ssrn.5365002,,,0,000-194-014-553-326; 000-923-162-492-825; 005-854-506-921-124; 005-989-438-190-750; 006-399-176-500-152; 007-401-074-296-919; 011-170-794-149-115; 015-751-483-636-478; 017-037-956-347-495; 023-021-876-389-805; 026-180-018-932-952; 028-151-692-716-91X; 035-083-606-205-833; 035-421-890-784-471; 039-098-290-939-749; 042-410-554-107-703; 044-732-771-889-339; 050-800-972-345-628; 058-355-094-972-457; 064-806-177-011-878; 070-057-472-950-87X; 076-574-638-004-036; 088-217-942-423-892; 089-102-829-068-880; 090-479-151-272-153; 116-052-853-310-74X; 117-047-065-769-760; 120-577-365-546-530; 132-290-965-934-761; 139-562-543-297-775; 144-762-895-307-217; 154-042-138-562-576; 154-692-460-912-663; 162-922-691-904-178; 173-419-696-197-928; 173-572-663-732-745; 194-473-227-778-278,1,false,,
155-758-982-534-044,A Fast Discovery Model for Production Financial Risk Using Large Language Model and Knowledge Graph,2025-04-10,2025,journal article,"Journal of Circuits, Systems and Computers",02181266; 17936454,World Scientific Pub Co Pte Ltd,Singapore,Pangli He; Huani Meng,"<jats:p> This paper focuses on building an innovative industrial financial risk rapid discovery model based on large language model and knowledge graph inference. Firstly, this work deeply analyzes the diversity, concealment and rapid propagation of industrial financial risks. Secondly, it points out the limitations of traditional risk assessment methods when dealing with massive and heterogeneous financial data. Then, it designs an integrated strategy to integrate the powerful natural language processing and understanding ability of large language models. Combined with the advantages of knowledge graph in structured information representation, this model analyzes financial text data through a large language model. Hence, key information can be automatically extracted and a preliminary risk knowledge base can be built. Finally, an industrial financial network can be established to realize correlation mining and dynamic evolution analysis of risk factors. The experimental results show that the model can effectively capture risk signals in the industrial chain, and detect potential financial risks in advance. This study not only enriches the theoretical framework of industrial financial risk management, but also provides strong technical support for risk prevention and control in practice. </jats:p>",34,9,,,Computer science; Production (economics); Language model; Graph; Risk model; Finance; Artificial intelligence; Business; Risk analysis (engineering); Theoretical computer science; Economics; Macroeconomics,,,,,,http://dx.doi.org/10.1142/s0218126625502068,,10.1142/s0218126625502068,,,0,003-651-627-207-581; 013-475-266-117-232; 015-472-532-721-90X; 015-879-463-984-355; 022-542-347-126-249; 024-307-084-932-267; 032-516-821-976-552; 032-705-656-245-352; 045-471-168-154-606; 045-880-214-907-97X; 058-408-364-289-487; 068-042-879-431-736; 071-254-436-127-80X; 104-887-249-665-472; 109-045-131-324-657; 116-298-631-876-270; 125-290-752-808-83X; 135-035-225-536-742; 136-615-057-550-234; 143-432-073-824-145; 163-298-324-923-115; 178-022-742-315-160; 189-793-548-244-025,0,false,,
156-158-530-167-391,Deep Learning with LLM system: A New Paradigm for Financial Market Prediction and Analysis,,2025,journal article,International Journal of Multidisciplinary Research and Growth Evaluation,25827138,Anfo Publication House,,Mark Ploto,"<jats:p>Rapid progress in large scale language models (LLM) has led to a wide range of business sectors AI You can develop a new frontier of applications. GPT-3 And BERT Both have revolutionary capabilities in natural language understanding, structured knowledge representation, and task specific adaptation. But LLM To implement and extend a base solution successfully within a company, it will have problems for the company. It is the maximization of the utility of its own data assets, securing data security and privacy, maintaining consistent knowledge representation, optimizing resource allocation, and dealing with a variety of business scenarios.; In order to address these issues, the enterprise scale LLM We propose a centralized knowledge base framework for introduction. The unique scalable architecture of the framework allows companies to use their own data resources safely and efficiently, in various business scenarios LLM Drive solutions can be enhanced and reusable. This framework has proven to realize a large Merritt in resource utilization, such as reduction of computational overhead, rapid solution implementation, and reusability of knowledge. These technological advances eventually lead to concrete business Merritt, such as improving investment returns, shortening the development cycle, and improving user experience. A comprehensive and practical approach to the introduction of centralized knowledge base (LLM) is the enterprise AI Provide valuable insights into the development and help businesses draw the value of these innovative technologies.</jats:p>",6,5,265,272,Deep learning; Artificial intelligence; Computer science; Machine learning,,,,,,http://dx.doi.org/10.54660/.ijmrge.2025.6.5.265-272,,10.54660/.ijmrge.2025.6.5.265-272,,,0,,0,false,,
156-230-313-423-209,Multimodal retrieval-augmented generation for financial documents: image-centric analysis of charts and tables with large language models,2025-03-11,2025,journal article,The Visual Computer,01782789; 14322315,Springer Science and Business Media LLC,Germany,Cheng Jiang; Pengle Zhang; Ying Ni; Xiaoli Wang; Hanghang Peng; Sen Liu; Mengdi Fei; Yuxin He; Yaxuan Xiao; Jin Huang; Xingyu Ma; Tian Yang,,41,10,7657,7670,Computer science; Computer graphics; Graphics; Information retrieval; Computer graphics (images); Image (mathematics); Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1007/s00371-025-03829-5,,10.1007/s00371-025-03829-5,,,0,000-574-805-537-693; 009-136-142-682-140; 012-132-514-521-998; 015-126-247-723-672; 016-531-084-325-607; 040-527-110-797-201; 044-456-506-571-452; 071-423-367-050-689; 075-356-529-898-550; 082-579-176-089-302; 089-243-336-301-479; 116-867-994-493-428; 128-950-131-708-238; 135-807-736-410-962; 136-791-795-258-721; 139-239-027-110-484; 149-527-216-132-278; 163-614-834-618-075; 168-385-661-870-808; 174-551-957-586-572; 176-502-644-606-546; 186-262-640-116-454; 188-079-526-378-618; 191-420-643-158-483; 192-806-908-794-847,2,false,,
156-904-297-275-944,Semantic Information Tells the Truth: A Novel Approach to Numerical Veracity for Large Language Models in Financial Market Analytics,2025-07-17,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Singapore,Germany,Yu Qin; Chengshang Zhang; Wei Xu,,,,297,308,Computer science; Analytics; Visual analytics; Financial market; Natural language processing; Artificial intelligence; Data science; Finance; Visualization; Economics,,,,,,http://dx.doi.org/10.1007/978-981-96-9914-8_25,,10.1007/978-981-96-9914-8_25,,,0,002-069-521-421-504; 017-676-196-726-599; 044-804-795-729-401; 056-542-756-996-88X; 081-562-385-441-31X; 081-776-456-098-496; 102-068-013-162-56X; 118-078-883-312-285; 124-910-901-957-512; 135-084-328-653-168; 139-233-157-706-748; 163-938-860-200-52X; 190-624-303-228-420,0,false,,
157-332-925-099-669,Can Large Language Models Trade? Testing Financial Theories with LLM Agents in Market Simulations,2025-01-01,2025,preprint,,,Elsevier BV,,Alejandro Lopez-Lira,,,,,,Financial market; Economics; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5217340,,10.2139/ssrn.5217340,,,0,,3,false,,
157-557-068-246-376,Chatbots for customer service in financial entities—A comprehensive systematic review,2024-12-27,2024,journal article,"Journal of Infrastructure, Policy and Development",25727931; 25727923,EnPress Publisher,,Roberto Eustaquio-Jiménez; Mercedes Durand-Azurza; Javier Gamboa-Cruzado; María León Morales; Nancy Pajares Ruiz; Reyna López de Montoya; N. Mercedes Ortiz Céliz,"<jats:p>The integration of chatbots in the financial sector has significantly improved customer service processes, providing efficient solutions for query management and problem resolution. These automated systems have proven to be valuable tools in enhancing operational efficiency and customer satisfaction in financial institutions. This study aims to conduct a systematic literature review on the impact of chatbots in customer service within the financial sector. A review of 61 relevant publications from 2018 to 2024 was conducted. Articles were selected from databases such as Scopus, IEEE Xplore, ARDI, Web of Science, and ProQuest. The findings highlight that efficiency and customer satisfaction are central to the perception of service quality, aligning with the automation of the user experience. The bibliometric analysis reveals a predominance of publications from countries such as India, Germany, and Australia, underscoring the academic and practical relevance of the topic. Additionally, essential thematic terms such as “artificial intelligence” and “advanced automation” were identified, reflecting technological evolution in this field. This study provides significant insights for future theoretical, practical, and managerial developments, offering a framework to optimize chatbot implementation in highly regulated environments.</jats:p>",8,16,10122,10122,Business; Service (business); Customer service; Computer science; Process management; Marketing,,,,,,http://dx.doi.org/10.24294/jipd10122,,10.24294/jipd10122,,,0,,2,true,,gold
157-889-703-158-641,"Financial News-Driven LLM Reinforcement Learning for Portfolio
  Management",2024-11-17,2024,preprint,arXiv (Cornell University),,,,Ananya Unnikrishnan,"Reinforcement learning (RL) has emerged as a transformative approach for financial trading, enabling dynamic strategy optimization in complex markets. This study explores the integration of sentiment analysis, derived from large language models (LLMs), into RL frameworks to enhance trading performance. Experiments were conducted on single-stock trading with Apple Inc. (AAPL) and portfolio trading with the ING Corporate Leaders Trust Series B (LEXCX). The sentiment-enhanced RL models demonstrated superior net worth and cumulative profit compared to RL models without sentiment and, in the portfolio experiment, outperformed the actual LEXCX portfolio's buy-and-hold strategy. These results highlight the potential of incorporating qualitative market signals to improve decision-making, bridging the gap between quantitative and qualitative approaches in financial trading.",,,,,Reinforcement learning; Portfolio; Reinforcement; Project portfolio management; Business; Finance; Computer science; Economics; Artificial intelligence; Management; Psychology; Project management; Social psychology,,,,,https://arxiv.org/abs/2411.11059,http://dx.doi.org/10.48550/arxiv.2411.11059,,10.48550/arxiv.2411.11059,,,0,,0,true,,green
158-046-752-737-771,Unmasking Bias in Financial AI: A Robust Framework for Evaluating and Mitigating Hidden Biases in LLMs,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Shreshth Mehrotra; Raghavendra P; Balraj Prajesh; Hrishikesh Kambale; Puspita Majumdar,,,,951,959,,,,,,,http://dx.doi.org/10.1145/3768292.3770355,,10.1145/3768292.3770355,,,0,007-354-410-729-64X; 011-563-658-531-43X; 016-556-654-810-446; 016-963-858-082-354; 040-642-316-626-946; 051-127-187-331-916; 066-933-934-296-95X; 107-537-485-965-484; 129-077-117-822-002; 139-769-528-665-304; 146-819-992-159-102; 156-289-597-317-470; 156-529-070-846-704; 173-116-597-012-168,0,false,,
158-423-107-059-26X,Research on Financial Investment Strategy Optimization with The Aid of Large Language Model,2025-04-26,2025,journal article,International Journal of Global Economics and Management,30058090; 30059690,Warwick Evans Publishing,,Jiayi Li,"<jats:p>This paper discusses the role and influence of large language model (LLM) in the optimization of financial investment strategy from the perspective of humanities and social sciences. By reviewing the evolution of investment behavior, psychology, and LLM, the paper builds a theoretical framework for understanding how these advanced technologies can help investors make more informed decisions. In particular, this paper emphasizes the importance of LLM as an information intermediary, which can interpret and integrate information from financial news, social media and other channels to help investors better grasp market sentiment and facilitate exchanges and interactions among investors. Further, the paper analyzes the ethical principles and social responsibilities that should be considered when applying LLM to investments, and proposes a new investment paradigm of human-machine collaboration, in which human intelligence and technical capabilities complement each other. Finally, the paper looks forward to the impact of technological progress and social change on financial markets, and discusses the challenges and opportunities brought about by the integration of different cultures and technologies in the context of globalization. This study not only provides policy makers with recommendations on the responsible application of technology, but also points the way for future research aimed at promoting a more transparent, fair and efficient financial environment.</jats:p>",6,3,119,125,Investment (military); Finance; Business; Financial modeling; Political science; Politics; Law,,,,,,http://dx.doi.org/10.62051/ijgem.v6n3.17,,10.62051/ijgem.v6n3.17,,,0,193-136-826-957-223,0,false,,
158-423-153-725-243,Financial News Analytics Using Fine-Tuned Llama 2 GPT Model,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Bohdan M. Pavlyshenko,"The paper considers the possibility to fine-tune Llama 2 GPT large language model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.",,,,,Computer science; JSON; Analytics; Artificial intelligence; Sentiment analysis; Machine learning; Data mining; Database,,,,,https://arxiv.org/abs/2308.13032,http://dx.doi.org/10.48550/arxiv.2308.13032,,10.48550/arxiv.2308.13032,,,0,,3,true,cc-by,green
158-463-725-232-492,Sentiment Trading with Large Language Models,2024-01-01,2024,preprint,,,Elsevier BV,,Kemal Kirtac; Guido Germano,,,,,,Sentiment analysis; Computer science; Natural language processing; Artificial intelligence,,,,,http://eprints.lse.ac.uk/122592/1/1_s2.0_S1544612324002575_main.pdf,http://dx.doi.org/10.2139/ssrn.4706629,,10.2139/ssrn.4706629,,,0,006-399-176-500-152; 017-358-296-871-204; 021-412-594-162-528; 022-486-017-682-206; 036-979-852-259-747; 047-871-035-706-003; 052-170-807-894-470; 057-982-454-558-150; 082-178-119-516-788; 094-992-121-743-156; 141-501-772-987-035; 154-042-138-562-576; 154-058-408-974-033; 183-026-309-652-990; 184-448-049-219-598,2,true,,green
158-790-852-557-36X,Research on Application of Financial Large Language Models,2024-12-24,2024,journal article,"Highlights in Business, Economics and Management",2957952x,Darcy & Roy Press Co. Ltd.,,Mingting Du,"<jats:p>With the increasing use of large language models such as chatgpt, it is not difficult to apply their capabilities to the research of natural language processing in the financial field, including but not limited to text extraction, sentiment analysis, etc. This paper analyzes the construction ideas and applications of three financial big language models, including BloombergGPT, PIXIU and FinBERT, and concludes that the current application of big language models in the financial field is possible, multi-faceted and suitable, but there are still shortcomings in ethics, data processing and other aspects. The application of large language models in the field of finance is still something to look forward to. Through this study and the comparative exploration of various models, we hope to provide valuable modeling experience for practitioners in the field of finance or computer. At the same time, it is hoped that each researcher can follow the ideas of these model-making teams to make up for the shortcomings in their own models and make their own financial big language models better.</jats:p>",45,,628,634,Business; Computer science,,,,,,http://dx.doi.org/10.54097/1z673097,,10.54097/1z673097,,,0,,0,false,,
159-150-551-702-595,Mechanistic interpretability of large language models with applications to the financial services industry,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Ashkan Golgoon; Khashayar Filom; Arjun Ravi Kannan,"Large Language Models exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads 10.2 (head 2, layer 10), 10.7, and 11.3, as well as the (negative) heads 9.6 and 10.6 play a significant role in the task completion.",,,660,668,Interpretability; Financial services; Computer science; Business; Artificial intelligence; Finance; Financial modeling,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698612 https://doi.org/10.1145/3677052.3698612 http://arxiv.org/pdf/2407.11215 http://arxiv.org/abs/2407.11215,http://dx.doi.org/10.1145/3677052.3698612,,10.1145/3677052.3698612,,,0,042-913-034-182-114; 065-567-028-113-695; 139-239-027-110-484,3,true,,bronze
159-235-759-760-420,A Comprehensive Review of Reinforcement Learning Augmented by Large Language Models for Portfolio Management,2025-05-31,2025,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Dr. R. A. Jamadar,"<jats:p>This paper presents a comprehensive review of reinforcement learning (RL) frameworks augmented by large; language models (LLMs) for portfolio management. By examining existing literature and state-of-the-art methodologies,; the review identifies the challenges faced in financial decision-making and explores solutions provided by the integration of; LLMs with RL systems. Key areas of focus include real-time sentiment analysis, transaction cost optimization, and; strategic portfolio allocation. This re- view highlights comparative performance metrics, challenges, and opportunities for; improvement in leveraging LLMs for RL-based financial strategies. Furthermore, future directions are proposed to refine; these hybrid systems for broader market applications.</jats:p>",13,5,4257,4259,Reinforcement learning; Reinforcement; Computer science; Project portfolio management; Portfolio; Artificial intelligence; Psychology; Engineering; Systems engineering; Economics; Project management; Social psychology; Financial economics,,,,,,http://dx.doi.org/10.22214/ijraset.2025.70979,,10.22214/ijraset.2025.70979,,,0,,0,false,,
159-328-417-054-983,Investment Risk Analysis and Mitigation Strategies Using Large Language Models,2025-04-01,2025,preprint,,,Institute of Electrical and Electronics Engineers (IEEE),,Sai Dheeraj Gummadi,,,,,,Investment (military); Risk analysis (engineering); Business; Computer science; Political science; Law; Politics,,,,,,http://dx.doi.org/10.36227/techrxiv.174353577.78532926/v1,,10.36227/techrxiv.174353577.78532926/v1,,,0,,0,false,,
159-585-251-610-202,FinSoSent: Advancing Financial Market Sentiment Analysis through Pretrained Large Language Models,2024-08-02,2024,journal article,Big Data and Cognitive Computing,25042289,MDPI AG,,Josiel Delgadillo; Johnson Kinyua; Charles Mutigwe,"<jats:p>Predicting the directions of financial markets has been performed using a variety of approaches, and the large volume of unstructured data generated by traders and other stakeholders on social media microblog platforms provides unique opportunities for analyzing financial markets using additional perspectives. Pretrained large language models (LLMs) have demonstrated very good performance on a variety of sentiment analysis tasks in different domains. However, it is known that sentiment analysis is a very domain-dependent NLP task that requires knowledge of the domain ontology, and this is particularly the case with the financial domain, which uses its own unique vocabulary. Recent developments in NLP and deep learning including LLMs have made it possible to generate actionable financial sentiments using multiple sources including financial news, company fundamentals, technical indicators, as well social media microblogs posted on platforms such as StockTwits and X (formerly Twitter). We developed a financial social media sentiment analyzer (FinSoSent), which is a domain-specific large language model for the financial domain that was pretrained on financial news articles and fine-tuned and tested using several financial social media corpora. We conducted a large number of experiments using different learning rates, epochs, and batch sizes to yield the best performing model. Our model outperforms current state-of-the-art FSA models based on over 860 experiments, demonstrating the efficacy and effectiveness of FinSoSent. We also conducted experiments using ensemble models comprising FinSoSent and the other current state-of-the-art FSA models used in this research, and a slight performance improvement was obtained based on majority voting. Based on the results obtained across all models in these experiments, the significance of this study is that it highlights the fact that, despite the recent advances of LLMs, sentiment analysis even in domain-specific contexts remains a difficult research problem.</jats:p>",8,8,87,87,Social media; Sentiment analysis; Microblogging; Variety (cybernetics); Computer science; Language model; Artificial intelligence; Machine learning; Financial market; Domain (mathematical analysis); Vocabulary; Financial modeling; Finance; Natural language processing; Data science; World Wide Web; Business; Linguistics; Mathematical analysis; Philosophy; Mathematics,,,,,https://www.mdpi.com/2504-2289/8/8/87/pdf?version=1723083292 https://doi.org/10.3390/bdcc8080087,http://dx.doi.org/10.3390/bdcc8080087,,10.3390/bdcc8080087,,,0,000-289-083-316-369; 000-524-338-757-738; 007-583-154-053-159; 007-814-872-111-101; 015-896-778-430-415; 020-563-084-760-554; 022-486-017-682-206; 022-999-828-221-677; 039-110-388-312-10X; 040-024-175-717-327; 042-269-147-288-746; 049-177-639-368-845; 057-982-454-558-150; 062-518-441-478-014; 071-423-367-050-689; 079-568-732-501-686; 080-600-379-598-503; 086-738-331-609-875; 089-377-001-338-863; 091-810-810-244-628; 137-925-447-012-933; 143-224-880-625-779; 158-560-044-795-411; 163-938-860-200-52X; 168-485-640-067-096; 190-624-303-228-420,15,true,cc-by,gold
159-755-920-211-719,LLM-Guided Ansätze Design for Quantum Circuit Born Machines in Financial Generative Modeling,2025-08-30,2025,conference proceedings article,2025 IEEE International Conference on Quantum Computing and Engineering (QCE),,IEEE,,Yaswitha Gujju; Romain Harang; Tetsuo Shibuya,,,,104,108,,,,,,,http://dx.doi.org/10.1109/qce65121.2025.10303,,10.1109/qce65121.2025.10303,,,0,003-985-325-343-887; 015-063-175-189-638; 030-589-504-330-533; 052-205-923-027-38X; 057-098-108-141-670; 057-198-145-297-074; 084-512-819-303-154; 087-394-536-396-164; 102-082-305-167-881; 133-435-109-394-839; 191-103-960-270-250,0,false,,
159-769-183-529-483,Financial Inclusion and Large Language Models,2025-01-01,2025,preprint,,,Elsevier BV,,Peterson K Ozili; Kingslep Obiora; Chinwendu Onuzo,,,,,,Financial inclusion; Inclusion (mineral); Business; Finance; Financial services; Psychology; Social psychology,,,,,,http://dx.doi.org/10.2139/ssrn.5327307,,10.2139/ssrn.5327307,,,0,021-887-316-511-133; 029-644-246-929-239; 035-190-809-124-818; 035-876-619-354-109; 059-533-229-398-25X; 126-891-953-162-545; 129-667-561-878-440; 139-069-233-849-176; 142-511-418-752-212; 152-751-015-281-925; 158-729-730-798-226; 164-670-314-521-352; 167-854-143-799-231; 177-619-872-912-487; 188-406-476-808-215; 189-523-252-991-996,0,false,,
159-876-811-087-756,A SURVEY ON USING LARGE LANGUAGE MODELS FOR DETECTING FRAUD IN FINANCIAL TRANSACTIONS,2025-08-14,2025,journal article,International Journal of Progressive Research in Engineering Management and Science,25831062,International Journal of Progressive Research in Engineering Management and Science,,,,,,,,Business; Actuarial science; Computer science; Finance,,,,,,http://dx.doi.org/10.58257/ijprems43325,,10.58257/ijprems43325,,,0,,0,false,,
160-180-032-445-040,Exploration and selection of LLM  models for financial text simplification,2024-09-09,2024,journal article,Revista Tecnología en Marcha,22153241; 03793982,Instituto Tecnologico de Costa Rica,,Bertha C Brenes-Brenes; Saul Calderón-Ramírez,"<jats:p>This research is dedicated to the simplification of Spanish-language financial texts to enhance accessibility for screen readers. We present a qualitative and quantitative analysis of the text simplification process, employing a set of Spanish simplification rules and metrics. Our study evaluates the outcomes resulting from the application of three distinct financial datasets to four pre-trained models. The primary objective is to identify the most effective models for text simplification and determine those warranting further investment through fine-tuning and training. This study contributes to improving the accessibility and comprehensibility of financial documents for individuals with visual impairments.</jats:p>",,,,,Selection (genetic algorithm); Computer science; Data science; Finance; Business; Artificial intelligence,,,,,,http://dx.doi.org/10.18845/tm.v37i7.7297,,10.18845/tm.v37i7.7297,,,0,,0,true,cc-by-nc-nd,gold
160-240-429-128-539,Enhancing FinRL Trading Agents with Advance LLM-Processed Financial News: An Improved Approach Using DeepSeek-V3,2025-05-09,2025,conference proceedings article,2025 IEEE 11th International Conference on Intelligent Data and Security (IDS),,IEEE,,Satish Chandra; G. Balakrishna,,,,52,54,Computer science; Business,,,,,,http://dx.doi.org/10.1109/ids66066.2025.00016,,10.1109/ids66066.2025.00016,,,0,067-240-567-001-391; 080-732-035-851-346; 122-619-267-680-643; 149-160-037-003-51X; 154-201-884-155-679,0,false,,
160-588-358-016-050,Detecting Financial Fraud Through AI-Powered Analysis of GPT-Generated Text,2025-09-09,2025,journal article,FMDB Transactions on Sustainable Technoprise Letters,29935997,Fernando Martins De Bulhão Publishing Company,,Amitabha Maheshwari; Praveen Aronkar,"<jats:p>This research paper is in response to the use of Artificial Intelligence (AI) to detect financial fraud using text analysis on Generative Pre-trained Transformers (GPT). Spammers have continued to utilise advanced language models to generate copied content; consequently, more traditional anti-fraud methods are correspondingly less effective. This article proposes a novel approach that combines Natural Language Processing (NLP) and machine learning techniques to detect deception patterns in GPT-generated content. This is achieved by generating a new dataset of authentic and artificially created financial reports, including emails, reports, and social media posts. The training dataset is tested and validated using a collection of AI models, which includes a fine-tuned version of GPT-3.5, a Long Short-Term Memory (LSTM) network, and a Transformer-based classifier. Python is the primary tool used in this paper, with TensorFlow and PyTorch packages employed for model development, and scikit-learn utilised for performance analysis. The outcome demonstrates that the developed AI system can identify phishing text with extremely high accuracy, providing financial institutions with a reasonable opportunity to enhance their ability to combat fraud in the digital era. The research highlights the future of artificial intelligence in combating new forms of fraud and emphasises the need for ongoing innovation in this area.</jats:p>",3,3,178,186,,,,,,,http://dx.doi.org/10.69888/ftstpl.2025.000450,,10.69888/ftstpl.2025.000450,,,0,,0,false,,
160-903-381-946-528,"Large Language Models on Small Resource-Constrained Systems: Performance
  Characterization, Analysis and Trade-offs",2024-12-19,2024,preprint,arXiv (Cornell University),,,,Liam Seymour; Basar Kutukcu; Sabur Baidya,"Generative AI like the Large Language Models (LLMs) has become more available for the general consumer in recent years. Publicly available services, e.g., ChatGPT, perform token generation on networked cloud server hardware, effectively removing the hardware entry cost for end users. However, the reliance on network access for these services, privacy and security risks involved, and sometimes the needs of the application make it necessary to run LLMs locally on edge devices. A significant amount of research has been done on optimization of LLMs and other transformer-based models on non-networked, resource-constrained devices, but they typically target older hardware. Our research intends to provide a 'baseline' characterization of more recent commercially available embedded hardware for LLMs, and to provide a simple utility to facilitate batch testing LLMs on recent Jetson hardware. We focus on the latest line of NVIDIA Jetson devices (Jetson Orin), and a set of publicly available LLMs (Pythia) ranging between 70 million and 1.4 billion parameters. Through detailed experimental evaluation with varying software and hardware parameters, we showcase trade-off spaces and optimization choices. Additionally, we design our testing structure to facilitate further research that involves performing batch LLM testing on Jetson hardware.",,,,,Computer science; Characterization (materials science); Distributed computing; Materials science; Nanotechnology,,,,,https://arxiv.org/abs/2412.15352,http://dx.doi.org/10.48550/arxiv.2412.15352,,10.48550/arxiv.2412.15352,,,0,,0,true,,green
161-063-788-304-217,Foundations of LLMs and Financial Applications,2025-08-22,2025,book chapter,Blockchain Technologies,26618338; 26618346,Springer Nature Singapore,,Yoonseo Chung; Jeonghyun Kim; MiYeon Kim; Minsuh Joo; Hyunsoo Cho,,,,59,90,Business; Finance,,,,,,http://dx.doi.org/10.1007/978-981-96-5833-6_4,,10.1007/978-981-96-5833-6_4,,,0,004-771-690-834-863; 010-611-207-037-544; 011-556-189-310-259; 018-866-913-622-957; 019-326-675-528-760; 027-441-448-201-884; 031-621-183-289-210; 037-515-626-082-067; 045-943-903-101-707; 048-600-557-712-015; 053-094-537-530-97X; 066-206-830-371-096; 071-423-367-050-689; 072-663-562-609-862; 074-730-885-462-295; 075-852-273-367-037; 082-178-119-516-788; 090-135-685-849-51X; 098-428-380-981-106; 104-956-882-948-669; 107-460-755-672-970; 137-120-754-193-596; 143-502-195-561-356; 145-619-611-747-670; 151-374-013-752-001; 157-786-078-320-612; 168-473-307-982-911; 192-806-908-794-847; 193-244-379-958-993; 195-706-909-301-830,0,false,,
161-172-679-686-474,Leveraging Large Language Models for Sentiment Analysis and Investment Strategy Development in Financial Markets,2025-04-20,2025,journal article,Journal of Theoretical and Applied Electronic Commerce Research,07181876,MDPI AG,Chile,Yejoon Mun; Namhyoung Kim,"<jats:p>This study investigates the application of large language models (LLMs) in sentiment analysis of financial news and their use in developing effective investment strategies. We conducted sentiment analysis on news articles related to the top 30 companies listed on Nasdaq using both discriminative models such as BERT and FinBERT, and generative models including Llama 3.1, Mistral, and Gemma 2. To enhance the robustness of the analysis, advanced prompting techniques—such as Chain of Thought (CoT), Super In-Context Learning (SuperICL), and Bootstrapping—were applied to generative LLMs. The results demonstrate that long strategies generally yield superior portfolio performance compared to short and long–short strategies. Notably, generative LLMs outperformed discriminative models in this context. We also found that the application of SuperICL to generative LLMs led to significant performance improvements, with further enhancements noted when both SuperICL and Bootstrapping were applied together. These findings highlight the profitability and stability of the proposed approach. Additionally, this study examines the explainability of LLMs by identifying critical data considerations and potential risks associated with their use. The research highlights the potential of integrating LLMs into financial strategy development to provide a data-driven foundation for informed decision-making in financial markets.</jats:p>",20,2,77,77,Sentiment analysis; Investment (military); Financial market; Financial analysis; Computer science; Finance; Business; Artificial intelligence; Politics; Political science; Law,,,,Korea government (MSIT),,http://dx.doi.org/10.3390/jtaer20020077,,10.3390/jtaer20020077,,,0,006-169-257-799-439; 019-711-284-600-64X; 022-486-017-682-206; 027-563-530-145-207; 028-723-846-593-336; 036-268-425-894-702; 045-924-822-745-988; 055-283-119-695-742; 055-368-719-911-202; 057-040-097-643-496; 059-592-450-070-325; 071-423-367-050-689; 074-362-647-607-886; 080-828-384-981-970; 097-052-956-339-636; 106-171-839-864-497; 127-982-575-913-793; 159-585-251-610-202; 160-838-562-631-770; 161-142-856-132-046; 172-356-345-863-966; 190-050-627-029-962,4,true,cc-by,gold
161-483-940-902-563,Leveraging Internet-Sourced Text Data for Financial Analytics in Supply Chain Finance: A Large Language Model-Enhanced Text Mining Workflow,,2025,journal article,IEEE Transactions on Engineering Management,00189391; 15580040,Institute of Electrical and Electronics Engineers (IEEE),United States,Jiaxing Wang; Guoquan Liu; Yang Cheng; Xiaobo Xu; Zhongyun Li,,72,,1924,1938,,,,,National Natural Science Foundation of China; National Social Science Fund of China,,http://dx.doi.org/10.1109/tem.2025.3567302,,10.1109/tem.2025.3567302,,,0,004-184-679-393-984; 006-442-395-399-227; 006-744-683-575-516; 007-942-308-220-426; 008-303-055-735-856; 008-952-466-563-534; 014-114-606-107-800; 016-767-556-679-448; 018-291-236-993-499; 022-486-017-682-206; 040-504-787-044-496; 045-484-225-394-116; 049-999-101-541-039; 051-956-822-861-632; 054-507-616-562-40X; 057-040-097-643-496; 058-017-105-532-050; 060-242-202-488-875; 063-469-244-022-358; 066-170-258-242-91X; 067-996-867-896-721; 068-524-075-402-966; 069-078-486-662-155; 073-413-328-692-885; 076-271-921-859-699; 080-401-913-719-722; 083-845-557-732-563; 086-162-916-640-721; 095-913-996-111-598; 098-381-475-442-441; 099-193-492-174-395; 100-778-041-951-99X; 111-784-048-080-218; 113-037-374-661-142; 115-852-375-415-190; 118-794-492-686-543; 119-093-700-670-533; 120-840-990-393-096; 122-976-706-777-611; 123-837-226-518-035; 124-158-980-599-57X; 124-484-424-454-154; 143-963-429-500-145; 144-277-276-503-762; 157-165-583-357-246; 158-912-252-008-992; 170-514-111-159-982; 170-577-727-618-516; 170-914-662-182-962; 183-764-443-755-602; 191-424-188-274-666; 197-691-142-603-06X; 198-084-749-318-320; 198-604-287-297-372,2,false,,
162-311-947-197-559,"Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline
  RL in Quantitative Trading",2024-11-26,2024,preprint,arXiv (Cornell University),,,,Suyeol Yun,"Developing effective quantitative trading strategies using reinforcement learning (RL) is challenging due to the high risks associated with online interaction with live financial markets. Consequently, offline RL, which leverages historical market data without additional exploration, becomes essential. However, existing offline RL methods often struggle to capture the complex temporal dependencies inherent in financial time series and may overfit to historical patterns. To address these challenges, we introduce a Decision Transformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned using Low-Rank Adaptation (LoRA). This architecture leverages the generalization capabilities of pre-trained language models and the efficiency of LoRA to learn effective trading policies from expert trajectories solely from historical data. Our model performs competitively with established offline RL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer with randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate that our approach effectively learns from expert trajectories and secures superior rewards in certain trading scenarios, highlighting the effectiveness of integrating pre-trained language models and parameter-efficient fine-tuning in offline RL for quantitative trading. Replication code for our experiments is publicly available at https://github.com/syyunn/finrl-dt",,,,,Transformer; Computer science; Engineering; Electrical engineering; Voltage,,,,,https://arxiv.org/abs/2411.17900,http://dx.doi.org/10.48550/arxiv.2411.17900,,10.48550/arxiv.2411.17900,,,0,,0,true,,green
162-492-447-309-636,<i>Artificial Intelligence Chatbots: Are Chatbots Programmed to Provide Financial Wellness Counseling to College Students?</i>,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,ZW Taylor; Andrew Marx; Dez Nixon; Sara Ray; Richard Simonds; Mallorie Smith; Sophie Glass; Joseph Mesa; Jenny Becker; Aly Blakeney; Tim Cerebe; Amanda Miller; Justin Enlow; Steven Hughes; Helen Colby; Tristia Kayser; Becky Smith; Brandan Wheeler,,,,,,Chatbot; Psychology; Medical education; Finance; Applied psychology; Computer science; Medicine; Business; Artificial intelligence,,,,,,http://dx.doi.org/10.2139/ssrn.5238572,,10.2139/ssrn.5238572,,,0,004-973-555-753-417; 019-159-421-940-098; 036-797-657-783-124; 043-924-563-683-408; 053-909-608-247-51X; 161-485-304-548-953; 168-649-657-005-877; 178-229-197-937-842,0,false,,
162-813-340-103-202,Recent Developments in Large Language Models and their Use in Financial Reporting Analyses,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Sungho Noh,,,,,,Business; Accounting,,,,,,http://dx.doi.org/10.2139/ssrn.4811631,,10.2139/ssrn.4811631,,,0,,0,false,,
162-994-695-237-870,"FinFlier: Automating Graphical Overlays for Financial Visualizations
  with Knowledge-Grounding Large Language Model",2024-12-05,2024,preprint,arXiv (Cornell University),,,,Jianing Hao; Manling Yang; Qing Shi; Yuzhe Jiang; Guang Zhang; Wei Zeng,"Graphical overlays that layer visual elements onto charts, are effective to convey insights and context in financial narrative visualizations. However, automating graphical overlays is challenging due to complex narrative structures and limited understanding of effective overlays. To address the challenge, we first summarize the commonly used graphical overlays and narrative structures, and the proper correspondence between them in financial narrative visualizations, elected by a survey of 1752 layered charts with corresponding narratives. We then design FinFlier, a two-stage innovative system leveraging a knowledge-grounding large language model to automate graphical overlays for financial visualizations. The text-data binding module enhances the connection between financial vocabulary and tabular data through advanced prompt engineering, and the graphics overlaying module generates effective overlays with narrative sequencing. We demonstrate the feasibility and expressiveness of FinFlier through a gallery of graphical overlays covering diverse financial narrative visualizations. Performance evaluations and user studies further confirm system's effectiveness and the quality of generated layered charts.",,,,,Overlay; Computer science; Ground; Graphical model; Graphical user interface; Programming language; Software engineering; Artificial intelligence; Engineering; Electrical engineering,,,,,https://arxiv.org/abs/2412.06821,http://dx.doi.org/10.48550/arxiv.2412.06821,,10.48550/arxiv.2412.06821,,,0,,0,true,,green
163-013-127-060-515,Sovereign Syntax in Financial Disclosure: How LLMs Shape Trust in Tokenized Economies,2025-01-01,2025,preprint,,,Elsevier BV,,Agustin V. Startari,,,,,,Sovereignty; Business; Syntax; Financial system; International economics; International trade; Finance; Economics; Political science; Politics; Linguistics; Philosophy; Law,,,,,,http://dx.doi.org/10.2139/ssrn.5366241,,10.2139/ssrn.5366241,,,0,068-431-253-894-906,0,false,,
163-042-291-294-789,"FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models
  for Financial Applications with High-Performance Computing",2024-02-21,2024,preprint,arXiv (Cornell University),,,,Xiao-Yang Liu; Jie Zhang; Guoxuan Wang; Weiqing Tong; Anwar Walid,"Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetune LLMs for financial applications. We replace one conventional linear layer of the transformer structure with two narrower linear layers, which allows us to reduce the number of parameters by several orders of magnitude. By quantizing the parameters into low precision (8-bit and 4-bit), the memory consumption of the resulting model is further reduced. Compared with existing LLMs, our methods achieve a speedup of 1.3X and a model compression ratio of 2.64X for pretaining without accuracy drop. For finetuning, our methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks and financial tasks, respectively, and GPU memory consumption ratio of 6.3X. The sizes of our models are smaller than 0.59 GB, allowing inference on a smartphone.",,,,,Computer science; Supercomputer; Finance; Parallel computing; Business,,,,,https://arxiv.org/abs/2402.13533,http://dx.doi.org/10.48550/arxiv.2402.13533,,10.48550/arxiv.2402.13533,,,0,,0,true,,green
163-300-603-818-06X,Financial Named Entity Recognition: How Far Can LLM Go?,2025-01-04,2025,preprint,arXiv (Cornell University),,,,Yi-Te Lu; Yintong Huo,"The surge of large language models (LLMs) has revolutionized the extraction and analysis of crucial information from a growing volume of financial statements, announcements, and business news. Recognition for named entities to construct structured data poses a significant challenge in analyzing financial documents and is a foundational task for intelligent financial analytics. However, how effective are these generic LLMs and their performance under various prompts are yet need a better understanding. To fill in the blank, we present a systematic evaluation of state-of-the-art LLMs and prompting methods in the financial Named Entity Recognition (NER) problem. Specifically, our experimental results highlight their strengths and limitations, identify five representative failure types, and provide insights into their potential and challenges for domain-specific tasks.",,,,,Business; Named-entity recognition; Finance; Computer science; Management; Economics; Task (project management),,,,,https://arxiv.org/abs/2501.02237,http://dx.doi.org/10.48550/arxiv.2501.02237,,10.48550/arxiv.2501.02237,,,0,,0,true,,green
163-442-208-010-101,"FinLoRA: Finetuning Quantized Financial Large Language Models Using
  Low-Rank Adaptation",2024-12-15,2024,preprint,arXiv (Cornell University),,,,Dannong Wang; Daniel Kim; Bo Jin; Xingjian Zhao; Tianfan Fu; Steve Yang; Xiao-Yang Liu,"Finetuned large language models (LLMs) have shown remarkable performance in financial tasks, such as sentiment analysis and information retrieval. Due to privacy concerns, finetuning and deploying Financial LLMs (FinLLMs) locally are crucial for institutions. However, finetuning FinLLMs poses challenges including GPU memory constraints and long input sequences. In this paper, we employ quantized low-rank adaptation (QLoRA) to finetune FinLLMs, which leverage low-rank matrix decomposition and quantization techniques to significantly reduce computational requirements while maintaining high model performance. We also employ data and pipeline parallelism to enable local finetuning using cost-effective, widely accessible GPUs. Experiments on financial datasets demonstrate that our method achieves substantial improvements in accuracy, GPU memory usage, and time efficiency, underscoring the potential of lowrank methods for scalable and resource-efficient LLM finetuning.",,,,,Adaptation (eye); Rank (graph theory); Computer science; Psychology; Mathematics; Neuroscience; Combinatorics,,,,,https://arxiv.org/abs/2412.11378,http://dx.doi.org/10.48550/arxiv.2412.11378,,10.48550/arxiv.2412.11378,,,0,,0,true,,green
163-461-900-536-20X,Evaluating Economic Impact: An Investment Tool for Large Language Model Integration in Workweek Management,,2025,conference proceedings article,Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT 2025),,American Nuclear Society,,Chaitee Godbole; John Johnson; Vivek Agarwal; Ryan Spangler,,,,1508,1517,Investment (military); Computer science; Law; Politics; Political science,,,,,,http://dx.doi.org/10.13182/xyz-46868,,10.13182/xyz-46868,,,0,,0,false,,
163-902-979-537-424,FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis,2024-06-09,2024,conference proceedings article,Companion of the 2024 International Conference on Management of Data,,ACM,,Chao Zhang; Yuren Mao; Yijiang Fan; Yu Mi; Yunjun Gao; Lu Chen; Dongfang Lou; Jinshu Lin,"Text-to-SQL, which provides zero-code interface for operating relational databases, has gained much attention in financial analysis; because financial professionals may not be well-skilled in SQL programming. However, until now, there is no practical Text-to-SQL benchmark dataset for financial analysis, and existing Text-to-SQL methods have not considered the unique characteristics of databases in financial applications, such as commonly existing wide tables. To address these issues, we collect a practical Text-to-SQL benchmark dataset and propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL framework for financial analysis. The benchmark dataset, BULL, is collected from the practical financial analysis business of Hundsun Technologies Inc., including databases for fund, stock, and macro economy. Besides, the proposed LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for financial Text-to-SQL from the perspectives of prompt construction, parameter-efficient fine-tuning and output calibration. Experiments on BULL demonstrate that FinSQL achieves state-of-the-art performance at low cost, and it brings up to 36.64% improvement in few-shot cross-database scenarios.",,,93,105,Computer science; SQL; Programming language,,,,,https://arxiv.org/pdf/2401.10506 https://arxiv.org/abs/2401.10506,http://dx.doi.org/10.1145/3626246.3653375,,10.1145/3626246.3653375,,,0,007-110-994-763-006; 008-111-258-128-064; 038-611-099-858-447; 068-864-486-586-772; 117-929-228-389-733; 124-339-146-710-566,23,true,,green
164-297-336-589-216,Combining Financial Data and News Articles for Stock Price Movement Prediction Using Large Language Models,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Ali Elahi; Fatemeh Taghvaei,,,,4875,4883,Computer science; Stock (firearms); Stock price; Movement (music); Data modeling; Database; Series (stratigraphy); Engineering; Geology; Mechanical engineering; Paleontology; Philosophy; Aesthetics,,,,,,http://dx.doi.org/10.1109/bigdata62323.2024.10825449,,10.1109/bigdata62323.2024.10825449,,,0,027-636-893-541-115; 029-633-297-149-300; 037-749-424-116-89X; 039-212-036-339-20X; 052-484-787-569-890; 054-697-148-899-051; 058-111-366-740-174; 071-086-716-527-939; 072-452-492-825-42X; 076-584-687-121-609; 139-239-027-110-484; 190-050-627-029-962; 190-624-303-228-420; 192-806-908-794-847,4,false,,
164-385-791-408-253,Data-driven portfolio management for motion pictures industry: A new data-driven optimization methodology using a large language model as the expert,,2024,journal article,Computers & Industrial Engineering,03608352; 18790550,Elsevier BV,United Kingdom,Mohammad Alipour-Vaezi; Kwok-Leung Tsui,,197,,110574,110574,Motion (physics); Computer science; Portfolio; Industrial engineering; Artificial intelligence; Engineering drawing; Engineering; Economics; Financial economics,,,,,,http://dx.doi.org/10.1016/j.cie.2024.110574,,10.1016/j.cie.2024.110574,,,0,004-817-632-357-019; 007-017-884-126-763; 010-654-745-829-151; 010-784-229-475-292; 017-843-531-366-742; 018-463-260-551-728; 019-706-757-416-919; 020-062-092-347-983; 025-606-896-797-810; 027-057-022-867-746; 032-141-140-603-437; 036-512-296-605-311; 036-747-420-253-437; 038-110-909-174-624; 039-448-705-017-285; 040-984-347-672-03X; 047-998-715-682-111; 048-476-957-423-890; 048-670-318-877-019; 057-474-763-266-345; 063-592-773-988-530; 068-407-883-069-046; 069-495-117-896-439; 071-057-625-894-028; 072-385-285-248-271; 073-374-307-511-281; 074-411-338-021-978; 074-625-566-847-628; 094-175-215-344-922; 106-604-221-665-323; 114-014-038-263-166; 114-256-412-255-441; 119-523-376-523-95X; 120-445-008-776-800; 130-227-448-590-011; 135-067-714-451-092; 138-830-554-151-968; 139-586-501-470-10X; 139-693-696-642-584; 150-321-681-209-455; 168-316-269-779-912; 173-558-515-221-379; 181-330-312-455-764; 193-294-470-759-819,7,false,,
164-632-309-285-204,Using GPT-4 for Financial Advice,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Christian Fieberg; Lars Hornuf; David Streich,"We show that the recently released text-based artificial intelligence tool GPT-4 can provide suitable financial advice. The tool suggests specific investment portfolios that reflect an investor's individual circumstances such as risk tolerance, risk capacity, and sustainability preference. Notably, while the suggested portfolios display home bias and are rather insensitive to the investment horizon, historical risk-adjusted performance is on par with a professionally managed benchmark portfolio. Given the current inability of GPT-4 to provide full-service financial advice, it may be used by financial advisors as a back-office tool for portfolio recommendation.",,,,,Advice (programming); Portfolio; Finance; Preference; Financial services; Business; Investment (military); Financial plan; Actuarial science; Financial risk; Benchmark (surveying); Economics; Computer science; Microeconomics; Political science; Programming language; Geodesy; Politics; Law; Geography,,,,,https://www.econstor.eu/bitstream/10419/279279/1/cesifo1_wp10529.pdf http://hdl.handle.net/10419/279279,http://dx.doi.org/10.2139/ssrn.4499485,,10.2139/ssrn.4499485,,,0,,6,true,,green
164-932-727-111-279,Shai: A large language model for asset management,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Zhongyang Guo; Guanran Jiang; Zhongdan Zhang; Peng Li; Zhefeng Wang; Yinchun Wang,"This paper introduces ""Shai"" a 10B level large language model specifically designed for the asset management industry, built upon an open-source foundational model. With continuous pre-training and fine-tuning using a targeted corpus, Shai demonstrates enhanced performance in tasks relevant to its domain, outperforming baseline models. Our research includes the development of an innovative evaluation framework, which integrates professional qualification exams, tailored tasks, open-ended question answering, and safety assessments, to comprehensively assess Shai's capabilities. Furthermore, we discuss the challenges and implications of utilizing large language models like GPT-4 for performance assessment in asset management, suggesting a combination of automated evaluation and human judgment. Shai's development, showcasing the potential and versatility of 10B-level large language models in the financial sector with significant performance and modest computational requirements, hopes to provide practical insights and methodologies to assist industry peers in their similar endeavors.",,,,,Asset (computer security); Computer science; Asset management; Baseline (sea); Domain (mathematical analysis); Language model; Risk analysis (engineering); Knowledge management; Data science; Artificial intelligence; Business; Computer security; Finance; Political science; Mathematical analysis; Mathematics; Law,,,,,https://arxiv.org/abs/2312.14203,http://dx.doi.org/10.48550/arxiv.2312.14203,,10.48550/arxiv.2312.14203,,,0,,0,true,cc-by,green
165-488-046-662-815,"Evaluating the Utility-Truthfulness Trade-off in Large Language Model Agents: A Comparative Study of ChatGPT, Gemini, and Claude",2024-09-19,2024,preprint,,,Wiley,,Keijon Whitbeck; Lucas Brown; Sebastian Abernathy,,,,,,Computer science; Psychology,,,,,,http://dx.doi.org/10.22541/au.172676470.05209512/v1,,10.22541/au.172676470.05209512/v1,,,0,,0,true,,green
166-053-653-869-360,MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading,2025-07-13,2025,preprint,arXiv (Cornell University),,,,Siyi Wu; Junqiao Wang; Zhaoyang Guan; Leyi Zhao; Xinyuan Song; Xinyu Ying; Dexu Yu; Jinhao Wang; Hanlin Zhang; Michele Pak; Yangfan He; Yi Xin; Jianhui Wang; Tianyu Shi,"Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.",,,,,Modal; Finance; Business; Computer science; Economics; Chemistry; Polymer chemistry,,,,,https://arxiv.org/abs/2507.20474,http://dx.doi.org/10.48550/arxiv.2507.20474,,10.48550/arxiv.2507.20474,,,0,,0,true,,green
166-395-368-056-547,Bridging the Data Gap in Financial Sentiment: LLM-Driven Augmentation,,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop),,Association for Computational Linguistics,,Rohit Kumar; Chandan Nolbaria,,,,1246,1254,Bridging (networking); Computer science; Finance; Business; Computer security,,,,,,http://dx.doi.org/10.18653/v1/2025.acl-srw.98,,10.18653/v1/2025.acl-srw.98,,,0,,0,false,,
166-416-707-027-119,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Boyu Zhang; Hongyang Yang; Tianyu Zhou; Ali Babar; Xiao-Yang Liu,"Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15\% to 48\% performance gain in accuracy and F1 score.",,,,,Context (archaeology); Computer science; Artificial intelligence; Finance; Machine learning; Business; Geography; Archaeology,,,,,https://arxiv.org/abs/2310.04027,http://dx.doi.org/10.48550/arxiv.2310.04027,,10.48550/arxiv.2310.04027,,,0,,2,true,other-oa,green
166-488-492-389-229,Leveraging Large Language Models for Few-Shot KPI Extraction from Financial Reports,2024-12-15,2024,conference proceedings article,2024 IEEE International Conference on Big Data (BigData),,IEEE,,Tobias Deußer; Cong Zhao; Daniel Uedelhoven; Lorenz Sparrenberg; Lars Hillebrand; Christian Bauckhage; Rafet Sifa,,,,4864,4868,Computer science; Shot (pellet); Extraction (chemistry); Artificial intelligence; Natural language processing; Materials science; Chemistry; Chromatography; Metallurgy,,,,Ministry of Education,,http://dx.doi.org/10.1109/bigdata62323.2024.10825458,,10.1109/bigdata62323.2024.10825458,,,0,002-125-891-407-666; 016-983-709-052-539; 023-201-346-648-150; 023-251-033-176-717; 027-085-810-757-634; 032-471-769-867-765; 032-509-322-900-566; 037-297-734-603-234; 061-880-191-179-008; 069-204-884-170-45X; 081-197-082-194-291; 088-157-603-923-110; 093-943-738-289-339; 140-128-553-998-843; 154-042-138-562-576; 157-087-950-580-650; 183-826-061-268-885; 185-804-818-019-086; 189-973-804-567-981; 192-806-908-794-847; 195-697-682-736-933,1,false,,
166-490-014-333-119,From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models,,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Association for Computational Linguistics,,Yidan Wang; Yubing Ren; Yanan Cao; Binxing Fang,,,,10306,10322,Computer science; Digital watermarking; Programming language; Artificial intelligence; Image (mathematics),,,,,,http://dx.doi.org/10.18653/v1/2025.acl-long.509,,10.18653/v1/2025.acl-long.509,,,0,,0,false,,
166-695-032-889-136,No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices,,2024,conference proceedings article,Advances in Neural Information Processing Systems 37,,"Neural Information Processing Systems Foundation, Inc. (NeurIPS)",,Shengyuan Hu; Qi Pang; Virginia Smith; Wenting Zheng,,,,138756,138788,,,,,,,http://dx.doi.org/10.52202/079017-4402,,10.52202/079017-4402,,,0,,0,false,,
166-957-953-046-675,"Evaluating Large Language Models on Science, Technology, Engineering and Mathematical tasks including trade-offs",2025-01-01,2025,preprint,,,Elsevier BV,,Marek  Piotr Mysior; Christian Iniotakis,,,,,,Computer science; Science and engineering; Management science; Engineering ethics; Engineering,,,,,,http://dx.doi.org/10.2139/ssrn.5579226,,10.2139/ssrn.5579226,,,0,,0,false,,
167-113-522-589-922,Lora-Enhanced Llm Fine-Tuning With Legal Dataset Generation for Multi-Component Financial Extraction: Zero-Shot Vs Few-Shot Evaluation Framework for Gulf States Hr Benefits Calculation,2025-11-10,2025,conference proceedings article,2025 IEEE International Conference on E-Business Engineering (ICEBE),,IEEE,,Nasser Aldosari; Farookh Hussain,,,,202,210,,,,,,,http://dx.doi.org/10.1109/icebe68123.2025.00039,,10.1109/icebe68123.2025.00039,,,0,008-545-673-965-933; 034-603-390-501-560; 063-190-751-839-487; 063-277-960-975-874; 076-610-883-310-446; 117-585-238-287-312; 129-830-286-001-595; 142-308-678-818-202; 147-305-061-226-950; 154-042-138-562-576; 158-326-485-072-459,0,false,,
167-318-435-824-292,Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Saizhuo Wang; Hang Yuan; Leon Zhou; Lionel M. Ni; Heung-Yeung Shum; Jian Guo,"One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.",,,,,Alpha (finance); Heuristic; Computer science; Factor (programming language); Genetic programming; Investment (military); Machine learning; Artificial intelligence; Data science; Data mining; Programming language; Mathematics; Construct validity; Statistics; Politics; Political science; Law; Psychometrics,,,,,https://arxiv.org/abs/2308.00016,http://dx.doi.org/10.48550/arxiv.2308.00016,,10.48550/arxiv.2308.00016,,,0,,3,true,cc-by,green
167-397-773-176-249,The Effectiveness of GPT-4 as Financial News Annotator Versus Human Annotator in Improving the Accuracy and Performance of Sentiment Analysis,2024-03-03,2024,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Singapore,,Satyajeet Azad,,,,105,119,Computer science,,,,,,http://dx.doi.org/10.1007/978-981-99-8129-8_10,,10.1007/978-981-99-8129-8_10,,,0,019-636-623-822-371; 057-040-097-643-496; 060-344-442-281-504; 063-485-995-052-910; 097-923-900-268-69X; 154-025-530-406-258; 168-485-640-067-096,2,false,,
167-639-828-530-356,Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4,2025-06-26,2025,conference proceedings article,2025 IEEE Swiss Conference on Data Science (SDS),,IEEE,,Adria Pop; Jan Spörer,,,,135,141,Identification (biology); Equity (law); Computer science; Finance; Business; Biology; Political science; Botany; Law,,,,,,http://dx.doi.org/10.1109/sds66131.2025.00025,,10.1109/sds66131.2025.00025,,,0,004-853-485-857-424; 006-028-823-293-047; 010-116-756-881-344; 023-499-558-739-059; 027-624-785-134-901; 028-415-784-153-84X; 031-014-566-795-925; 035-014-823-490-790; 056-542-756-996-88X; 057-982-454-558-150; 058-930-735-839-575; 063-039-062-084-865; 066-508-279-094-544; 068-649-351-528-91X; 081-776-456-098-496; 082-595-603-544-251; 093-035-322-248-695; 097-275-375-816-244; 097-756-599-404-06X; 119-490-071-329-680; 125-622-190-136-098; 139-199-849-238-694; 141-596-253-901-521; 176-510-579-464-202; 190-971-494-166-723,1,false,,
167-661-668-802-877,Investment decisions driven by fine-tuned large language models and uniform manifold approximation and projection-supported clustering and hierarchical density-based spatial clustering,,2024,journal article,The Journal of Investment Strategies,20471238; 20471246,Infopro Digital Services Limited,,David Romoff,,,,,,Cluster analysis; Projection (relational algebra); Manifold (fluid mechanics); Computer science; Hierarchical clustering; Mathematics; Econometrics; Statistical physics; Artificial intelligence; Algorithm; Physics; Engineering; Mechanical engineering,,,,,,http://dx.doi.org/10.21314/jois.2025.001,,10.21314/jois.2025.001,,,0,,0,false,,
168-490-367-765-948,Agents are All you need: Elevating Trading Dynamics with Advanced Generative AI-Driven Conversational LLM Agents and Tools,2024-04-05,2024,conference proceedings article,2024 IEEE 9th International Conference for Convergence in Technology (I2CT),,IEEE,,Indrajit Kar; Zonunfeli Ralte; Maheshakumara Shivakumara; Rana Roy; Arti Kumari,"This paper presents the development of a groundbreaking LLM multi-agent system designed to optimize the Energy Exchange (EX)'s electricity trading. The system integrates cutting-edge, Generative AI, embedding-based deep learning models and LLM Agents to forecast electricity prices with heightened accuracy and facilitate interactive reporting. Our first agent performs advanced deep learning , tapping into IEX's rich databases for day-ahead and intraday market prices, alongside additional data streams such as weather and economic indicators. We eschew traditional predictive models in favor of sophisticated embedding-based models adept at discerning complex temporal patterns, enabling precise forecasts up to seven days ahead. Rigorous validation methods, including k-fold cross-validation, are applied, with accuracy gauged by metrics like Root Mean Squared Error (RMSE). The second agent is founded on a robust GenAI tools framework, translating intricate model predictions into intelligible reports and extract insights through another LLM based Agents. This interface adeptly handles energy market specifics, ensuring contextually relevant interactions. This tool's integration aims to enhance decision-making for market participants and to inject unprecedented predictive transparency into market dynamics. Our initiative heralds a transformative step toward realizing a data-centric, efficient, and customer-focused energy market in India, with potential expansion throughout the South Asian region powered by LLM and generative AI.",,,1,6,Generative grammar; Computer science; Dynamics (music); Artificial intelligence; Human–computer interaction; Psychology; Pedagogy,,,,,,http://dx.doi.org/10.1109/i2ct61223.2024.10543356,,10.1109/i2ct61223.2024.10543356,,,0,005-081-792-080-087; 007-794-532-423-090; 014-297-666-384-206; 024-384-717-536-846; 034-667-053-206-934; 040-690-431-609-769; 043-871-173-599-124; 046-553-799-545-625; 050-888-794-384-278; 052-053-423-285-358; 053-110-349-037-413; 064-318-593-637-308; 070-574-902-371-508; 074-489-849-085-033; 092-329-704-874-971; 104-425-430-895-452; 105-239-509-538-009; 126-372-197-661-482; 139-956-968-819-489; 160-685-006-095-116; 168-014-544-788-060,3,false,,
168-502-627-259-036,Reasoning with financial regulatory texts via Large Language Models,,2025,journal article,Journal of Behavioral and Experimental Finance,22146350; 22146369,Elsevier BV,Netherlands,Bledar Fazlija; Meriton Ibraimi; Aynaz Forouzandeh; Arber Fazlija,,47,,101067,101067,Economics; Business; Finance,,,,Zürcher Hochschule für Angewandte Wissenschaften,,http://dx.doi.org/10.1016/j.jbef.2025.101067,,10.1016/j.jbef.2025.101067,,,0,002-663-392-287-514; 004-239-413-234-842; 009-278-725-876-522; 012-805-413-375-374; 019-515-167-233-141; 021-369-884-343-908; 032-338-966-279-53X; 035-730-317-696-224; 060-277-408-364-608; 065-371-617-881-903; 097-914-300-871-028; 119-689-861-768-766; 136-134-457-808-751,0,true,cc-by,hybrid
168-617-495-471-122,<p>SecureFinAI: Integrating LLM-Generated Signals with Reinforcement Learning for High-Frequency Crypto Trading</p>,,2025,preprint,,,Elsevier BV,,Marek Grzesiak; Raghav Aggarwal,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5714622,,10.2139/ssrn.5714622,,,0,,0,false,,
168-743-650-604-982,LLM-Enhanced Financial Appraisal of Mechanical Carbon Capture and Storage Systems through Automated Technical-Economic Analysis,2025-09-22,2025,journal article,Computer Life,18194818,Darcy & Roy Press Co. Ltd.,,Yao Ge,"<jats:p>The escalating deployment of mechanical carbon capture and storage (CCS) technologies demands sophisticated financial appraisal methodologies that can accurately assess the complex technical-economic relationships inherent in these capital-intensive systems. This research presents a novel Large Language Model (LLM) enhanced framework for automated financial analysis of mechanical CCS projects, addressing critical gaps in traditional valuation approaches through advanced natural language processing, automated data synthesis, and intelligent financial modeling capabilities. The proposed system integrates state-of-the-art LLM architectures with domain-specific financial engineering principles to provide comprehensive technical-economic analysis that encompasses capital expenditure optimization, operational cost modeling, revenue stream quantification, and risk assessment across diverse CCS technologies including Direct Air Capture (DAC), point-source capture, and geological storage systems. Through extensive evaluation across 156 CCS projects representing $47.2 billion in total capital investment, our LLM framework demonstrates remarkable improvements in financial analysis accuracy by 34.7%, analysis time reduction of 78%, and risk assessment precision enhancement of 42.3% compared to traditional financial modeling approaches. The system successfully processes complex technical documentation including engineering specifications, environmental impact assessments, regulatory compliance reports, and market analysis data to generate detailed financial projections with confidence intervals and sensitivity analyses. Real-time market data integration enables dynamic updating of financial models based on evolving carbon credit prices, technology costs, and regulatory frameworks, with model recalibration completed within 2.7 hours compared to weeks required for manual analysis updates. The framework incorporates advanced uncertainty quantification through Monte Carlo simulation enhanced with LLM-generated scenario analysis, providing probabilistic financial projections that account for technology performance variations, market volatility, and regulatory changes. Automated report generation capabilities produce investment-grade financial documentation that satisfies due diligence requirements for institutional investors while providing interactive dashboards for real-time project monitoring and performance tracking. Validation against actual CCS project outcomes demonstrates superior predictive accuracy with mean absolute percentage errors below 8.3% for capital cost estimation and 11.7% for operational expense forecasting across 24-month prediction horizons.</jats:p>",13,3,16,23,Economic analysis; Business; Finance; Computer science; Economics; Agricultural economics,,,,,https://computer-life.org/index.php/ojs/article/download/23/23 https://doi.org/10.54097/3dwqbf59,http://dx.doi.org/10.54097/3dwqbf59,,10.54097/3dwqbf59,,,0,001-783-909-419-816; 003-837-960-466-873; 008-436-067-917-269; 011-112-865-135-787; 021-112-105-736-785; 029-279-127-060-822; 033-871-146-145-191; 044-659-064-803-286; 045-122-054-097-070; 059-154-935-553-894; 060-789-556-600-034; 061-344-262-458-559; 064-590-229-849-833; 072-514-094-007-043; 095-821-582-209-108; 100-910-366-727-836; 115-578-088-117-052; 127-276-665-277-614; 139-855-190-177-573; 142-708-646-299-64X; 153-160-297-137-084; 178-959-014-026-283; 199-392-235-944-050,0,false,,
168-905-676-553-211,FinPersona: An LLM-Driven Conversational Agent for Personalized Financial Advising,2025-04-03,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Takehiro Takayanagi; Masahiro Suzuki; Kiyoshi Izumi; Javier Sanz-Cruzado; Richard McCreadie; Iadh Ounis,,,,13,18,Computer science; Human–computer interaction; Finance; Dialog system; Artificial intelligence; Software engineering; World Wide Web; Business; Dialog box,,,,,,http://dx.doi.org/10.1007/978-3-031-88720-8_3,,10.1007/978-3-031-88720-8_3,,,0,016-531-084-325-607; 056-318-118-175-394; 102-487-387-999-635; 102-692-521-084-587; 122-704-973-028-245; 133-419-572-269-204; 139-239-027-110-484; 141-251-127-978-049; 153-610-124-467-363; 165-089-307-573-426; 175-961-542-771-176; 187-510-765-035-187,4,false,,
169-258-498-100-153,"Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from
  Monetary Policy Conference Calls with LLM",2024-03-24,2024,preprint,arXiv (Cornell University),,,,Kun Ouyang; Yi Liu; Shicheng Li; Ruihan Bao; Keiko Harimoto; Xu Sun,"Financial prediction from Monetary Policy Conference (MPC) calls is a new yet challenging task, which targets at predicting the price movement and volatility for specific financial assets by analyzing multimodal information including text, video, and audio. Although the existing work has achieved great success using cross-modal transformer blocks, it overlooks the potential external financial knowledge, the varying contributions of different modalities to financial prediction, as well as the innate relations among different financial assets. To tackle these limitations, we propose a novel Modal-Adaptive kNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER. Specifically, MANAGER resorts to FinDKG to obtain the external related knowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit BERT (HuBERT) to extract the video and audio features, respectively. Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph that fully characterizes the semantic relations among text, external knowledge, video and audio, to adaptively utilize the information in different modalities, with ChatGLM2 as the backbone. Extensive experiments on a publicly available dataset Monopoly verify the superiority of our model over cutting-edge methods.",,,,,Modal; Monetary policy; Graph; Computer science; Finance; Business; Economics; Theoretical computer science; Monetary economics; Chemistry; Polymer chemistry,,,,,https://arxiv.org/abs/2403.16055,http://dx.doi.org/10.48550/arxiv.2403.16055,,10.48550/arxiv.2403.16055,,,0,,0,true,,green
169-459-193-375-317,Gendered AI in banking services: the influence of financial chatbots' gender on consumer behaviour,2025-03-04,2025,journal article,International Journal of Bank Marketing,02652323; 17585937,Emerald,United Kingdom,Irene Roozen; Mariet Raedts; Christel Claeys; Giulia Di Gennaro,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title><jats:p>This study explores whether the gender of a financial chatbot influences how competent potential users perceive the chatbot to be and whether they would choose to use the chatbot themselves.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title><jats:p>The study had a between-subjects design: participants (<jats:italic>N</jats:italic> = 420, ages between 18 and 75) viewed and evaluated either a male or a female financial chatbot. Data were collected via an online questionnaire.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title><jats:p>Male chatbots led to a significantly higher willingness to consult the service and were perceived as more competent. Furthermore, AI-literacy and sensitivity to gender perspectives significantly influenced these findings.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Practical implications</jats:title><jats:p>The findings offer actionable insights for financial institutions to optimise chatbot interactions by considering user preferences for male versus female chatbots, potentially guiding the development of more effective AI-driven financial services. Companies can use these insights to tailor chatbot gender strategies to meet user expectations better and enhance service satisfaction.</jats:p></jats:sec><jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title><jats:p>This study provides novel empirical evidence on the impact of chatbot gender in male-dominated financial services, revealing how AI literacy and gender sensitivity influence consumer behaviour and perceptions. Additionally, it contributes to the theoretical understanding of AI gendering and its societal implications.</jats:p></jats:sec>",43,6,1231,1251,Business; Financial services; Marketing; Consumer behaviour; Finance,,,,,,http://dx.doi.org/10.1108/ijbm-07-2024-0421,,10.1108/ijbm-07-2024-0421,,,0,004-221-662-200-111; 007-042-092-181-558; 010-232-879-372-214; 013-470-693-377-976; 015-826-816-936-502; 018-380-452-881-875; 022-391-395-977-333; 025-633-953-278-560; 028-289-734-658-693; 028-702-069-138-546; 028-921-181-344-710; 029-191-997-332-645; 030-951-221-183-894; 031-420-886-532-773; 031-827-604-696-820; 032-637-445-493-370; 038-531-474-512-959; 049-583-574-785-835; 056-044-390-568-79X; 056-810-730-290-406; 058-450-914-323-707; 063-233-158-964-102; 064-811-520-753-841; 066-205-482-810-632; 069-106-373-436-798; 072-054-394-414-009; 073-303-412-668-303; 075-023-072-732-60X; 076-024-914-612-841; 080-044-820-111-424; 082-864-119-365-051; 088-388-348-081-873; 088-961-120-983-298; 096-214-959-565-973; 104-366-725-015-566; 106-034-730-233-922; 108-834-059-755-131; 110-981-195-477-326; 112-818-453-623-529; 123-600-674-223-112; 124-107-341-800-724; 125-119-044-608-941; 135-574-159-381-537; 139-728-534-969-805; 142-654-114-772-052; 155-795-682-200-352; 160-250-899-202-640; 171-902-100-404-939; 172-093-928-148-276; 174-990-700-664-891; 181-795-576-723-97X; 188-851-151-889-26X; 190-568-139-934-281; 199-559-526-412-734,1,false,,
169-566-596-694-201,The application of chatbot on Vietnamese misgrant workers' right protection in the implementation of new generation free trade agreements (FTAS),2022-03-14,2022,journal article,AI & SOCIETY,09515666; 14355655,Springer Science and Business Media LLC,Germany,Quoc Nguyen Phan; Chin-Chin Tseng; Thu Thi Hoai Le; Thi Bich Ngoc Nguyen,,38,4,1771,1783,Vietnamese; Chatbot; Government (linguistics); Business; Investment (military); Public relations; Internet privacy; Political science; World Wide Web; Computer science; Politics; Law; Philosophy; Linguistics,,,,,,http://dx.doi.org/10.1007/s00146-022-01416-z,,10.1007/s00146-022-01416-z,,,0,002-093-183-923-279; 005-049-330-086-214; 005-927-979-767-23X; 006-494-449-801-037; 007-454-697-247-661; 011-049-175-769-580; 012-093-832-883-127; 012-909-667-618-467; 015-583-058-931-02X; 021-541-700-929-608; 022-338-712-199-828; 023-029-400-817-751; 030-280-854-277-466; 032-684-378-218-505; 040-481-565-458-221; 048-262-908-312-381; 050-648-956-355-953; 056-859-060-200-375; 064-613-169-606-125; 068-459-229-559-82X; 084-667-197-932-98X; 085-914-840-412-775; 101-123-132-384-475; 110-417-871-298-706; 119-275-549-696-155; 133-688-880-901-254; 137-389-042-443-448; 138-455-737-231-862; 144-326-018-681-994; 144-370-345-063-147; 150-859-551-059-502; 158-154-417-527-126; 158-376-152-577-868; 164-158-558-942-572; 179-890-521-841-542; 188-153-369-133-993; 194-316-702-362-118,5,false,,
169-608-032-682-224,Teaching LLM Students Financial Law,,2012,,,,,,Alexander Kozyrin,"Kozyrin Alexander - Professor, Head of the Department of Financial Law, National Research University Higher School of Economics. Doctor of Juridical Sciences. E-mail: kozyrine@mail.ru Address: National Research University - Higher School of Economics, 20, Myasnitskaya str., Moscow, 101000, Russian Federation.The article discusses special features of teaching financial law (as well as other subjects of financial law specialization). A significant attention is drawn to the role of the LLM program under its status among other stages of education according to the educational reform which has started at the end of the 20th century. It is noted that the current RF law On Education,1992 is becoming obsolete. LLM programs have been introduced almost in all Russian universities. The paper poses a question if LLM programs in Russia are the main and final stage of university education (entitling to a diploma) or it represents a research stage (entitling to the first academic degree)? Ambiguity is seen in the successive character of LLB and LLM programs as well as LLM and postgraduate programs, LLM and candidate of science degrees. It is stressed that the standards of the Education Ministry make the disciplines financial and tax laws obligatory for LLB programs overlapping. However, LLM programs are open for those who do not have an LLB degree and thus such students did not study the subjects vital for lawyers. The author makes a number of proposals to improve the situation. He argues that LLM programs should be an integral part of university legal education, but postgraduate programs should serve as a post higher education qualification process. Most disciplines on finances and law may be taught in an abridged form at LLM programs. However, postgraduate programs require a more scrutinized approach to teaching them. In particular, the discipline History of Financial Law may last for one or two years on postgraduate programs. At the same time, the author is against amalgamating financial private law and civil law.",,2,173,188,Finance; Higher education; Political science; Civil law (legal system); Law; Ambiguity; University education; Christian ministry; Legal education; Private law; Schools of economic thought,,,,,https://www.hse.ru/en/mag/nohead/pravo/2012--2/55145890.html https://law-journal.hse.ru/en/2012--2/55145890.html,https://www.hse.ru/en/mag/nohead/pravo/2012--2/55145890.html,,,107104043,,0,,0,false,,
169-626-346-768-150,"FinSentio: A Modular LLM-Powered Financial Assistant for Investment Guidance, Risk Analysis, and Education",2025-09-17,2025,conference proceedings article,2025 10th International Conference on Computer Science and Engineering (UBMK),,IEEE,,Yavuz Selim Sever; Başak Nehir Artan; Berna Danışman; Nuran Er; Murat Karakaya,,,,119,124,Modular design; Investment (military); Finance; Business; Engineering management; Computer science; Engineering; Political science; Operating system; Politics; Law,,,,,,http://dx.doi.org/10.1109/ubmk67458.2025.11207001,,10.1109/ubmk67458.2025.11207001,,,0,021-745-993-572-933; 038-241-735-129-443; 168-485-640-067-096; 183-434-875-935-824,0,false,,
169-923-753-227-117,An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Ziao Wang; Jianning Wang; Junda Wu; Xiaofeng Zhang,"At the beginning era of large language model, it is quite critical to generate a high-quality financial dataset to fine-tune a large language model for financial related tasks. Thus, this paper presents a carefully designed data creation pipeline for this purpose. Particularly, we initiate a dialogue between an AI investor and financial expert using ChatGPT and incorporate the feedback of human financial experts, leading to the refinement of the dataset. This pipeline yielded a robust instruction tuning dataset comprised of 103k multi-turn chats. Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge. The promising experimental results verify that our approach led to significant advancements in generating accurate, relevant, and financial-style responses from AI models, and thus providing a powerful tool for applications within the financial sector.",,,,,Pipeline (software); Computer science; Quality (philosophy); Language model; Finance; Financial modeling; Data science; Machine learning; Data mining; Artificial intelligence; Programming language; Business; Philosophy; Epistemology,,,,,https://arxiv.org/abs/2308.01415,http://dx.doi.org/10.48550/arxiv.2308.01415,,10.48550/arxiv.2308.01415,,,0,,0,true,cc-by,green
169-981-584-055-080,SusGen-GPT: A Data-Centric LLM for Financial NLP and Sustainability Report Generation,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: NAACL 2025,,Association for Computational Linguistics,,Qilong Wu; Xiaoneng Xiang; Huang Hejia; Xuan Wang; Yeo Wei Jie; Ranjan Satapathy; Ricardo Shirota Filho; Bharadwaj Veeravalli,,,,1184,1203,Computer science; Sustainability; Natural language processing; Artificial intelligence; Ecology; Biology,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-naacl.66,,10.18653/v1/2025.findings-naacl.66,,,0,,1,false,,
170-110-595-397-31X,Large Language Models for Financial Time Series Forecasting,2024-01-01,2024,preprint,,,Elsevier BV,,Miquel Noguer I Alonso; Rodolfo Pereira Franklin,,,,,,Series (stratigraphy); Time series; Finance; Computer science; Economics; Machine learning; Geology; Paleontology,,,,,,http://dx.doi.org/10.2139/ssrn.4988022,,10.2139/ssrn.4988022,,,0,,0,false,,
170-623-688-250-660,Smart Financial Learning: An Intelligent Agent with Chatbot Assistance,2025-11-22,2025,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Switzerland,,Siddhant Sawant; Sajal Nampalliwar; Ansh Masand; Variza Negi,,,,347,356,,,,,,,http://dx.doi.org/10.1007/978-3-032-06677-0_34,,10.1007/978-3-032-06677-0_34,,,0,029-989-780-719-243; 031-707-943-327-265; 035-303-261-889-193; 037-292-691-107-798; 046-248-464-565-463; 063-604-319-865-27X; 083-263-009-660-984; 084-930-883-475-454; 103-377-098-657-355; 108-261-234-815-184; 117-649-595-859-622; 118-820-147-264-389; 163-183-469-398-382; 183-295-352-028-662; 194-636-213-064-833; 199-520-819-756-452,0,false,,
170-629-592-611-596,When LLMs Go Abroad: Foreign Bias in AI Financial Predictions,2025-01-01,2025,preprint,,,Elsevier BV,,Sean Cao; Charles C. Y. Wang; Yi Xiang,,,,,,Economics; Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5440116,,10.2139/ssrn.5440116,,,0,004-969-538-054-17X; 012-644-413-132-255; 014-794-347-854-941; 020-227-352-842-036; 023-909-465-959-73X; 026-151-433-369-906; 027-846-787-035-679; 028-023-315-942-165; 034-816-512-027-190; 047-325-472-840-931; 049-522-967-316-715; 053-591-547-250-936; 081-850-624-262-623; 089-606-362-434-742; 102-873-989-720-49X; 103-990-895-002-373; 107-361-788-668-43X; 130-185-490-312-282; 173-380-641-384-982; 198-520-814-627-145,1,false,,
170-662-739-548-46X,Security Considerations for the Application of Large Language Models in the Financial Sector,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Michael Reynolds,,,,,,Financial sector; Business; Computer science; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5055959,,10.2139/ssrn.5055959,,,0,041-922-541-075-522; 127-777-867-209-148,0,false,,
170-704-386-774-623,"When Context Grows, So Does the Challenge: Human Oversight in LLM Evaluation of Financial Tables",2025-09-27,2025,conference proceedings article,Adjunct Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology,,ACM,,Arijit Sehanobish; Shirley Anderson; Guillaume Michel; Mike Arov,"We evaluate the reliability of Large Language Models (LLMs)-as-a-Judge for comparing financial tables derived from text-to-SQL pipelines. Our findings show degraded performance on larger tables. To address this, we propose a triggered oversight framework that defers ambiguous cases to human experts.",,,1,3,Context (archaeology); Computer science; Finance; Accounting; Business; Paleontology; Biology,,,,,https://dl.acm.org/doi/pdf/10.1145/3746058.3758457 https://doi.org/10.1145/3746058.3758457,http://dx.doi.org/10.1145/3746058.3758457,,10.1145/3746058.3758457,,,0,015-844-686-206-315; 062-336-341-564-639; 072-939-822-350-266; 143-381-393-380-048; 161-627-424-611-380,0,true,,bronze
170-868-542-195-174,WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models,,2024,conference proceedings article,Findings of the Association for Computational Linguistics: NAACL 2024,,Association for Computational Linguistics,,Piotr Molenda; Adian Liusie; Mark Gales,,,,3515,3525,Digital watermarking; Computer science; Quality (philosophy); Artificial intelligence; Image (mathematics); Philosophy; Epistemology,,,,,https://arxiv.org/pdf/2403.19548 https://arxiv.org/abs/2403.19548,http://dx.doi.org/10.18653/v1/2024.findings-naacl.223,,10.18653/v1/2024.findings-naacl.223,,,0,,2,true,,green
171-059-001-278-563,Quantifying legal risk with Large Language Models: A text-based investment signal,2025-09-11,2025,journal article,Journal of High School Science,25756206,Journal of High School Science,,James Offutt; Yale Xie,"<jats:p>This study developed a novel, text-based investment signal by applying Large Language Model (LLM) technology to quantify changes in legal risk disclosed in twin Securities and Exchange Commission (SEC) 10-Q and 10-K filings. Using the OpenAI GPT-4o LLM, the method compared the Legal Proceedings sections and associated footnotes from period-adjacent filings to score directional changes in legal risk. All firms with non-zero scores were incorporated into monthly long-short sub-portfolios, where negative scores were ‘longed’ and positive scores were shorted. Positions were dynamically weighted using a hyperbolic tangent transformation based on each firm’s historical volatility and magnitude of legal risk change score. This approach offers a novel method for extracting firm-specific legal risk signals that have been historically underutilized in asset pricing models. It has potential applications in alpha generation, risk management, litigation monitoring, ESG compliance, and research automation. Ordinary Least Squares (OLS) regression was used to test the portfolio’s returns against the standard Fama-French five-factor model plus momentum. The long-short strategy produced a statistically significant mean monthly alpha of 0.471% (annualized to 5.80%) for each sub-portfolio, indicating excess return generation beyond traditional risk factors, though with exposure to a large-to-moderate drawdown. All results were unlevered. Given the strategy’s measured volatility and delta exposure, returns could be further scaled according to risk tolerance via leverage. Controlling for standard risk factors, the strategy’s returns were partially explained by exposure to both growth and conservative-investment firms, but a meaningful portion of alpha remained unexplained by market risk, size, profitability, or momentum, highlighting the distinct contribution of the legal risk signal. An LLM-only, equal-weight mirror test—portfolios traded solely on the LLM legal-risk score (no position sizing or additional parameters)—was also executed: the normal equal-weight portfolio (long risk decreases / short risk increases) decisively outperformed its mirror (same trades but with reversed longs/shorts) over both control and winsorized iterations. These findings suggest that LLMs can uncover priced legal risk shifts and generate predictive signals beyond traditional textual analysis techniques.</jats:p>",9,3,,,SIGNAL (programming language); Investment (military); Computer science; Econometrics; Linguistics; Actuarial science; Natural language processing; Artificial intelligence; Business; Economics; Political science; Philosophy; Law; Programming language; Politics,,,,,,http://dx.doi.org/10.64336/001c.144303,,10.64336/001c.144303,,,0,,0,false,,
171-084-545-192-105,Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM,2024-12-11,2024,preprint,arXiv (Cornell University),,,,Van-Duc Le; Hai-Thien To,"Financial analysis heavily relies on the evaluation of earnings reports to gain insights into company performance. Traditional generation of these reports requires extensive financial expertise and is time-consuming. With the impressive progress in Large Language Models (LLMs), a wide variety of financially focused LLMs has emerged, addressing tasks like sentiment analysis and entity recognition in the financial domain. This paper presents a novel challenge: developing an LLM specifically for automating the generation of earnings reports analysis. Our methodology involves an in-depth analysis of existing earnings reports followed by a unique approach to fine-tune an LLM for this purpose. This approach combines retrieval augmentation and the generation of instruction-based data, specifically tailored for the financial sector, to enhance the LLM's performance. With extensive financial documents, we construct financial instruction data, enabling the refined adaptation of our LLM to financial contexts. Preliminary results indicate that our augmented LLM outperforms general open-source models and rivals commercial counterparts like GPT-3.5 in financial applications. Our research paves the way for streamlined and insightful automation in financial report generation, marking a significant stride in the field of financial analysis.",,,,,Earnings; Financial analysis; Business; Finance; Economics; Financial system,,,,,https://arxiv.org/abs/2412.08179,http://dx.doi.org/10.48550/arxiv.2412.08179,,10.48550/arxiv.2412.08179,,,0,,0,true,,green
171-549-941-004-660,Large Language Model in Financial Regulatory Interpretation,2024-10-22,2024,conference proceedings article,2024 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr),,IEEE,,Zhiyu Cao; Zachary Feinstein,,,,1,7,Interpretation (philosophy); Computer science; Programming language,,,,,,http://dx.doi.org/10.1109/cifer62890.2024.10772991,,10.1109/cifer62890.2024.10772991,,,0,,4,false,,
171-612-045-195-808,COVID19: INTELLIGENT CHATBOT AND FINANCIAL AID APPLICATION,2022-11-08,2022,conference proceedings article,Proceedings of the International Conferences on Applied Computing 2022 and WWW/Internet 2022,,IADIS Press,,Kenza Douifir; Naoual Chaouni Benabdellah; Ali Idri,"Covid19 has devastated all continents causing disasters not only on the health sector but also at social, economic, and at political levels. The world is still trying to eradicate the virus. One of the measures taken is to inform citizens about the",,,,,Chatbot; Politics; Business; Computer security; Computer science; Political science; World Wide Web; Law,,,,,http://www.iadisportal.org/components/com_booklibrary/ebooks/202208C023.pdf https://doi.org/10.33965/ac_icwi2022_202208c023,http://dx.doi.org/10.33965/ac_icwi2022_202208c023,,10.33965/ac_icwi2022_202208c023,,,0,,0,true,,bronze
171-767-014-669-018,Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation,2024-11-13,2024,conference proceedings article,"2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)",,IEEE,,Andrei Lazarev; Dmitrii Sedov,"The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.",,,317,321,Computer science; Data science,,,,,http://arxiv.org/pdf/2510.01225 http://arxiv.org/abs/2510.01225,http://dx.doi.org/10.1109/summa64428.2024.10803746,,10.1109/summa64428.2024.10803746,,,0,030-089-078-880-151; 129-011-436-906-885,3,true,,green
171-944-508-916-837,"Vision, Voice, and Text: Pioneering Zero-shot Multimodal LLMs for Sentiment-driven Investment",2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Su Tan; Chi Chiu So; Yueyue Sun; Jun-Min Wang; Wai Keung Anthony Loh; Siu Pang Yung,,,,960,968,,,,,,,http://dx.doi.org/10.1145/3768292.3770368,,10.1145/3768292.3770368,,,0,009-661-190-177-945; 014-503-732-193-606; 014-613-376-281-980; 018-033-516-845-368; 033-663-067-306-974; 038-122-614-092-11X; 044-007-627-437-715; 047-773-957-263-019; 048-042-669-571-970; 050-411-223-690-003; 053-042-256-500-905; 059-848-628-902-595; 064-208-031-850-492; 070-504-773-098-766; 074-534-527-838-135; 075-356-529-898-550; 079-043-106-593-18X; 080-828-384-981-970; 083-489-102-388-067; 102-068-013-162-56X; 102-269-667-054-251; 139-711-048-505-537; 157-623-028-732-778; 186-827-515-649-986,0,false,,
171-948-606-642-823,Fine-tuning of financial Large Language Model and application at edge device,2024-01-26,2024,conference proceedings article,"Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering",,ACM,,Juntao Zeng; Bo Chen; Yuandan Deng; Weiqin Chen; Yanlin Mao; Jiawei Li,,,,42,47,Enhanced Data Rates for GSM Evolution; Language model; Computer science; Natural language processing; Artificial intelligence,,,,,,http://dx.doi.org/10.1145/3672758.3672766,,10.1145/3672758.3672766,,,0,062-342-667-327-87X,0,false,,
172-544-999-482-343,Augmenting Financial Planning and Analysis: Leveraging AI and LLMs for Predictive Insights and Strategic Foresight,2025-08-14,2025,journal article,European Modern Studies Journal,25229400,Lomaka & Romina Publisher,,Gautham Panneer Selvam,"<jats:p>The Financial Planning &amp; Analysis (FP&amp;A) profession is at a critical turning point, shifting from being a back-end reporting function to becoming a forward-looking predictive engine for strategic insight. This shift is because of the adoption of artificial intelligence (AI), specifically machine learning (ML) and large language models (LLMs). Old-school FP&amp;A processes, constrained by manual intervention and past-centric data, are making way for sophisticated analytical paradigms. ML models now reveal subtle patterns in large sets of data, across internal metrics and external signals, to make highly sophisticated, probabilistic projections. At the same time, LLMs are transforming unstructured data analysis, revealing actionable insight from financial statements and market research. This fusion of technologies makes forecasts more accurate, allows for real-time anomaly detection, and gives deep strategic context. Yet successful adoption is no technical upgrade; it's a strategic imperative dependent on strong data infrastructure, severe model governance, and a focus on upskilling human capital. This paper offers a complete roadmap for leaders to successfully embark on this complicated journey, making their finance function a genuine force of strategic foresight and competitive edge.</jats:p>",9,4,474,486,Futures studies; Strategic planning; Business; Finance; Political science; Computer science; Artificial intelligence; Marketing,,,,,,http://dx.doi.org/10.59573/emsj.9(4).2025.46,,10.59573/emsj.9(4).2025.46,,,0,033-871-146-145-191; 087-278-150-227-634; 150-766-107-126-307,0,false,,
172-601-162-915-184,A Study on the Influence of Large Language Model on Financial System--Taking DeepSeek as an Example,2025-07-18,2025,conference proceedings article,"Proceedings of the 2025 International Conference on Big Data, Artificial Intelligence and Digital Economy",,ACM,,Yueyao Yang; Heshan Zhou; Yingxue Xia,,,,1,5,Computer science,,,,,,http://dx.doi.org/10.1145/3767052.3767053,,10.1145/3767052.3767053,,,0,,0,false,,
172-756-346-289-081,Hybrid Architectures that Combine LLMs and Predictive Analytics for Next-Generation Financial Modeling,2025-10-20,2025,journal article,Mathematical Modeling and Algorithm Application,30060842; 30062020,Darcy & Roy Press Co. Ltd.,,Shiyang Chen; Shaochen Ren; Qun Zhang,"<jats:p>The convergence of large language models (LLMs) and predictive analytics represents a transformative paradigm shift in financial modeling, offering unprecedented capabilities for processing multimodal data and generating actionable insights. This review examines the evolution, architecture, and applications of hybrid systems that integrate LLMs with traditional predictive models to address complex challenges in financial forecasting, risk management, and portfolio optimization. Recent advances in natural language processing (NLP) have enabled LLMs to extract nuanced sentiment and contextual information from vast textual datasets, while deep learning (DL) architectures such as long short-term memory (LSTM), gated recurrent units (GRU), and transformer-based models have demonstrated superior performance in capturing temporal dependencies within financial time series. The integration of these technologies through early, intermediate, and late fusion strategies has yielded hybrid architectures that leverage the complementary strengths of linguistic understanding and numerical prediction. This paper synthesizes current research on financial LLMs including BloombergGPT and FinGPT, explores attention mechanisms and multimodal data fusion techniques, and evaluates the application of these hybrid systems across sentiment analysis, stock prediction, portfolio management, and fraud detection. Critical challenges including explainability, regulatory compliance, computational efficiency, and data quality are examined alongside emerging solutions. The review concludes that hybrid architectures combining LLMs and predictive analytics represent the future of financial modeling, offering enhanced accuracy, interpretability, and adaptability to dynamic market conditions, while emphasizing the need for continued research in model transparency, ethical AI deployment, and standardized evaluation frameworks.</jats:p>",6,2,31,43,Predictive analytics; Analytics; Computer science; Data science,,,,,https://drpress.org/ojs/index.php/mmaa/article/download/32152/31474 https://doi.org/10.54097/j1cjb453,http://dx.doi.org/10.54097/j1cjb453,,10.54097/j1cjb453,,,0,,1,false,,
172-794-031-491-602,Financial Knowledge Large Language Model,2024-06-29,2024,preprint,arXiv (Cornell University),,,,Cehao Yang; Chengjin Xu; Yiyan Qi,"Artificial intelligence is making significant strides in the finance industry, revolutionizing how data is processed and interpreted. Among these technologies, large language models (LLMs) have demonstrated substantial potential to transform financial services by automating complex tasks, enhancing customer service, and providing detailed financial analysis. Firstly, we introduce IDEA-FinBench, an evaluation benchmark specifically tailored for assessing financial knowledge in large language models (LLMs). This benchmark utilizes questions from two globally respected and authoritative financial professional exams, aimimg to comprehensively evaluate the capability of LLMs to directly address exam questions pertinent to the finance sector. Secondly, we propose IDEA-FinKER, a Financial Knowledge Enhancement framework designed to facilitate the rapid adaptation of general LLMs to the financial domain, introducing a retrieval-based few-shot learning method for real-time context-level knowledge injection, and a set of high-quality financial knowledge instructions for fine-tuning any general LLM. Finally, we present IDEA-FinQA, a financial question-answering system powered by LLMs. This system is structured around a scheme of real-time knowledge injection and factual enhancement using external knowledge. IDEA-FinQA is comprised of three main modules: the data collector, the data querying module, and LLM-based agents tasked with specific functions.",,,,,Business; Finance,,,,,https://arxiv.org/abs/2407.00365,http://dx.doi.org/10.48550/arxiv.2407.00365,,10.48550/arxiv.2407.00365,,,0,,0,true,,green
173-134-216-469-869,Leveraging Large Language Models for Enhancing Financial Compliance: A Focus on Anti-Money Laundering Applications,2024-12-19,2024,conference proceedings article,"2024 4th International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",,IEEE,,Yuqi Yan; Tiechuan Hu; Wenbo Zhu,,,,260,273,Money laundering; Compliance (psychology); Focus (optics); Business; Computer science; Finance; Psychology; Social psychology; Physics; Optics,,,,,,http://dx.doi.org/10.1109/raai64504.2024.10949516,,10.1109/raai64504.2024.10949516,,,0,009-967-336-501-106; 024-751-614-569-276; 048-221-351-264-683; 050-241-577-769-318; 070-629-716-643-763; 099-738-153-393-902; 126-009-880-529-674; 127-422-034-758-895; 141-595-423-457-108; 151-694-796-158-270; 162-677-200-359-667; 164-338-314-930-979; 176-449-480-185-385; 177-510-260-015-540; 186-823-133-597-449; 188-430-562-456-666,2,false,,
173-317-857-392-613,A Dual Pipeline AI Framework for Real Time Detection of Phishing and Financial Fraud in Fraud GPT,2024-12-07,2024,journal article,FMDB Transactions on Sustainable Technoprise Letters,29935997,Fernando Martins De Bulhão Publishing Company,,Y. Ajitha; P. Sudha; G. Gowthami; C. Shanthini; Mohammad Ayaz Ahmad,"<jats:p>GenAI can create original writing, music, movies, and images and collaborate with humans and other AI models. It is one of our most revolutionary creations. Firms, academics, developers, and consumers have profited from increased productivity, innovation, and efficiency. Generative AI has challenges like any disruptive technology.  It increases creativity and efficiency but creates ethical, disinformation, data privacy, and prejudice concerns.  Recently developed AI tool fraudGPT became popular in the dark market where hackers may automate fraud emails, SMS, manufacture fake text messages, mimic a well-known organisation, and make hacking easy. This traps many innocent people in hackers' webs. AI should be used to detect fraudGPT-related phishing, smishing, social engineering, and financial crimes, according to this study. Python, transformer-based NLP model, anomaly detection model, binary classification, BERT, PyTorch, transformers, and sci-kit-learn are used.  The suggested method detects phishing emails with 92% accuracy and transaction fraud with 94% accuracy in trials.  To test the model's resilience, precision (91%), recall (90%), and F1-score (90.5%) were used for phishing identification and precision (93%), recall (92%), and F1-score (92.5%) for transaction fraud detection. Real-time monitoring and AI-based detection proactively take on AI-driven cyber risks. This work improves AI-powered cybersecurity and informs financial institutions and policymakers.  AI researchers, cybersecurity experts, and regulatory bodies must collaborate to combat rogue AI technologies like FraudGPT.</jats:p>",2,4,200,211,Phishing; Pipeline (software); Dual (grammatical number); Computer security; Business; Financial fraud; Computer science; Accounting; The Internet; World Wide Web; Operating system; Art; Literature,,,,,,http://dx.doi.org/10.69888/ftstpl.2024.000335,,10.69888/ftstpl.2024.000335,,,0,,0,false,,
173-827-864-896-502,ELfolio: Strategy Evolution via Large Language Models for Portfolio Optimization,2025-12-22,2025,journal article,Intelligent Computing,27715892,American Association for the Advancement of Science (AAAS),,Xirui Zeng; Cen Chen; Yanhao Wang; Yuqi Liang,"<jats:p>With increasing financial market fluctuation, classical portfolio optimization methods, particularly those based on static market assumptions and heavily relying on feature engineering, face considerable challenges. In addition, existing heuristic algorithms for strategy optimization often depend on expert knowledge, showing limited adaptability in real-world financial market scenarios. To address these limitations, this paper proposes a novel framework, ELfolio, which combines evolutionary algorithms (EAs) with large language models (LLMs) to automatically generate and evolve heuristic strategies for portfolio optimization. Using structured prompts, ELfolio can generate diverse, executable, and financially sound strategies. We systematically explore 3 key optimization paradigms, namely, reinforcement learning (RL), evolutionary search (ES), and deep learning (DL), and introduce multiple sets of general and specialized prompting techniques, along with chain-of-thought (CoT) reasoning, to enhance strategy diversity and search efficiency. Empirical results on real-world financial datasets show that ELfolio considerably outperforms several baseline methods in terms of risk-return trade-offs, providing an effective LLM-enhanced solution for intelligent financial decision-making.</jats:p>",4,,,,,,,,"National Natural Science Foundation of China; National Natural Science Foundation of China; Seek Data Group, Emoney Inc.",,http://dx.doi.org/10.34133/icomputing.0176,,10.34133/icomputing.0176,,,0,000-541-797-292-217; 001-416-506-109-341; 002-217-009-312-714; 006-163-546-648-920; 008-634-212-021-555; 018-237-482-574-137; 019-055-830-687-357; 021-650-082-782-436; 028-958-798-317-287; 030-311-314-578-937; 030-655-456-128-631; 032-324-856-416-671; 035-833-358-195-065; 039-449-916-324-407; 041-799-359-340-78X; 048-366-148-454-585; 052-545-229-802-690; 058-578-992-209-777; 065-057-073-432-177; 065-743-960-663-65X; 067-171-711-503-817; 104-316-065-636-213; 104-492-835-943-600; 107-329-461-385-497; 110-126-790-869-976; 119-023-625-227-107; 120-995-198-385-62X; 134-875-353-218-453; 142-637-644-580-520; 165-783-637-414-137; 190-624-303-228-420; 195-808-933-120-009,0,true,cc-by,gold
173-981-738-302-908,Generative AI in Action: LLM Applications for Financial Analysis,2025-01-01,2025,preprint,,,Elsevier BV,,Guilherme Souza,,,,,,Generative grammar; Action (physics); Computer science; Artificial intelligence; Finance; Business; Physics; Quantum mechanics,,,,,,http://dx.doi.org/10.2139/ssrn.5272980,,10.2139/ssrn.5272980,,,0,169-667-816-570-773,0,false,,
174-087-143-193-837,"User-defined trade-offs in LLM benchmarking: balancing accuracy, scale, and sustainability",,2025,journal article,Knowledge-Based Systems,09507051; 18727409,Elsevier BV,Netherlands,Ana Gjorgjevikj; Ana Nikolikj; Barbara Koroušić Seljak; Tome Eftimov,,330,,114405,114405,Benchmarking; Sustainability; Computer science; Scale (ratio); Environmental economics; Business; Economics; Geography; Marketing; Ecology; Cartography; Biology,,,,HORIZON EUROPE Widening participation and spreading excellence; European Commission; Slovenian Research and Innovation Agency; Slovenian Research and Innovation Agency; Slovenian Research and Innovation Agency; European Commission Marie Sklodowska-Curie Actions,,http://dx.doi.org/10.1016/j.knosys.2025.114405,,10.1016/j.knosys.2025.114405,,,0,005-146-067-308-370; 009-661-190-177-945; 014-753-510-124-818; 020-131-367-064-365; 020-144-985-741-821; 023-176-491-383-812; 023-785-774-258-601; 026-793-890-342-949; 028-454-054-636-147; 029-992-231-476-004; 035-876-619-354-109; 038-343-173-615-200; 038-723-720-462-720; 040-919-225-603-959; 053-386-547-665-784; 054-019-986-010-351; 054-191-995-790-112; 056-542-756-996-88X; 062-155-404-655-325; 064-577-333-968-721; 064-756-754-191-387; 065-003-705-220-161; 067-946-624-100-658; 071-477-906-048-849; 080-600-379-598-503; 081-370-959-105-221; 090-520-585-368-138; 094-759-407-249-881; 101-469-295-583-565; 104-715-402-779-011; 111-927-924-258-238; 122-161-931-164-718; 131-194-021-149-567; 132-800-157-067-548; 137-014-686-937-063; 137-912-559-374-362; 144-899-958-865-492; 154-785-163-501-246; 161-613-625-488-625; 176-661-395-138-128; 183-337-179-947-421; 194-094-887-495-433; 194-573-579-155-398,2,true,cc-by,hybrid
174-092-625-351-320,LLMs for Quantitative Investment Research: A Practitioner's Guide,,2025,preprint,,,Elsevier BV,,Anna-Helena Mihov; Nick Firoozye; Philip Treleaven,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5934015,,10.2139/ssrn.5934015,,,0,,0,false,,
174-243-492-401-755,Evaluation of Generative AI Q&A Chatbot Chained to Optical Character Recognition Models for Financial Documents,2024-01-26,2024,conference proceedings article,2024 The 8th International Conference on Machine Learning and Soft Computing,,ACM,,Yu Qiu; Venkata C Duvvuri; Pratibha Yadavalli; Neal Prasad,"Financial statements are cornerstones of several analyses, such as loan applications, as well as for legal firms collecting evidence and analysis. They exert a significant influence on the decisions of these institutions. Streamlining the processing of these statements, regardless of their form—be it digital or hard copies—stands as a pivotal objective for banks and similar firms. This research explores the integration of Optical Character Recognition (OCR) and generative AI for automating the extraction of crucial financial data from bank statement images. Furthermore, we design an architecture to make a generic analysis possible on multiple types of financial documents by utilizing a classification model tailored to categorize bank statement documents. This facilitates seamless data preparation for subsequent analysis or model training. Emphasizing precision and efficiency, we investigate OCR model architectures designed specifically to enhance text extraction accuracy from low-resolution bank statement images. The study evaluates two different OCR model architectures—the accuracy of FSRCNN model being the best—achieving an accuracy above 93% in OCR. Additionally, we analyze a generative AI-based Q&A chatbot to simplify analysis for novice users.",,,101,110,Chatbot; Computer science; Optical character recognition; Financial statement; Statement (logic); Categorization; Generative grammar; Artificial intelligence; Character (mathematics); Natural language processing; Loan; Generative model; Machine learning; Information retrieval; Finance; Accounting; Image (mathematics); Linguistics; Geometry; Mathematics; Philosophy; Audit; Economics; Business,,,,,,http://dx.doi.org/10.1145/3647750.3647766,,10.1145/3647750.3647766,,,0,019-868-459-581-491; 094-206-454-118-462; 160-613-178-963-001,1,false,,
174-531-577-095-642,"Combining Domain and Alignment Vectors to Achieve Better
  Knowledge-Safety Trade-offs in LLMs",2024-11-11,2024,preprint,arXiv (Cornell University),,,,Megh Thakkar; Quentin Fournier; Matthew Riemer; Pin-Yu Chen; Amal Zouaq; Payel Das; Sarath Chandar,"There is a growing interest in training domain-expert LLMs that excel in specific technical fields compared to their general-purpose instruction-tuned counterparts. However, these expert models often experience a loss in their safety abilities in the process, making them capable of generating harmful content. As a solution, we introduce an efficient and effective merging-based alignment method called \textsc{MergeAlign} that interpolates the domain and alignment vectors, creating safer domain-specific models while preserving their utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in medicine and finance, obtaining substantial alignment improvements with minimal to no degradation on domain-specific benchmarks. We study the impact of model merging through model similarity metrics and contributions of individual models being merged. We hope our findings open new research avenues and inspire more efficient development of safe expert LLMs.",,,,,Domain (mathematical analysis); Computer science; Business; Risk analysis (engineering); Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2411.06824,http://dx.doi.org/10.48550/arxiv.2411.06824,,10.48550/arxiv.2411.06824,,,0,,0,true,,green
174-784-886-757-852,Agent-Based Financial Report Indicator Extraction with Large Language Models,,2025,journal article,Artificial Intelligence and Robotics Research,23263415; 23263423,Hans Publishers,,亚豪 张,,14,6,1361,1371,,,,,,,http://dx.doi.org/10.12677/airr.2025.146127,,10.12677/airr.2025.146127,,,0,028-082-464-771-603; 141-407-691-449-883; 146-494-288-994-072; 156-230-313-423-209,0,false,,
175-032-715-360-046,Designing multi-model conversational AI financial systems: understanding sensitive values of women entrepreneurs in Brazil,2024-06-12,2024,conference proceedings article,Proceedings of the 2024 ACM International Conference on Interactive Media Experiences Workshops,,ACM,,Heloisa Candello; Gabriel Soella; Leandro Nascimento,,,,11,18,Computer science; Finance; Business,,,,,https://arxiv.org/pdf/2406.19601 https://arxiv.org/abs/2406.19601,http://dx.doi.org/10.1145/3672406.3672409,,10.1145/3672406.3672409,,,0,000-220-309-440-261; 000-427-065-268-204; 000-452-819-791-881; 000-615-386-500-391; 008-919-183-248-935; 009-019-619-590-786; 015-750-259-627-855; 017-328-773-401-33X; 019-116-733-472-895; 024-920-582-562-188; 026-841-158-209-34X; 026-862-494-111-651; 027-179-705-512-640; 030-759-856-042-948; 032-790-783-962-273; 041-013-691-064-097; 043-152-978-025-418; 051-139-878-109-482; 051-818-963-658-026; 056-414-568-592-355; 060-112-932-904-775; 070-076-856-649-887; 070-144-759-722-976; 072-374-727-222-391; 072-841-596-350-19X; 081-364-078-831-329; 082-801-031-856-579; 084-250-929-452-903; 086-140-920-871-082; 091-604-560-823-100; 110-114-042-134-888; 120-525-417-115-536; 120-612-242-451-161; 142-385-734-803-03X; 143-542-044-032-816; 150-342-281-748-487; 173-168-295-522-220; 177-328-819-167-086; 189-974-462-184-431; 190-167-411-362-108,3,true,,green
175-131-556-556-415,Research on Financial Statement Checking Relationship Recognition System Based on Large Language Models,2025-03-28,2025,conference proceedings article,Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence,,ACM,,Haichao Zhang; Jie Zhang; Jiancheng Zhou,,,,320,326,Statement (logic); Computer science; Financial statement; Programming language; Natural language processing; Artificial intelligence; Accounting; Linguistics; Business; Philosophy; Audit,,,,,,http://dx.doi.org/10.1145/3745238.3745291,,10.1145/3745238.3745291,,,0,126-843-385-491-60X; 159-561-923-358-649; 176-408-616-328-234; 179-055-763-667-39X; 192-949-063-372-324,0,false,,
175-144-122-847-568,Construction of a Japanese Financial Benchmark for Large Language Models,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Masanori Hirano,"With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity.There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain.Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models.Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively.According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties.",,,,,Download; Benchmark (surveying); Computer science; World Wide Web; Geography; Geodesy,,,,,https://arxiv.org/pdf/2403.15062 https://arxiv.org/abs/2403.15062,http://dx.doi.org/10.2139/ssrn.4769124,,10.2139/ssrn.4769124,,,0,062-518-441-478-014; 082-178-119-516-788; 103-708-848-416-467,5,true,,green
175-161-461-889-775,Integrating Large Language Models and CNN-LSTM for Enhanced Portfolio Optimization,2025-10-21,2025,journal article,Operations Research Forum,26622556,Springer Science and Business Media LLC,,Arega Denekew; Tesfahun Berehane; Molalign Adam,,6,4,,,Computer science; Portfolio; Artificial intelligence; Natural language processing; Financial economics; Economics,,,,,,http://dx.doi.org/10.1007/s43069-025-00564-4,,10.1007/s43069-025-00564-4,,,0,006-399-176-500-152; 010-747-147-471-762; 022-486-017-682-206; 023-124-046-997-109; 023-794-697-766-278; 039-710-477-365-480; 041-799-359-340-78X; 042-410-554-107-703; 050-596-903-746-668; 052-545-229-802-690; 053-094-537-530-97X; 057-040-097-643-496; 065-616-589-815-994; 066-412-347-102-758; 071-860-959-402-137; 076-782-472-400-835; 078-981-577-025-706; 095-576-644-614-98X; 101-747-021-605-920; 105-749-255-720-379; 112-705-069-209-103; 119-770-239-869-389; 134-322-114-869-769; 147-968-872-055-726; 153-937-769-535-33X,0,false,,
175-423-932-362-847,Revolutionizing Financial Industry with Large Language Models,2025-11-18,2025,book chapter,Lecture Notes in Networks and Systems,23673370; 23673389,Springer Nature Switzerland,,Yifan Wang; Michel Kadoch; Ruicong Zhang; Jingyang Ren,,,,143,153,,,,,,,http://dx.doi.org/10.1007/978-3-032-09694-4_12,,10.1007/978-3-032-09694-4_12,,,0,012-742-473-302-403; 013-499-722-026-497; 033-737-189-013-386; 055-097-049-452-286; 079-644-682-617-374; 124-004-117-116-918; 128-577-363-503-052; 153-895-658-444-133,0,false,,
175-900-307-612-141,A Deep Reinforcement Learning Approach Combining Technical and Fundamental Analyses with a Large Language Model for Stock Trading,2024-11-19,2024,conference proceedings article,2024 14th International Conference on Computer and Knowledge Engineering (ICCKE),,IEEE,,Mahan Veisi; Sadra Berangi; Mahdi Shahbazi Khojasteh; Armin Salimi-Badr,,,,224,229,Reinforcement learning; Computer science; Artificial intelligence; Stock trading; Stock (firearms); Machine learning; Stock market; Engineering; Mechanical engineering; Paleontology; Horse; Biology,,,,,,http://dx.doi.org/10.1109/iccke65377.2024.10874515,,10.1109/iccke65377.2024.10874515,,,0,023-091-941-997-545; 034-925-032-204-170; 053-130-210-066-031; 053-934-549-208-207; 057-187-412-224-184; 057-711-657-946-116; 082-103-972-111-618; 089-364-345-994-749; 104-943-960-749-41X; 137-898-855-175-685; 143-455-353-681-603,0,false,,
176-039-282-440-697,B2B Financial Sector Behavior Concerning Cognitive Chatbots. Personalized Contextual Chatbots in Financial Sector,2023-09-22,2023,conference proceedings article,2023 14th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),,IEEE,,Sayyed Khawar Abbas; Andrea Kő; Zoltán Szabó,"There are numerous customer service articles on chatbots, but only a few studies deal with their acceptability and the cognitive components of personalization and context have been neglected so far. Due to the increasing usage of chatbots across all industries, our study reveals that B2B firms are beginning to implement intelligent chatbots to enhance customer service. 483 responses were collected from employees of B2B banking organizations. This study investigates the acceptance of cognitive technology in unsupervised settings, revealing that perceived information quality significantly impacts customer experience, while perceived system and service quality do not exhibit notable influence. Regarding cognitive chatbots' contextual knowledge, the findings highlight that factors like ease of use, usefulness, and trust do not significantly impact user attitudes. Furthermore, the research demonstrates that risk perception interacts with system characteristics, notably influencing customer experience and intention to adopt, emphasizing the importance of addressing risk concerns for successful cognitive chatbot integration. This paper provides major theoretical advances to chatbot literature by combining two models into one. Intelligent chatbots boost automation because consumers are more willing to utilize them if they build confidence in them.",10,,85,90,Chatbot; Personalization; Context (archaeology); Cognition; Financial services; Knowledge management; Quality (philosophy); Service (business); Business; Computer science; Marketing; Psychology; Finance; World Wide Web; Paleontology; Philosophy; Epistemology; Neuroscience; Biology,,,,,,http://dx.doi.org/10.1109/coginfocom59411.2023.10397514,,10.1109/coginfocom59411.2023.10397514,,,0,,2,false,,
176-090-516-354-817,Synthetic Voices: Evaluating the Fidelity of LLM-Generated Personas in Representing People's Financial Wellbeing,2025-06-13,2025,conference proceedings article,"Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization",,ACM,,Arshnoor Kaur; Amanda Aird; Harris Borman; Andrea Nicastro; Anna Leontjeva; Luiz Pizzato; Dan Jermyn,,,,185,193,Persona; Fidelity; Computer science; Psychology; Finance; Human–computer interaction; Business; Telecommunications,,,,,,http://dx.doi.org/10.1145/3699682.3728339,,10.1145/3699682.3728339,,,0,002-312-304-916-460; 022-380-454-123-124; 032-922-615-686-267; 044-574-009-106-67X; 075-091-914-116-008; 076-137-659-999-540; 081-411-168-638-564; 190-057-892-487-181; 197-656-479-690-214,0,false,,
176-334-018-120-549,A Financial Service Chatbot based on Deep Bidirectional Transformers,2020-02-17,2020,preprint,arXiv: Computation and Language,,,,Shi Yu; Yuxin Chen; Hussain Zaidi,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to handle client questions in financial investment customer service. The bot can recognize 381 intents, and decides when to say ""I don't know"" and escalates irrelevant/uncertain questions to human operators. Our main novel contribution is the discussion about uncertainty measure for BERT, where three different approaches are systematically compared on real problems. We investigated two uncertainty metrics, information entropy and variance of dropout sampling in BERT, followed by mixed-integer programming to optimize decision thresholds. Another novel contribution is the usage of BERT as a language model in automatic spelling correction. Inputs with accidental spelling errors can significantly decrease intent classification performance. The proposed approach combines probabilities from masked language model and word edit distances to find the best corrections for misspelled words. The chatbot and the entire conversational AI system are developed using open-source tools, and deployed within our company's intranet. The proposed approach can be useful for industries seeking similar in-house solutions in their specific business domains. We share all our code and a sample chatbot built on a public dataset on Github.",,,,,Machine learning; Language model; Financial services; Artificial intelligence; Spelling; Chatbot; Transformer (machine learning model); Computer science,,,,,https://arxiv.org/pdf/2003.04987.pdf https://arxiv.org/abs/2003.04987 http://export.arxiv.org/pdf/2003.04987 http://ui.adsabs.harvard.edu/abs/2020arXiv200304987Y/abstract,https://arxiv.org/abs/2003.04987,,,3010649878,,0,004-563-954-315-703; 007-814-872-111-101; 015-211-825-353-092; 015-683-510-811-015; 028-733-625-338-066; 028-784-093-249-926; 030-296-841-469-553; 032-221-948-483-976; 033-034-368-260-285; 034-304-301-930-110; 038-134-268-437-952; 038-443-748-925-073; 044-498-949-210-002; 046-083-595-644-433; 046-791-994-958-226; 049-324-875-236-331; 051-073-344-349-685; 056-935-251-080-195; 059-431-949-623-26X; 065-134-013-925-216; 071-685-214-817-308; 074-033-855-257-801; 074-191-444-250-721; 082-178-119-516-788; 083-876-377-813-712; 086-980-365-076-590; 087-363-447-270-007; 087-864-289-366-200; 088-017-358-323-095; 089-199-749-285-053; 089-325-498-291-33X; 089-802-595-581-370; 091-430-772-314-589; 101-221-194-401-567; 103-506-297-548-029; 108-749-522-052-627; 119-525-054-688-38X; 123-979-645-524-601; 130-460-460-908-695; 135-467-519-462-979; 137-172-995-516-493; 138-275-133-139-362; 156-643-811-054-720; 158-902-227-944-755; 161-996-011-736-816; 162-880-630-852-463; 166-753-131-423-481; 168-635-279-909-985; 171-885-864-003-332; 174-677-065-164-255; 175-819-593-427-749; 176-054-463-554-509; 187-234-190-686-086,7,true,,unknown
176-724-730-938-233,StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets,2025-07-12,2025,preprint,arXiv (Cornell University),,,,Charidimos Papadakis; Giorgos Filandrianos; Angeliki Dimitriou; Maria Lymperaiou; Konstantinos Thomas; Giorgos Stamou,"We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.",,,,,Order (exchange); Dual (grammatical number); Dual mode; Financial market; Mode (computer interface); Simulation; Computer science; Business; Finance; Engineering; Human–computer interaction; Art; Literature; Aerospace engineering,,,,,https://arxiv.org/abs/2507.09255,http://dx.doi.org/10.48550/arxiv.2507.09255,,10.48550/arxiv.2507.09255,,,0,,0,true,,green
176-789-073-876-083,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Investment Law",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Economics; Law; Investment (macroeconomics); Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7902565,https://guides.library.harvard.edu/c.php?g=1082940&p=7902565,,,3088935361,,0,,0,false,,
176-979-855-746-002,The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry,2024-02-13,2024,preprint,,,Center for Open Science,,Minkyu Shin; Jin Kim; Jiwoong Shin,"<p>Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. We analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We find that LLM usage is associated with an increased likelihood of obtaining relief from financial firms. To investigate this relationship, we employ an instrumental variable approach to mitigate endogeneity concerns around LLM adoption. Although instrumental variables suggest a potential causal link, they cannot fully capture all unobserved heterogeneity. To further establish this causal relationship, we conducted controlled experiments, which support that LLMs can enhance the clarity and persuasiveness of consumer narratives, thereby increasing the likelihood of obtaining relief. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.</p>",,,,,Business,,,,,,http://dx.doi.org/10.31234/osf.io/fdzqg_v1,,10.31234/osf.io/fdzqg_v1,,,0,,0,true,,unknown
177-238-083-614-090,Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Sumuk Shashidhar; Abhinav Chinta; Vaibhav Sahai; Zhenhailong Wang; Heng Ji,"The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by case studies.",,,,,Computer science; Leverage (statistics); Ranking (information retrieval); Metric (unit); Performance metric; Benchmark (surveying); Inference; Machine learning; Artificial intelligence; Business; Operations management; Economics; Geodesy; Marketing; Geography,,,,,https://arxiv.org/abs/2310.07611,http://dx.doi.org/10.48550/arxiv.2310.07611,,10.48550/arxiv.2310.07611,,,0,,0,true,cc-by,hybrid
177-296-169-623-620,A Comprehensive Review of Vertical Applications in the Financial Sector Based on Large Language Models,,2024,conference proceedings article,"Proceedings of the 3rd International Conference on Big Data Economy and Digital Management, BDEDM 2024, January 12–14, 2024, Ningbo, China",,EAI,,Yanlin Mao; Bo Chen; Weiqin Chen; Yuandan Deng; Juntao Zeng; Mengzhen Du,,,,,,Financial sector; Computer science; Business; Finance,,,,,,http://dx.doi.org/10.4108/eai.12-1-2024.2347198,,10.4108/eai.12-1-2024.2347198,,,0,,1,false,,
177-351-503-852-138,ADAM: An intelligent virtual assistant for personal financial management,2022-05-16,2022,conference proceedings article,XVIII Brazilian Symposium on Information Systems,,ACM,,Lucas Schneider Müller; Luís Guilherme Eich; Rosemary Francisco; Jorge Luis Victória Barbosa,"Context: According to the Serviço de Proteção ao Crédito (SPC Brasil), approximately 48% of Brazilians do not control their monthly budget.",,,1,8,Context (archaeology); Computer science; Control (management); Financial management; Business; Finance; Artificial intelligence; Paleontology; Biology,,,,,,http://dx.doi.org/10.1145/3535511.3535560,,10.1145/3535511.3535560,,,0,021-158-651-397-876; 021-212-751-046-613; 047-548-977-784-785; 118-784-853-475-823; 119-146-042-099-078; 122-899-805-315-918; 146-180-997-683-881; 177-781-271-272-045,1,false,,
177-410-632-382-942,FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design,2025-01-01,2025,journal article,IEEE Transactions on Big Data,23327790; 23722096,,,Yangyang Yu; Haohang Li; Zhi Chen; Yuechen Jiang; Yang Li; Jordan W. Suchow; Denghui Zhang; Khaldoun Khashanah,,,,1,18,Computer science; Character (mathematics); Distributed computing; Computer security; Parallel computing; Mathematics; Geometry,,,,,,,,,,,0,,0,false,,
177-440-569-219-209,HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization,2025-07-24,2025,preprint,arXiv (Cornell University),,,,Benjamin Coriat; Eric Benhamou,"This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.",,,,,Reinforcement learning; Portfolio; Portfolio optimization; Reinforcement; Computer science; Finance; Artificial intelligence; Business; Engineering; Structural engineering,,,,,https://arxiv.org/abs/2507.18560,http://dx.doi.org/10.48550/arxiv.2507.18560,,10.48550/arxiv.2507.18560,,,0,,0,true,,green
177-471-763-151-211,Dissecting Disposition Effect of Large Language Models in Financial Decisions,,2024,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Liangdi Tan; Chunxiao Li,,,,,,Disposition; Disposition effect; Business; Finance; Actuarial science; Economics; Psychology; Social psychology; Paleontology; Context (archaeology); Biology,,,,,,http://dx.doi.org/10.2139/ssrn.4818096,,10.2139/ssrn.4818096,,,0,009-133-000-340-061; 014-954-601-101-444; 022-916-079-093-025; 025-139-489-059-239; 027-238-593-026-492; 036-655-999-623-223; 038-298-591-759-899; 047-534-014-813-03X; 047-822-888-245-340; 049-176-263-880-570; 065-757-520-647-813; 077-226-922-083-339; 089-056-414-210-983,0,false,,
177-575-126-909-348,FinBen: A Holistic Financial Benchmark for Large Language Models,,2024,conference proceedings article,Advances in Neural Information Processing Systems 37,,"Neural Information Processing Systems Foundation, Inc. (NeurIPS)",,Sophia Ananiadou; Zhengyu Chen; Yongfu Dai; Zhiyang Deng; Duanyu Feng; Weiguang Han; Yueru He; Gang Hu; Jiajia Huang; Jimin Huang; Yuechen Jiang; Haoqiang Kang; Ziyan Kuang; Yanzhao Lai; Dong Li; Haohang Li; Xiao-Yang Liu; Zhiwei Liu; Alejandro Lopez-Lira; Zheheng Luo; Min Peng; Benyou Wang; Hao Wang; Ruoyu Xiang; Mengxi Xiao; Qianqian Xie; Guojun Xiong; Yijing Xu; Kailai Yang; Zhiyuan Yao; Yangyang Yu; Chenhan Yuan; Tianlin Zhang; Xiao Zhang,,,,95716,95743,,,,,,,http://dx.doi.org/10.52202/079017-3033,,10.52202/079017-3033,,,0,,5,false,,
177-592-725-582-847,Implementing domain-specific LLMs for strategic investment decisions: a retrospective case study comparing AI and human expertise,2025-12-11,2025,journal article,Digital Finance,25246984; 25246186,Springer Science and Business Media LLC,,Maher Hamid,,8,1,,,,,,,,,http://dx.doi.org/10.1007/s42521-025-00163-2,,10.1007/s42521-025-00163-2,,,0,001-187-887-233-680; 006-399-176-500-152; 007-170-216-169-697; 008-888-108-947-948; 010-423-042-177-734; 014-777-923-392-649; 017-093-171-005-141; 018-866-913-622-957; 022-063-693-041-178; 022-486-017-682-206; 026-451-375-335-947; 031-457-499-585-305; 034-111-475-208-944; 035-078-993-466-884; 039-095-764-217-091; 042-410-554-107-703; 045-444-446-427-472; 050-899-255-880-743; 051-057-313-108-15X; 052-998-200-003-993; 057-040-097-643-496; 059-418-168-304-964; 068-489-673-470-079; 070-861-916-071-276; 071-242-924-838-979; 077-444-269-577-533; 082-178-119-516-788; 086-260-434-815-196; 088-852-902-759-028; 089-592-981-136-82X; 089-736-715-942-079; 090-541-956-888-654; 091-206-469-244-294; 093-035-322-248-695; 100-446-896-445-856; 106-963-046-279-605; 118-084-806-493-044; 126-550-652-294-389; 133-528-521-954-642; 136-134-457-808-751; 137-120-754-193-596; 154-293-426-700-837; 156-441-933-566-286; 170-594-007-938-073; 170-622-080-796-193; 190-624-303-228-420; 192-969-060-216-244,0,false,,
178-073-558-415-68X,Startup Success Prediction Using GRU-SAM: A Big Data-Driven Financial Modeling Approach with LLM-Enhanced Insights,2025-05-16,2025,conference proceedings article,2025 3rd International Conference on Data Science and Information System (ICDSIS),,IEEE,,Harshini Gadam; Rakesh Kumar Pal; Tanvi A Desai,,,,1,5,Computer science; Big data; Data modeling; Data science; Data mining; Software engineering,,,,,,http://dx.doi.org/10.1109/icdsis65355.2025.11070487,,10.1109/icdsis65355.2025.11070487,,,0,002-993-163-526-763; 003-321-410-449-246; 014-049-958-186-33X; 016-281-919-168-920; 026-294-874-697-970; 036-147-460-320-877; 040-763-053-541-924; 046-739-912-951-939; 066-748-385-034-732; 077-207-004-035-684; 080-186-203-119-993; 082-877-369-227-797; 095-861-669-293-310; 106-819-746-575-898; 115-635-714-431-929; 116-788-626-310-477; 128-161-842-212-587; 159-280-168-002-567; 161-863-261-205-71X; 181-409-772-857-977; 183-764-443-755-602,0,false,,
178-139-870-996-668,"Review of: ""Safeguarding Large Language Models in Real-time with Tunable Safety-Performance Trade-offs""",2025-03-06,2025,review,,,Qeios Ltd,,Richard J. Gruss,,,,,,Safeguarding; Computer science; Reliability engineering; Engineering; Medicine; Nursing,,,,,,http://dx.doi.org/10.32388/wh8qc5,,10.32388/wh8qc5,,,0,,0,false,,
178-290-386-300-448,GEMs-LLM: Integrating Large Language Models with Goal-Aware Exploration for RL-Based Portfolio Optimization,2025-07-15,2025,book chapter,Communications in Computer and Information Science,18650929; 18650937,Springer Nature Singapore,Germany,Yining Wang; Zhixiang Lu; Pin Qian; Jionglong Su; Mian Zhou; Chong Li; Zhengyong Jiang,,,,516,527,Computer science; Portfolio; Business; Finance,,,,,,http://dx.doi.org/10.1007/978-981-96-9949-0_43,,10.1007/978-981-96-9949-0_43,,,0,003-973-251-475-431; 016-269-848-088-621; 018-866-913-622-957; 033-540-203-296-417; 079-376-808-859-849; 102-575-899-378-023; 114-770-757-187-095; 128-891-583-667-88X; 142-721-088-979-670; 171-332-121-199-960,0,false,,
178-582-305-035-131,Financial Sentiment Analysis with Large Language Models,2025-12-19,2025,journal article,Finance & Economics,29596130,Dean & Francis Press,,Xinyu Cheng,"<jats:p>Financial sentiment analysis is vital for applications such as market prediction and risk management. While domain-specific models like (Financial Bidirectional Encoder Representations from Transformers) FinBERT are widely used, their limited scalability constrains performance across diverse financial texts. This paper investigates the effectiveness of large language models (LLMs) with parameter-efficient fine-tuning strategies. We fine-tune Llama-3.1-8B and Owen-3-8B using LoRA and QLoRA, and evaluate them on Financial PhraseBank and FiOASA datasets. Experiments show that LLMs consistently outperform FinBERT, achieving up to 88.9% accuracy on PhraseBank and 81.7% accuracy with 0.74 macro-Fl on FiQA-SA LoRA yields stronger performance, especially on minority classes, while QLoRA maintains comparable accuracy with significantly reduced memory cost. Moreover, Qwen-3 outperforms Llama-3.1 on noisy microblogs, benefiting from its Mixture-of-Experts (MoE) architecture, which enhances efficiency and diversity through conditional computation. These findings confirm that parameter-efficient fine-tuned LLMs provide both accuracy and efficiency, and represent strong alternatives to domain-specific models in financial sentiment analysis.</jats:p>",1,6,,,,,,,,,http://dx.doi.org/10.61173/gvntfv56,,10.61173/gvntfv56,,,0,,0,false,,
178-644-828-725-837,"TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral
  Trading Interactions",2024-10-10,2024,preprint,arXiv (Cornell University),,,,Alicia Vidler; Toby Walsh,"We introduce a novel hybrid approach that augments Agent-Based Models (ABMs) with behaviors generated by Large Language Models (LLMs) to simulate human trading interactions. We call our model TraderTalk. Leveraging LLMs trained on extensive human-authored text, we capture detailed and nuanced representations of bilateral conversations in financial trading. Applying this Generative Agent-Based Model (GABM) to government bond markets, we replicate trading decisions between two stylised virtual humans. Our method addresses both structural challenges, such as coordinating turn-taking between realistic LLM-based agents, and design challenges, including the interpretation of LLM outputs by the agent model. By exploring prompt design opportunistically rather than systematically, we enhance the realism of agent interactions without exhaustive overfitting or model reliance. Our approach successfully replicates trade-to-order volume ratios observed in related asset markets, demonstrating the potential of LLM-augmented ABMs in financial simulations",,,,,Psychology; Economics; Computer science; Cognitive psychology,,,,,https://arxiv.org/abs/2410.21280,http://dx.doi.org/10.48550/arxiv.2410.21280,,10.48550/arxiv.2410.21280,,,0,,0,true,,green
179-017-035-158-602,Enhancing Financial Risk Analysis using RAG-based Large Language Models,2024-12-04,2024,conference proceedings article,"2024 3rd International Conference on Automation, Computing and Renewable Systems (ICACRS)",,IEEE,,Abhishek Darji; Fenil Kheni; Dhruvil Chodvadia; Parth Goel; Dweepna Garg; Bankim Patel,,,,754,760,Computer science; Finance; Business,,,,,,http://dx.doi.org/10.1109/icacrs62842.2024.10841711,,10.1109/icacrs62842.2024.10841711,,,0,008-075-778-671-41X; 018-866-913-622-957; 026-294-977-463-130; 043-614-893-865-040; 063-810-161-231-61X; 140-402-686-444-798; 192-806-908-794-847,3,false,,
179-062-370-073-561,Finlingo: A Conversational AI for Enhancing Financial Literacy Education in Africa,2024-11-04,2024,conference proceedings article,"2024 IEEE International Conference on Technology Management, Operations and Decisions (ICTMOD)",,IEEE,,Japheth Kiplang'at Mursi; Hamid Nach; Austin Odera; Betty Mwende; Daniel Dhol; Faith Mwikali,,,,1,7,Financial literacy; Computer science; Literacy; Finance; Business; Psychology; Pedagogy,,,,,,http://dx.doi.org/10.1109/ictmod63116.2024.10878167,,10.1109/ictmod63116.2024.10878167,,,0,001-833-083-105-126; 008-987-258-845-73X; 012-542-153-235-452; 018-366-817-074-363; 033-419-680-124-445; 034-169-479-712-752; 038-079-662-181-458; 038-663-662-470-623; 045-194-097-498-379; 046-540-669-615-979; 067-924-433-689-140; 070-103-686-134-952; 080-566-184-485-895; 085-087-207-759-582; 091-182-650-913-540; 095-980-479-104-322; 099-977-184-593-075; 105-149-419-112-427; 106-636-741-695-493; 119-146-042-099-078; 120-188-103-770-584; 121-215-593-767-368; 128-935-031-407-651; 138-494-746-730-171; 147-466-260-158-476; 154-937-821-558-691; 165-942-111-601-398; 171-680-470-383-230; 174-950-580-771-601,0,false,,
180-007-110-109-664,TAT-LLM: A Specialized Language Model for Discrete Reasoning over Financial Tabular and Textual Data,2024-11-14,2024,conference proceedings article,Proceedings of the 5th ACM International Conference on AI in Finance,,ACM,,Fengbin Zhu; Ziyang Liu; Fuli Feng; Chao Wang; Moxin Li; Tat Seng Chua,"In this work, we develop a specialized language model with strong discrete reasoning capabilities to tackle question answering (QA) over hybrid tabular and textual data in finance. Compared with adopting online LLMs, specializing smaller LLMs is more advantageous in response to users' concerns about cost, network latency, and data security risks. To this end, we first abstract a Step-wise Pipeline for tabular and textual QA to help LLMs better execute multi-step inference, which includes three key steps, i.e. Extractor, Reasoner and Executor. This pipeline is proved to bring great performance grains compared with applying other prompting strategies like Chain-of-Thought (CoT), and meanwhile provides better interpretability to the derivation of the answer, with fixed inference steps and intermediate outcomes as references. We then develop a TAT-LLM model by fine-tuning LLaMA 2 with the training data generated automatically from existing datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all compared models, including prior best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks. It is hoped that this work will shed light on practical solutions to the intelligent understanding of financial documents in the future. The generated datasets and trained models will be made publicly available to facilitate future research on the development of financial LLMs.",,,310,318,Computer science; Language model; Natural language processing; Data modeling; Artificial intelligence; Software engineering,,,,,https://dl.acm.org/doi/pdf/10.1145/3677052.3698685 https://doi.org/10.1145/3677052.3698685,http://dx.doi.org/10.1145/3677052.3698685,,10.1145/3677052.3698685,,,0,010-918-453-969-378; 044-456-506-571-452; 056-542-756-996-88X; 076-877-101-190-287; 099-098-288-754-311; 102-183-346-461-22X; 110-127-601-868-566; 121-915-742-459-383; 139-239-027-110-484; 158-534-212-420-864; 174-945-090-891-911; 197-673-280-254-524,5,true,,bronze
180-355-667-802-360,"Temporal Relational Reasoning of Large Language Models for Detecting
  Stock Portfolio Crashes",2024-10-07,2024,preprint,arXiv (Cornell University),,,,Kelvin J. L. Koa; Yunshan Ma; Yi Xu; Ritchie Ng; Huanhuan Zheng; Tat-Seng Chua,"Stock portfolios are often exposed to rare consequential events (e.g., 2007 global financial crisis, 2020 COVID-19 stock market crash), as they do not have enough historical information to learn from. Large Language Models (LLMs) now present a possible tool to tackle this problem, as they can generalize across their large corpus of training data and perform zero-shot reasoning on new events, allowing them to detect possible portfolio crash events without requiring specific training data. However, detecting portfolio crashes is a complex problem that requires more than basic reasoning abilities. Investors need to dynamically process the impact of each new information found in the news articles, analyze the the relational network of impacts across news events and portfolio stocks, as well as understand the temporal context between impacts across time-steps, in order to obtain the overall aggregated effect on the target portfolio. In this work, we propose an algorithmic framework named Temporal Relational Reasoning (TRR). It seeks to emulate the spectrum of human cognitive capabilities used for complex problem-solving, which include brainstorming, memory, attention and reasoning. Through extensive experiments, we show that TRR is able to outperform state-of-the-art solutions on detecting stock portfolio crashes, and demonstrate how each of the proposed components help to contribute to its performance through an ablation study. Additionally, we further explore the possible applications of TRR by extending it to other related complex problems, such as the detection of possible global crisis events in Macroeconomics.",,,,,Portfolio; Computer science; Stock (firearms); Artificial intelligence; Natural language processing; Financial economics; Economics; Engineering; Mechanical engineering,,,,,https://arxiv.org/abs/2410.17266,http://dx.doi.org/10.48550/arxiv.2410.17266,,10.48550/arxiv.2410.17266,,,0,,0,true,,green
180-545-268-920-607,GPT-3 Models are Few-Shot Financial Reasoners,2023-07-22,2023,conference proceedings article,Software Engineering Advances,,Academy & Industry Research Collaboration,,Raul Salles de Padua; Imran Qureshi; Mustafa U. Karakaplan,"<jats:p>Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 [1] have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents. With this understanding, our refined promptengineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.</jats:p>",,,183,197,Computer science; Task (project management); Variety (cybernetics); Question answering; Language model; Open domain; Finance; Domain (mathematical analysis); Financial modeling; Artificial intelligence; Machine learning; Economics; Mathematical analysis; Mathematics; Management,,,,,http://arxiv.org/pdf/2307.13617 http://arxiv.org/abs/2307.13617,http://dx.doi.org/10.5121/csit.2023.131216,,10.5121/csit.2023.131216,,,0,,2,true,,bronze
180-833-450-136-647,"INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with
  LLM-based Agent",2024-12-24,2024,preprint,arXiv (Cornell University),,,,Haohang Li; Yupeng Cao; Yangyang Yu; Shashidhar Reddy Javaji; Zhiyang Deng; Yueru He; Yuechen Jiang; Zining Zhu; Koduvayur Subbalakshmi; Guojun Xiong; Jimin Huang; Lingfei Qian; Xueqing Peng; Qianqian Xie; Jordan W. Suchow,"Recent advancements have underscored the potential of large language model (LLM)-based agents in financial decision-making. Despite this progress, the field currently encounters two main challenges: (1) the lack of a comprehensive LLM agent framework adaptable to a variety of financial tasks, and (2) the absence of standardized benchmarks and consistent datasets for assessing agent performance. To tackle these issues, we introduce \textsc{InvestorBench}, the first benchmark specifically designed for evaluating LLM-based agents in diverse financial decision-making contexts. InvestorBench enhances the versatility of LLM-enabled agents by providing a comprehensive suite of tasks applicable to different financial products, including single equities like stocks, cryptocurrencies and exchange-traded funds (ETFs). Additionally, we assess the reasoning and decision-making capabilities of our agent framework using thirteen different LLMs as backbone models, across various market environments and tasks. Furthermore, we have curated a diverse collection of open-source, multi-modal datasets and developed a comprehensive suite of environments for financial decision-making. This establishes a highly accessible platform for evaluating financial agents' performance across various scenarios.",,,,,Benchmark (surveying); Computer science; Business; Finance; Geography; Cartography,,,,,https://arxiv.org/abs/2412.18174,http://dx.doi.org/10.48550/arxiv.2412.18174,,10.48550/arxiv.2412.18174,,,0,,0,true,,green
180-875-560-081-048,"GPT Conjecture: Understanding the Trade-offs between Granularity, Performance and Timeliness in Control-Flow Integrity.",2019-11-18,2019,preprint,arXiv: Cryptography and Security,,,,Zhilong Wang; Peng Liu,"Performance/security trade-off is widely noticed in CFI research, however, we observe that not every CFI scheme is subject to the trade-off. Motivated by the key observation, we ask three questions. Although the three questions probably cannot be directly answered, they are inspiring. We find that a deeper understanding of the nature of the trade-off will help answer the three questions. Accordingly, we proposed the GPT conjecture to pinpoint the trade-off in designing CFI schemes, which says that at most two out of three properties (fine granularity, acceptable performance, and preventive protection) could be achieved.",,,,,Granularity; Key (cryptography); Scheme (programming language); Subject (documents); Control-flow integrity; Conjecture; Trade offs; Computer security; Computer science,,,,,https://dblp.uni-trier.de/db/journals/corr/corr1911.html#abs-1911-07828 https://arxiv.org/abs/1911.07828 https://arxiv.org/pdf/1911.07828v3,https://dblp.uni-trier.de/db/journals/corr/corr1911.html#abs-1911-07828,,,2988339859,,0,007-514-931-491-85X; 007-742-877-016-058; 010-748-261-933-830; 011-997-543-652-467; 012-904-563-160-954; 026-286-540-185-473; 027-722-844-855-581; 028-479-924-724-521; 033-227-885-328-533; 033-835-220-422-049; 034-916-320-484-379; 038-975-805-630-480; 040-702-625-233-021; 045-580-952-295-667; 047-697-110-305-752; 047-791-031-284-114; 048-063-903-859-87X; 060-040-887-973-412; 060-707-544-279-528; 062-884-139-071-053; 063-790-834-939-319; 065-991-008-490-699; 077-273-733-985-052; 082-527-236-362-936; 085-898-888-580-412; 089-623-507-149-374; 089-942-191-098-23X; 095-401-923-194-370; 101-112-782-985-201; 103-317-096-954-79X; 105-619-153-441-904; 113-482-709-668-924; 114-872-390-648-621; 116-136-129-754-823; 116-638-632-122-911; 127-293-438-443-410; 133-657-885-512-205; 134-429-333-061-559; 145-168-222-392-677; 149-795-268-415-967; 168-253-216-307-860; 170-099-353-437-126; 172-582-172-572-524; 178-443-009-507-544; 193-590-525-980-606,2,true,,unknown
180-947-316-786-249,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Private International Law",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905711,https://guides.library.harvard.edu/c.php?g=1082940&p=7905711,,,3088972571,,0,,0,false,,
181-432-462-344-434,Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks,,2023,conference proceedings article,Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track,,Association for Computational Linguistics,,Xianzhi Li; Samuel Chan; Xiaodan Zhu; Yulong Pei; Zhiqiang Ma; Xiaomo Liu; Sameena Shah,"The most recent large language models (LLMs) such as ChatGPT and GPT-4 have shown exceptional capabilities of generalist models, achieving state-of-the-art performance on a wide range of NLP tasks with little or no adaptation.How effective are such models in the financial domain?Understanding this basic question would have a significant impact on many downstream financial analytical tasks.In this paper, we conduct an empirical study and provide experimental evidences of their performance on a wide variety of financial text analytical problems, using eight benchmark datasets from five categories of tasks.We report both the strengths and limitations of the current models by comparing them to the state-of-the-art fine-tuned approaches and the recently released domainspecific pretrained models.We hope our study can help understand the capability of the existing models in the financial domain and facilitate further improvements.",,,,,Computer science; Analytics; Data science; Finance; Business,,,,,https://arxiv.org/pdf/2305.05862 https://arxiv.org/abs/2305.05862,http://dx.doi.org/10.18653/v1/2023.emnlp-industry.39,,10.18653/v1/2023.emnlp-industry.39,,,0,,51,true,,green
181-502-778-787-822,"Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating
  Financial Large Language Models",2024-11-09,2024,preprint,arXiv (Cornell University),,,,Xiaojun Wu; Junxi Liu; Huanyi Su; Zhouchi Lin; Yiyan Qi; Chengjin Xu; Jiajun Su; Jiajie Zhong; Fuwei Wang; Saizhuo Wang; Fengrui Hua; Jia Li; Jian Guo,"As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose ""Golden Touchstone"", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at \url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.",,,,,Benchmark (surveying); Computer science; Economics; Artificial intelligence; Geography; Cartography,,,,,https://arxiv.org/abs/2411.06272,http://dx.doi.org/10.48550/arxiv.2411.06272,,10.48550/arxiv.2411.06272,,,0,,0,true,,green
181-941-834-155-235,DeepFinLLM 2.0: an optimized and scalable multilingual financial advisor unleashing strategic insights through multi-API orchestration and large language models,2025-09-29,2025,journal article,Journal of Intelligent Information Systems,09259902; 15737675,Springer Science and Business Media LLC,Netherlands,Veerababu Reddy; N. Veeranjaneyulu,,,,,,Orchestration; Computer science; Scalability; Data science; Software engineering; Process management; Knowledge management; Database; Business; Art; Musical; Visual arts,,,,,,http://dx.doi.org/10.1007/s10844-025-00988-8,,10.1007/s10844-025-00988-8,,,0,005-350-127-982-518; 005-407-048-718-561; 016-938-308-196-336; 022-096-598-555-980; 022-600-707-827-261; 032-471-769-867-765; 044-579-322-796-833; 073-031-995-357-278; 079-784-925-855-452; 092-234-413-806-731; 116-022-714-546-130; 116-262-553-652-68X; 128-639-517-251-187; 138-729-962-867-512; 139-239-027-110-484; 140-921-627-035-630; 145-282-452-601-71X; 150-729-485-553-434; 160-657-979-907-602; 162-480-980-657-50X; 166-100-162-814-874; 169-664-751-543-841; 173-031-218-503-969; 173-521-634-095-118; 188-323-505-964-185; 188-615-473-545-318; 193-222-672-874-089; 195-706-909-301-830; 199-281-001-425-053; 199-392-235-944-050,1,false,,
182-649-912-767-682,Reducing emotional bias in investment decisions: the role of GPT-4 in financial analysis,2025-09-08,2025,journal article,Asia-Pacific Journal of Business Administration,17574323; 17574331,Emerald,United Kingdom,Wen-Chin Hsu; Ming-Chun Wang; Hsiu-I Ting,"<jats:sec>;                   <jats:title>Purpose</jats:title>;                   <jats:p>This study examines the investment recommendations made by AI models (GPT-4) in two versions (V1 and V2) compared to human analysts. The research investigates whether AI provides a more objective evaluation with reduced emotional and cognitive biases, addressing concerns about human analysts’ susceptibility to market sentiment.</jats:p>;                </jats:sec>;                <jats:sec>;                   <jats:title>Design/methodology/approach</jats:title>;                   <jats:p>Using Probit regression analysis, this study compares the investment recommendations of V1, which relies on financial statements, and V2, which integrates both financial statements and news sentiment. Additionally, the study assesses the performance of stocks following AI and human recommendations using buy-and-hold and abnormal return measures.</jats:p>;                </jats:sec>;                <jats:sec>;                   <jats:title>Findings</jats:title>;                   <jats:p>Findings reveal that both AI models issue fewer “Strong Buy” recommendations and more “Sell” recommendations than human analysts, particularly for underperforming stocks. V2 outperforms human analysts in predicting returns for “Strong Buy” recommendations, suggesting that AI’s integration of financial and news sentiment data improves predictive accuracy. AI models offer more balanced and consistent investment recommendations, reducing emotional biases.</jats:p>;                </jats:sec>;                <jats:sec>;                   <jats:title>Research limitations/implications</jats:title>;                   <jats:p>A limitation of this study is its reliance on historical data, which may not fully reflect real-time market conditions. Additionally, AI’s conservative nature may limit its ability to identify sentiment-driven opportunities. Future research should explore hybrid frameworks that integrate AI with human expertise to enhance decision-making.</jats:p>;                </jats:sec>;                <jats:sec>;                   <jats:title>Originality/value</jats:title>;                   <jats:p>This study contributes to the literature on AI in financial decision-making by demonstrating its ability to mitigate cognitive biases. The findings highlight AI’s potential to complement human analysts, improving objectivity and consistency in investment recommendations. These insights are valuable for investors, financial institutions and policymakers.</jats:p>;                </jats:sec>",,,1,26,Investment (military); Investment decisions; Finance; Business; Economics; Monetary economics; Behavioral economics; Political science; Politics; Law,,,,,,http://dx.doi.org/10.1108/apjba-03-2025-0181,,10.1108/apjba-03-2025-0181,,,0,000-810-519-562-283; 006-028-823-293-047; 006-750-012-058-578; 016-728-247-553-014; 026-449-509-255-772; 030-559-382-816-297; 037-154-238-083-286; 042-346-068-962-102; 051-448-485-398-729; 054-190-251-386-892; 054-561-737-956-47X; 062-190-852-227-395; 062-534-902-991-428; 063-959-668-476-419; 064-163-167-559-016; 068-944-254-717-091; 070-739-009-566-233; 082-189-954-679-605; 105-956-835-654-883; 106-379-750-339-277; 106-954-159-714-883; 130-558-058-079-847; 146-301-466-127-630; 156-159-727-174-274; 157-995-960-565-853; 168-384-579-553-219; 173-254-562-711-386; 173-905-185-652-582; 175-502-649-836-520; 179-300-388-862-850; 194-347-983-609-632,0,false,,
182-806-617-469-902,"Narratives from GPT-derived Networks of News, and a link to Financial Markets Dislocations",2023-01-01,2023,preprint,arXiv (Cornell University),,,,Deborah Miori; Constantin Petrov,"Starting from a corpus of economic articles from The Wall Street Journal, we present a novel systematic way to analyse news content that evolves over time. We leverage on state-of-the-art natural language processing techniques (i.e. GPT3.5) to extract the most important entities of each article available, and aggregate co-occurrence of entities in a related graph at the weekly level. Network analysis techniques and fuzzy community detection are tested on the proposed set of graphs, and a framework is introduced that allows systematic but interpretable detection of topics and narratives. In parallel, we propose to consider the sentiment around main entities of an article as a more accurate proxy for the overall sentiment of such piece of text, and describe a case-study to motivate this choice. Finally, we design features that characterise the type and structure of news within each week, and map them to moments of financial markets dislocations. The latter are identified as dates with unusually high volatility across asset classes, and we find quantitative evidence that they relate to instances of high entropy in the high-dimensional space of interconnected news. This result further motivates the pursued efforts to provide a novel framework for the systematic analysis of narratives within news.",,,,,Narrative; Leverage (statistics); Computer science; Financial market; Volatility (finance); News analytics; Artificial intelligence; Aggregate (composite); Data science; Machine learning; Data mining; Finance; Economics; Linguistics; Philosophy; Materials science; Composite material,,,,,https://arxiv.org/abs/2311.14419,http://dx.doi.org/10.48550/arxiv.2311.14419,,10.48550/arxiv.2311.14419,,,0,,0,true,public-domain,green
183-001-220-937-841,CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model,2025-06-16,2025,preprint,arXiv (Cornell University),,,,Jiangtong Li; Yiyun Zhu; Dawei Cheng; Zhijun Ding; Changjun Jiang,"Multimodal Large Language Models (MLLMs) have rapidly evolved with the growth of Large Language Models (LLMs) and are now applied in various fields. In finance, the integration of diverse modalities such as text, charts, and tables is crucial for accurate and efficient decision-making. Therefore, an effective evaluation system that incorporates these data types is essential for advancing financial application. In this paper, we introduce CFBenchmark-MM, a Chinese multimodal financial benchmark with over 9,000 image-question pairs featuring tables, histogram charts, line charts, pie charts, and structural diagrams. Additionally, we develop a staged evaluation system to assess MLLMs in handling multimodal information by providing different visual content step by step. Despite MLLMs having inherent financial knowledge, experimental results still show limited efficiency and robustness in handling multimodal financial context. Further analysis on incorrect responses reveals the misinterpretation of visual content and the misunderstanding of financial concepts are the primary issues. Our research validates the significant, yet underexploited, potential of MLLMs in financial analysis, highlighting the need for further development and domain-specific optimization to encourage the enhanced use in financial domain.",,,,,Benchmark (surveying); Computer science; Language model; Natural language processing; Artificial intelligence; Geography; Cartography,,,,,https://arxiv.org/abs/2506.13055,http://dx.doi.org/10.48550/arxiv.2506.13055,,10.48550/arxiv.2506.13055,,,0,,0,true,,green
183-759-352-749-200,Enhancing Financial Inclusion through Conversational Technologies: A Study of Chatbot Adoption in Mozambique,2025-10-19,2025,journal article,"International Journal of Science, Architecture, Technology and Environment",30488222,"International Journal of Science, Architecture, Technology and Environment",,Lucio Daniel Mavundla; Francis Pol Costoy,,,,484,492,Chatbot; Financial inclusion; Inclusion (mineral); Business; Computer science; World Wide Web; Finance; Financial services; Psychology; Social psychology,,,,,,http://dx.doi.org/10.63680/ijsate1025053.050,,10.63680/ijsate1025053.050,,,0,,0,false,,
183-827-604-079-920,"Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating
  Privacy Trade-offs in LLM-Based Conversational Agent",2024-10-09,2024,preprint,arXiv (Cornell University),,,,Jijie Zhou; Eryue Xu; Yaoyao Wu; Tianshi Li,"The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users' personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=12) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users' subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users' trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.",,,,,Computer science; Minification; Human–computer interaction; World Wide Web,,,,,https://arxiv.org/abs/2410.11876,http://dx.doi.org/10.48550/arxiv.2410.11876,,10.48550/arxiv.2410.11876,,,0,,0,true,,green
184-056-419-906-836,Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements Using Large Language Models,2025-10-12,2025,book chapter,Lecture Notes in Computer Science,03029743; 16113349,Springer Nature Switzerland,Germany,Durgesh Nandini; Rebekka Koch; Mirco Schönfeld,,,,3,10,Computer science; Extraction (chemistry); Artificial intelligence; Natural language processing; Chromatography; Chemistry,,,,,,http://dx.doi.org/10.1007/978-3-031-97207-2_1,,10.1007/978-3-031-97207-2_1,,,0,022-669-053-579-988; 030-089-078-880-151; 047-519-952-343-710; 053-171-799-754-152; 061-543-741-766-622; 063-166-142-888-38X; 070-505-333-544-720; 083-449-940-348-930; 101-781-504-791-165; 109-863-432-791-069; 117-929-228-389-733; 166-810-934-264-013,0,false,,
184-188-544-388-059,Integrating LLM-Based Time Series and Regime Detection with RAG for Adaptive Trading Strategies and Portfolio Management,2025-08-22,2025,book chapter,Blockchain Technologies,26618338; 26618346,Springer Nature Singapore,,Chenkai Li; Chi Ho Roger Chan; Seth H. Huang; Paul Moon Sub Choi,,,,129,146,Series (stratigraphy); Project portfolio management; Portfolio; Computer science; Financial economics; Economics; Management; Geology; Project management; Paleontology,,,,,,http://dx.doi.org/10.1007/978-981-96-5833-6_7,,10.1007/978-981-96-5833-6_7,,,0,018-866-913-622-957; 019-618-174-825-497; 042-267-267-198-535; 072-878-583-167-977; 080-600-379-598-503; 103-208-293-954-294; 132-966-239-856-014,0,false,,
184-320-180-354-119,KodeXv0.1: A Family of State-of-the-Art Financial Large Language Models,2024-09-13,2024,preprint,arXiv (Cornell University),,,,Neel Rajani; Lilli Kiessling; Aleksandr Ogaltsov; Claus Lang,"Although powerful, current cutting-edge LLMs may not fulfil the needs of highly specialised sectors. We introduce KodeXv0.1, a family of large language models that outclass GPT-4 in financial question answering. We utilise the base variants of Llama 3.1 8B and 70B and adapt them to the financial domain through a custom training regime. To this end, we collect and process a large number of publicly available financial documents such as earnings calls and business reports. These are used to generate a high-quality, synthetic dataset consisting of Context-Question-Answer triplets which closely mirror real-world financial tasks. Using the train split of this dataset, we perform RAG-aware 4bit LoRA instruction tuning runs of Llama 3.1 base variants to produce KodeX-8Bv0.1 and KodeX-70Bv0.1. We then complete extensive model evaluations using FinanceBench, FinQABench and the withheld test split of our dataset. Our results show that KodeX-8Bv0.1 is more reliable in financial contexts than cutting-edge instruct models in the same parameter regime, surpassing them by up to 9.24%. In addition, it is even capable of outperforming state-of-the-art proprietary models such as GPT-4 by up to 7.07%. KodeX-70Bv0.1 represents a further improvement upon this, exceeding GPT-4's performance on every tested benchmark.",,,,,State (computer science); Business; Economics; Computer science; Programming language,,,,,https://arxiv.org/abs/2409.13749,http://dx.doi.org/10.48550/arxiv.2409.13749,,10.48550/arxiv.2409.13749,,,0,,0,true,,green
184-706-781-067-475,Q&A In Financial Queries Using Zero-Shot Learning with LLM for Novel Task Understanding,2025-05-03,2025,journal article,Journal of Information Systems Engineering and Management,24684376,Science Research Society,,null D. D Sarpate,"<jats:p>For understanding novel tasks, Zero-Shot Learning (ZSL) in combination with Large Language Models (LLMs) exhibits immense potential. By solely depending on task descriptions or guidelines provided in natural language, LLMs can deduce solutions without requiring explicit training data. For instance, an LLM could be assigned the task of summarizing a newly introduced scientific principle or responding to inquiries on an unfamiliar subject. The model's capability to understand tasks from linguistic indicators and apply pre-acquired knowledge is what makes ZSL particularly effective. Despite these advancements, challenges persist in implementing ZSL with LLMs for new task comprehension. Performance inconsistencies arise when novel tasks significantly differ from training data. Additionally, errors or irrelevant outputs may occur due to misinterpretations. Addressing biases in training data, ensuring output consistency, and enhancing interpretability remain crucial areas for further research.</jats:p>",10,42s,629,637,Zero (linguistics); Shot (pellet); Task (project management); Computer science; Finance; Business; Economics; Materials science; Management; Philosophy; Linguistics; Metallurgy,,,,,,http://dx.doi.org/10.52783/jisem.v10i42s.7922,,10.52783/jisem.v10i42s.7922,,,0,,0,true,,bronze
184-963-759-319-135,LLMs as Research Assistants: The Risk of Topic-Overclassification and Effective Mitigation Strategies in Financial Disclosure Research,2025-01-01,2025,preprint,,,Elsevier BV,,Anne d'Arcy; Christian Haas; Nicolaus Wallner,,,,,,Business; Risk management; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.5611629,,10.2139/ssrn.5611629,,,0,000-064-613-799-138; 002-245-426-260-458; 003-219-471-309-721; 003-408-495-316-187; 005-711-813-273-296; 006-399-176-500-152; 012-863-923-490-239; 016-467-739-536-920; 021-412-594-162-528; 026-983-179-499-108; 026-984-186-973-562; 030-071-847-836-783; 042-269-147-288-746; 043-390-882-813-43X; 045-201-107-411-584; 046-251-701-189-260; 046-493-993-308-509; 052-934-028-404-791; 058-243-157-441-711; 074-742-877-053-350; 078-402-340-160-564; 080-098-228-926-572; 083-449-940-348-930; 084-969-191-085-090; 087-091-692-502-629; 089-529-403-469-532; 090-135-685-849-51X; 100-517-337-333-319; 110-653-886-784-058; 111-738-431-531-117; 113-184-876-303-935; 121-570-640-268-403; 125-905-465-169-99X; 139-708-775-748-220; 139-890-394-178-547; 152-125-719-970-085; 154-042-138-562-576; 154-293-426-700-837; 155-938-020-901-649; 163-665-418-997-811; 163-702-158-124-810; 166-571-115-896-53X; 168-271-832-341-80X; 180-301-585-928-732; 184-577-652-692-946; 192-743-385-236-655,0,false,,
185-526-682-860-062,Profitable Trade-Off Between Memory and Performance In Multi-Domain Chatbot Architectures,2021-11-06,2021,preprint,arXiv: Computation and Language,,,,D. Emre Tasar; Sukru Ozan; M. Fatih Akca; Oguzhan Olmez; Semih Gulum; Secilay Kutay; Ceren Belhan,"Text classification problem is a very broad field of study in the field of natural language processing. In short, the text classification problem is to determine which of the previously determined classes the given text belongs to. Successful studies have been carried out in this field in the past studies. In the study, Bidirectional Encoder Representations for Transformers (BERT), which is a frequently preferred method for solving the classification problem in the field of natural language processing, is used. By solving classification problems through a single model to be used in a chatbot architecture, it is aimed to alleviate the load on the server that will be created by more than one model used for solving more than one classification problem. At this point, with the masking method applied during the estimation of a single BERT model, which was created for classification in more than one subject, the estimation of the model was provided on a problem-based basis. Three separate data sets covering different fields from each other are divided by various methods in order to complicate the problem, and classification problems that are very close to each other in terms of field are also included in this way. The dataset used in this way consists of five classification problems with 154 classes. A BERT model containing all classification problems and other BERT models trained specifically for the problems were compared with each other in terms of performance and the space they occupied on the server.",,,,,Encoder; Machine learning; Basis (linear algebra); Artificial intelligence; Subject (documents); Space (commercial competition); Chatbot; Point (geometry); Transformer (machine learning model); Field (computer science); Computer science,,,,,https://arxiv.org/pdf/2111.03963 http://export.arxiv.org/abs/2111.03963 https://arxiv.org/abs/2111.03963 https://dblp.uni-trier.de/db/journals/corr/corr2111.html#abs-2111-03963 http://arxiv-export-lb.library.cornell.edu/abs/2111.03963v1,http://export.arxiv.org/abs/2111.03963,,,3213762998,,0,007-431-178-135-244; 036-522-685-210-240; 082-178-119-516-788; 090-927-217-917-652; 139-933-194-538-090; 165-337-999-749-68X,0,true,,unknown
185-729-052-221-813,Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models,2025-08-21,2025,preprint,arXiv (Cornell University),,,,Yuanchen Zhou; Shuo Jiang; Jie Zhu; Junhui Li; Lifan Guo; Feng Chen; Chi Zhang,"Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.",,,,,Process (computing); Domain (mathematical analysis); Computer science; Cognitive science; Language model; Psychology; Artificial intelligence; Natural language processing; Programming language; Mathematics; Mathematical analysis,,,,,https://arxiv.org/abs/2508.15202,http://dx.doi.org/10.48550/arxiv.2508.15202,,10.48550/arxiv.2508.15202,,,0,,0,true,,green
186-085-098-285-578,INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent,,2025,conference proceedings article,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,Association for Computational Linguistics,,Haohang Li; Yupeng Cao; Yangyang Yu; Shashidhar Reddy Javaji; Zhiyang Deng; Yueru He; Yuechen Jiang; Zining Zhu; K.p. Subbalakshmi; Jimin Huang; Lingfei Qian; Xueqing Peng; Jordan W. Suchow; Qianqian Xie,,,,2509,2525,Benchmark (surveying); Computer science; Finance; Artificial intelligence; Business; Geodesy; Geography,,,,,,http://dx.doi.org/10.18653/v1/2025.acl-long.126,,10.18653/v1/2025.acl-long.126,,,0,,1,false,,
186-221-799-922-73X,An Evaluation of Reasoning Capabilities of Large Language Models in Financial Sentiment Analysis,2024-06-25,2024,conference proceedings article,2024 IEEE Conference on Artificial Intelligence (CAI),,IEEE,,Kelvin Du; Frank Xing; Rui Mao; Erik Cambria,,,,189,194,Computer science; Sentiment analysis; Natural language processing; Language model; Artificial intelligence,,,,,,http://dx.doi.org/10.1109/cai59869.2024.00042,,10.1109/cai59869.2024.00042,,,0,003-020-751-127-684; 004-098-317-899-397; 006-399-176-500-152; 007-635-920-160-781; 014-571-264-321-121; 023-331-435-226-414; 029-668-873-995-071; 030-902-247-987-89X; 035-553-976-537-265; 037-853-357-392-375; 037-982-650-070-165; 039-255-997-298-072; 039-683-146-605-338; 043-435-669-341-724; 048-664-966-575-763; 050-947-368-519-915; 057-982-454-558-150; 060-387-646-895-313; 062-518-441-478-014; 065-406-065-724-703; 066-277-557-325-395; 081-081-928-966-538; 092-832-898-852-612; 098-235-239-849-201; 108-998-043-381-391; 112-578-336-541-436; 114-298-872-576-313; 118-558-821-254-086; 125-020-211-841-75X; 129-315-519-581-820; 132-094-157-491-968; 134-795-837-065-860; 135-952-044-063-817; 139-119-876-531-586; 142-045-123-082-279; 154-042-138-562-576; 155-498-300-221-705; 181-246-395-796-414; 191-557-257-026-076,17,false,,
186-276-172-528-155,Structured Agentic Workflows for Financial Time-Series Modelling with LLMs and Reflective Feedback,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Yihao Ang; Yifan Bao; Lei Jiang; Jiajie Tao; Anthony K. H. Tung; Lukasz Szpurch; Hao Ni,,,,924,932,,,,,"EPSRC; UCL Centre for Digital Innovation, powered by Amazon Web Services; Ministry of Education, Singapore; UKRI",,http://dx.doi.org/10.1145/3768292.3771251,,10.1145/3768292.3771251,,,0,000-359-175-901-363; 006-701-602-294-537; 018-237-482-574-137; 061-462-343-026-691; 067-632-684-767-700; 098-340-878-853-594; 120-647-690-048-229; 147-548-886-422-178; 169-386-715-874-566,0,false,,
186-407-037-941-139,Exploring Factors Influencing Usage Intention of Chatbot – Chatbot in Financial Service,2019-12-27,2019,journal article,Journal of the Korean Society for Quality Management,12291889,,,Min Kyu Lee; Heejun Park,,47,4,755,765,Financial services; Business; Chatbot; Knowledge management,,,,,https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002533887 http://jksqm.jatsxml.org/journal/view.php?number=2142 https://www.jksqm.org/journal/view.php?number=2142,http://jksqm.jatsxml.org/journal/view.php?number=2142,,,2998555890,,0,010-854-534-444-434; 011-057-105-680-492; 018-077-006-779-166; 028-292-605-075-78X; 032-946-061-922-067; 037-994-584-216-071; 039-847-799-312-100; 042-614-616-511-424; 046-400-576-138-005; 068-722-886-861-661; 070-952-046-590-227; 087-664-051-963-498; 095-564-691-234-578; 115-210-166-828-226; 128-466-645-385-478; 139-283-893-155-150; 139-629-108-899-345; 144-573-806-291-194; 148-127-649-642-16X,3,false,,
186-593-827-434-854,"Large Language Model Agents for Investment Management: Foundations, Benchmarks, and Research Frontiers",,2025,preprint,,,Elsevier BV,,Preetha Saha; Jingrao Lyu; Arnav Saxena; Tianjiao Zhao; Dhagash Mehta,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5447274,,10.2139/ssrn.5447274,,,0,,0,false,,
186-842-930-767-360,Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding,2023-01-01,2023,preprint,arXiv (Cornell University),,,,Seongjun Yang; Gibbeum Lee; Jaewoong Cho; Dimitris Papailiopoulos; Kangwook Lee,"This paper presents ""Predictive Pipelined Decoding (PPD),"" an approach that speeds up greedy decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This innovative method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as p_correct. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM greedy decoding.",,,,,Decoding methods; Security token; Computer science; Latency (audio); Sequential decoding; List decoding; Computation; Low latency (capital markets); Algorithm; Parallel computing; Computer network; Telecommunications; Concatenated error correction code; Block code,,,,,https://arxiv.org/abs/2307.05908,http://dx.doi.org/10.48550/arxiv.2307.05908,,10.48550/arxiv.2307.05908,,,0,,0,true,other-oa,green
187-165-793-273-196,"FinAudio: A Benchmark for Audio Large Language Models in Financial
  Applications",2025-03-26,2025,preprint,arXiv (Cornell University),,,,Yupeng Cao; Haohang Li; Yangyang Yu; Shashidhar Reddy Javaji; Yueru He; Jimin Huang; Qianqian Xie; Fabrizio Dimino; Xiao-yang Liu; K. P. Subbalakshmi; Meikang Qiu; Sophia Ananiadou; Jian-Yun Nie,"Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce \textsc{FinAudio}, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the \textsc{FinAudio} benchmark. Then, we evaluate seven prevalent AudioLLMs on \textsc{FinAudio}. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.",,,,,Benchmark (surveying); Computer science; Finance; Business; Geography; Cartography,,,,,https://arxiv.org/abs/2503.20990,http://dx.doi.org/10.48550/arxiv.2503.20990,,10.48550/arxiv.2503.20990,,,0,,0,true,,green
187-241-445-840-600,"CFinBench: A Comprehensive Chinese Financial Benchmark for Large
  Language Models",2024-07-02,2024,preprint,arXiv (Cornell University),,,,Ying Nie; Binwei Yan; Tianyu Guo; Hao Liu; Haoyu Wang; Wei He; Binfan Zheng; Weihao Wang; Qiang Li; Weijian Sun; Yunhe Wang; Dacheng Tao,"Large language models (LLMs) have achieved remarkable performance on various NLP tasks, yet their potential in more challenging and domain-specific task, such as finance, has not been fully explored. In this paper, we present CFinBench: a meticulously crafted, the most comprehensive evaluation benchmark to date, for assessing the financial knowledge of LLMs under Chinese context. In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing. (2) Financial Qualification: whether LLMs can obtain the needed financial qualified certifications, such as certified public accountant, securities qualification and banking qualification. (3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst. (4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law. CFinBench comprises 99,100 questions spanning 43 second-level categories with 3 question types: single-choice, multiple-choice and judgment. We conduct extensive experiments of 50 representative LLMs with various model size on CFinBench. The results show that GPT4 and some Chinese-oriented models lead the benchmark, with the highest average accuracy being 60.16%, highlighting the challenge presented by CFinBench. The dataset and evaluation code are available at https://cfinbench.github.io/.",,,,,Benchmark (surveying); Computer science; Finance; Business; Geography; Cartography,,,,,https://arxiv.org/abs/2407.02301,http://dx.doi.org/10.48550/arxiv.2407.02301,,10.48550/arxiv.2407.02301,,,0,,0,true,,green
187-521-965-652-191,lIntegration of NLP and NLU in the Implementation of Chatbot in Asset Management System,2024-12-23,2024,journal article,Dinasti International Journal of Education Management And Social Science,26866331; 26866358,Yayasan Dharma Indonesia Tercinta (Dinasti),,Thoha Cahya Ash Shoddiqy; Akhmad Unggul Priantoro; Gunawan Pria Utama,"<jats:p>PT XYZ, a startup in the Information Technology sector, developed an asset management application to digitalize the asset management process for its clients. However, as new features were added, the application became more complex, causing difficulties for new users. PT XYZ responded by introducing a customer service system to assist new users in exploring the company’s services and application features. To improve service efficiency while maintaining quality, the company opted to implement a chatbot. The chatbot was designed to provide automatic and responsive assistance, reducing the load on the customer service team and increasing user satisfaction. The author integrated NLP and NLU in designing the chatbot for PT XYZ using the open-source RASA framework. This framework was chosen for its strong capabilities in natural language processing and understanding conversational context. The NLP and NLU models are used to create a customer service engine in the form of text messages that answer questions specifically related to the use of the asset management application. By leveraging this technology, the chatbot can provide relevant and accurate responses, even when faced with variations in language and complex questions. Based on black box testing, the chatbot successfully recognized the intent behind user queries. The testing was conducted to evaluate how well the chatbot understood and responded to user questions. The results, using a confusion matrix, showed that precision, recall, accuracy, and f1-score all achieved a perfect score of 1.0.</jats:p>",6,2,1299,1313,Chatbot; Computer science; Natural language processing; Asset management; Artificial intelligence; Business; Finance,,,,,,http://dx.doi.org/10.38035/dijemss.v6i2.3716,,10.38035/dijemss.v6i2.3716,,,0,,0,true,,gold
187-542-415-840-318,Ranking Business Trade Preferences using GPT,2025-01-01,2025,preprint,,,Elsevier BV,,Rodrigo Cezar,,,,,,Ranking (information retrieval); Business; Chemistry; Economics; Computer science; Information retrieval,,,,,,http://dx.doi.org/10.2139/ssrn.5342099,,10.2139/ssrn.5342099,,,0,000-980-691-969-52X; 005-107-894-544-949; 010-315-266-005-781; 014-002-524-846-152; 022-449-917-607-866; 026-140-166-599-34X; 033-585-871-155-037; 034-209-145-285-862; 036-013-435-719-161; 053-683-051-669-387; 053-705-922-976-345; 053-781-832-769-283; 059-296-193-414-011; 060-538-407-847-045; 060-750-493-385-489; 061-137-542-877-334; 062-040-826-704-748; 066-489-109-542-673; 068-060-175-664-995; 073-914-941-311-687; 074-144-131-719-160; 078-737-969-557-010; 081-411-168-638-564; 084-564-866-505-060; 084-640-544-612-543; 093-212-331-446-321; 103-669-927-844-989; 104-114-600-650-127; 123-825-816-623-726; 133-355-503-776-368; 137-970-331-327-557; 146-714-084-508-679; 152-029-625-260-20X; 162-125-481-838-385; 162-384-537-573-221; 184-664-378-718-837; 195-854-365-127-110,0,false,,
187-613-980-254-851,"Unveiling the potential: "" Large language models in financial sentiment analysis, education, and market analysis""",,2025,conference proceedings article,AIP Conference Proceedings,0094243x; 15517616; 19350465,AIP Publishing,,Manan Pathak; Bela Shah; Amit Thakkar,,3255,,20010,020010,Sentiment analysis; Market analysis; Computer science; Financial market; Economic analysis; Natural language processing; Finance; Business; Economics; Marketing; Classical economics,,,,,,http://dx.doi.org/10.1063/5.0254174,,10.1063/5.0254174,,,0,057-982-454-558-150; 080-600-379-598-503; 124-690-193-709-451; 161-163-126-260-743; 189-682-373-296-939,2,false,,
187-629-888-206-845,Effects of chain-of-thought on large language model in designing and development of a financial assistant application,2025-07-01,2025,journal article,International Journal of Computing and Artificial Intelligence,27076571; 2707658x,Comprehensive Publications,,Indraneel Parthasarathy; Tanu Priya; Moayed Daneshyari,,6,2,115,122,Chain (unit); Computer science; Business; Physics; Astronomy,,,,,,http://dx.doi.org/10.33545/27076571.2025.v6.i2b.185,,10.33545/27076571.2025.v6.i2b.185,,,0,,0,false,,
187-998-021-933-832,Large language models in finance: estimating financial sentiment for stock prediction,2025-01-01,2025,preprint,,,Elsevier BV,,K. Kirtac; G. Germano,,,,,,Stock (firearms); Financial modeling; Finance; Economics; Financial economics; Econometrics; Geography; Archaeology,,,,,,http://dx.doi.org/10.2139/ssrn.5166656,,10.2139/ssrn.5166656,,,0,005-989-438-190-750; 006-399-176-500-152; 022-486-017-682-206; 052-170-807-894-470; 057-040-097-643-496; 057-982-454-558-150; 059-300-969-491-735; 063-627-948-247-679; 078-981-577-025-706; 080-206-031-206-331; 082-178-119-516-788; 113-256-775-452-798; 137-925-447-012-933; 154-058-408-974-033,2,false,,
188-206-396-124-386,InvestMate: A Hybrid AI-Driven Financial Chatbot for Personalized Stock Predictions and Investor Education,2024-11-28,2024,conference proceedings article,"2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)",,IEEE,,Neha Garg; Ayush Raghav; Nitish Adhana; Keshav Sharma,,,,738,743,Chatbot; Stock (firearms); Computer science; Stock trading; Finance; Stock market; Business; Artificial intelligence; Engineering; Mechanical engineering; Paleontology; Horse; Biology,,,,,,http://dx.doi.org/10.1109/icaiccit64383.2024.10912150,,10.1109/icaiccit64383.2024.10912150,,,0,,0,false,,
188-322-845-720-369,Comparing AI in Online Learning: The Transition and Trade-offs Between Intent-Based Learning Assistants and LLM-Chatbots in MOOCs,2024-10-16,2024,conference proceedings article,2024 IEEE Digital Education and MOOCS Conference (DEMOcon),,IEEE,,Theresa Zobel; Christoph Meinel,,,,1,6,Computer science; Chatbot; Online learning; Transition (genetics); Artificial intelligence; Multimedia; Machine learning; Human–computer interaction; Biochemistry; Chemistry; Gene,,,,,,http://dx.doi.org/10.1109/democon63027.2024.10748211,,10.1109/democon63027.2024.10748211,,,0,005-619-539-884-016; 081-117-919-928-400; 105-962-588-331-69X; 176-554-501-445-911; 187-184-252-530-464; 193-050-538-820-292; 198-421-781-400-601,4,false,,
188-366-562-172-774,WEALTH WHISPERERS: A Financial LLM,2024-04-18,2024,journal article,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,25823930,Edtech Publishers (OPC) Private Limited,,DARREN ANCHEN,"<jats:p>In an era of continually evolving financial markets, the demand for sophisticated financial advisory models tailored to the unique landscape of the Indian market is more prominent than ever before. This Paper offers a comprehensive model designed to address the nuances and complexities of the Indian financial ecosystem, empowering financial advisors with a culturally relevant and powerful toolset to effectively serve their clients.  The Financial Advisory LLM Model, customized specifically for the Indian market, seamlessly integrates Legal, Financial, and Management (LLM) expertise into a unified and cohesive platform. Leveraging cutting-edge technologies such as artificial intelligence and machine learning, this model significantly enhances the advisory process, making it more efficient and precise.  This project report delves into the extensive range of solutions provided by the Financial Advisory LLM Model, specifically tailored to address various financial challenges within the Indian context. These solutions encompass areas such as investment management, wealth preservation, and tax optimization. By incorporating these elements into the model, the report aims to empower financial advisors to meet the diverse needs of their clients effectively and efficiently.  Key Words:  Financial Advisory LLM Model, Indian financial ecosystem, Artificial intelligence and machine learning, Cutting-edge technologies, Financial technology (FinTech), Investment management</jats:p>",8,4,1,5,Financial modeling; Financial services; Finance; Context (archaeology); FinTech; Financial management; Business; Financial market; Financial engineering; Financial plan; Computer science; Knowledge management; Paleontology; Biology,,,,,https://ijsrem.com/download/wealth-whisperers-a-financial-llm/?wpdmdl=30409&refresh=665b9e585661f1717280344 https://doi.org/10.55041/ijsrem31097,http://dx.doi.org/10.55041/ijsrem31097,,10.55041/ijsrem31097,,,0,,0,true,,bronze
188-615-473-545-318,"FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal
  Reinforcement for Enhanced Financial Decision Making",2024-07-09,2024,preprint,arXiv (Cornell University),,,,Yangyang Yu; Zhiyuan Yao; Haohang Li; Zhiyang Deng; Yupeng Cao; Zhi Chen; Jordan W. Suchow; Rong Liu; Zhenyu Cui; Zhaozhuo Xu; Denghui Zhang; Koduvayur Subbalakshmi; Guojun Xiong; Yueru He; Jimin Huang; Dong Li; Qianqian Xie,"Large language models (LLMs) have demonstrated notable potential in conducting complex tasks and are increasingly utilized in various financial applications. However, high-quality sequential financial investment decision-making remains challenging. These tasks require multiple interactions with a volatile environment for every decision, demanding sufficient intelligence to maximize returns and manage risks. Although LLMs have been used to develop agent systems that surpass human teams and yield impressive investment returns, opportunities to enhance multi-sourced information synthesis and optimize decision-making outcomes through timely experience refinement remain unexplored. Here, we introduce the FinCon, an LLM-based multi-agent framework with CONceptual verbal reinforcement tailored for diverse FINancial tasks. Inspired by effective real-world investment firm organizational structures, FinCon utilizes a manager-analyst communication hierarchy. This structure allows for synchronized cross-functional agent collaboration towards unified goals through natural language interactions and equips each agent with greater memory capacity than humans. Additionally, a risk-control component in FinCon enhances decision quality by episodically initiating a self-critiquing mechanism to update systematic investment beliefs. The conceptualized beliefs serve as verbal reinforcement for the future agent's behavior and can be selectively propagated to the appropriate node that requires knowledge updates. This feature significantly improves performance while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon demonstrates strong generalization capabilities in various financial tasks, including single stock trading and portfolio management.",,,,,Reinforcement; Reinforcement learning; Conceptual framework; Computer science; Psychology; Artificial intelligence; Social psychology; Sociology; Social science,,,,,https://arxiv.org/abs/2407.06567,http://dx.doi.org/10.48550/arxiv.2407.06567,,10.48550/arxiv.2407.06567,,,0,,1,true,,green
188-876-873-194-372,BONIK: A Blockchain Empowered Chatbot for Financial Transactions,2020-01-01,2020,preprint,arXiv (Cornell University),,,,Md. Saiful Islam Bhuiyan; Abdur Razzak; Md Sadek Ferdous; Mohammad Jabed M. Chowdhury; Mohammad A. Hoque; Sasu Tarkoma,"A Chatbot is a popular platform to enable users to interact with a software or website to gather information or execute actions in an automated fashion. In recent years, chatbots are being used for executing financial transactions, however, there are a number of security issues, such as secure authentication, data integrity, system availability and transparency, that must be carefully handled for their wide-scale adoption. Recently, the blockchain technology, with a number of security advantages, has emerged as one of the foundational technologies with the potential to disrupt a number of application domains, particularly in the financial sector. In this paper, we forward the idea of integrating a chatbot with blockchain technology in the view to improve the security issues in financial chatbots. More specifically, we present BONIK, a blockchain empowered chatbot for financial transactions, and discuss its architecture and design choices. Furthermore, we explore the developed Proof-of-Concept (PoC), evaluate its performance, analyse how different security and privacy issues are mitigated using BONIK.",,,,,Blockchain; Chatbot; Computer science; Transparency (behavior); Computer security; Authentication (law); Financial transaction; Architecture; World Wide Web; Database; Database transaction; Art; Visual arts,,,,,https://arxiv.org/abs/2011.08846,http://dx.doi.org/10.48550/arxiv.2011.08846,,10.48550/arxiv.2011.08846,,,0,,0,true,other-oa,green
189-094-112-356-648,"The Power of Ensemble Methods: A Comparative Study of Machine Learning, Deep Learning, and LLMs for Financial Fraud Detection",2024-12-11,2024,conference proceedings article,2024 International Conference on AI x Data and Knowledge Engineering (AIxDKE),,IEEE,,Zahra Rezaei; Sara Safi Samghabadi; Mohammad Amin Amini; Yaser Mike Banad,,,,125,126,Deep learning; Artificial intelligence; Ensemble learning; Computer science; Machine learning; Power (physics); Quantum mechanics; Physics,,,,,,http://dx.doi.org/10.1109/aixdke63520.2024.00031,,10.1109/aixdke63520.2024.00031,,,0,003-494-823-259-024; 030-647-688-742-016; 042-008-214-459-82X; 050-097-035-089-349; 051-085-776-788-591,0,false,,
189-184-635-424-506,"Evaluating Large Language Models on Financial Report Summarization: An
  Empirical Study",2024-11-11,2024,preprint,arXiv (Cornell University),,,,Xinqi Yang; Scott Zang; Yong Ren; Dingjie Peng; Zheng Wen,"In recent years, Large Language Models (LLMs) have demonstrated remarkable versatility across various applications, including natural language understanding, domain-specific knowledge tasks, etc. However, applying LLMs to complex, high-stakes domains like finance requires rigorous evaluation to ensure reliability, accuracy, and compliance with industry standards. To address this need, we conduct a comprehensive and comparative study on three state-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their effectiveness in generating automated financial reports. Our primary motivation is to explore how these models can be harnessed within finance, a field demanding precision, contextual relevance, and robustness against erroneous or misleading information. By examining each model's capabilities, we aim to provide an insightful assessment of their strengths and limitations. Our paper offers benchmarks for financial report analysis, encompassing proposed metrics such as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative evaluation framework that integrates both quantitative metrics (e.g., precision, recall) and qualitative analyses (e.g., contextual fit, consistency) to provide a holistic view of each model's output quality. Additionally, we make our financial dataset publicly available, inviting researchers and practitioners to leverage, scrutinize, and enhance our findings through broader community engagement and collaborative improvement. Our dataset is available on huggingface.",,,,,Automatic summarization; Computer science; Empirical research; Natural language processing; Statistics; Mathematics,,,,,https://arxiv.org/abs/2411.06852,http://dx.doi.org/10.48550/arxiv.2411.06852,,10.48550/arxiv.2411.06852,,,0,,0,true,,green
190-138-429-232-034,AI-Driven Psychological Profiling and Risk Management in Margin and Options Trading Using Large Language Models,2025-02-27,2025,conference proceedings article,2025 International Conference on Technology Enabled Economic Changes (InTech),,IEEE,,Prashant Mehta; Samridhi Mehta; Devanshu Arora; Puneet Arora; Aditya Kumar Gupta; Prashant Dev Yadav,,,,1253,1259,Profiling (computer programming); Margin (machine learning); Computer science; Risk management; Data science; Machine learning; Business; Finance; Programming language,,,,,,http://dx.doi.org/10.1109/intech64186.2025.11198467,,10.1109/intech64186.2025.11198467,,,0,020-444-035-076-068; 048-109-769-834-76X; 055-207-608-310-415; 064-391-258-113-161; 116-910-168-232-163; 155-731-090-603-34X; 179-089-749-492-189,0,false,,
190-318-881-354-378,Democratizing Alpha: LLM-Driven Portfolio Construction for Retail Investors Using Public Financial Media,2025-11-14,2025,conference proceedings article,Proceedings of the 6th ACM International Conference on AI in Finance,,ACM,,Daesan Oh; Taehwan Kim; Junkyu Jang; Sung-Hyuk Park,,,,326,334,,,,,,,http://dx.doi.org/10.1145/3768292.3770376,,10.1145/3768292.3770376,,,0,002-169-596-648-859; 009-098-158-849-692; 011-223-654-787-20X; 018-164-954-712-370; 020-285-004-975-743; 022-226-959-504-357; 022-486-017-682-206; 023-201-346-648-150; 025-352-207-023-08X; 026-984-186-973-562; 037-417-163-054-836; 040-283-190-657-285; 059-789-456-451-224; 061-738-282-442-505; 066-233-070-773-263; 077-226-922-083-339; 080-828-384-981-970; 083-352-643-357-039; 083-489-102-388-067; 087-145-096-589-415; 088-640-407-815-702; 096-245-639-466-521; 099-632-707-150-545; 113-733-939-965-098; 122-145-872-610-369; 139-239-027-110-484; 162-980-429-426-882,0,false,,
190-421-734-510-989,Use of chatbots in the trade of building materials,2022-12-18,2022,journal article,Marketing and Digital Technologies,25229087; 2523434x,Odessa Polytechnic National University,,Olesia Romanenko; Liudmyla Alaverdian; Ganna Basova,"<jats:p>The aim of the article is analysing the effectiveness of the use of chatbots in comparison with other e-commerce tools, in particular with websites; determining the functions and capabilities of the chatbot when used in the construction materials trade; indicating the advantages of using chatbots for sellers and for consumers; creation of a basic scheme of chatbot operation in the trade of building materials. Analyses results. Modern types of Internet communications are capable of acting as a virtual interlocutor, repeating and reproducing a written set of human signs, providing a programmed answer to the questions asked. Chatbots use conversational intelligence as a new user interface for e-commerce applications and, accordingly, have significant advantages over other sales channels, increasing sales, improving end-customer service and significantly reducing costs. According to Juniper Research, the total number of chatbot messaging applications available worldwide will grow from 3.5 billion in 2022 to 9.5 billion by 2026.  A Ukrainian team of experienced specialists in the construction industry and IT developers developed a chatbot called BMBOT, which is aimed at the digital transformation of the construction industry using artificial intelligence, starting from communication with customers and ending with the collection and analysis of sales data. The article describes the features and capabilities of the chatbot, as well as the advantages of using it for customers and businesses. The authors showed a schematic diagram of a chatbot for a company - a supplier of construction goods, using the example of the selection and calculation of construction materials for the installation of a laminate floor. A chatbot can be developed for all types of system products. This project will be of interest to suppliers, as they will be able to convey to the client important differentials of their products, such as environmental friendliness, European standards, cost efficiency, ease of use, and others. Through the chatbot, you can form a complex customer order with the possibility of delivery, which will save the buyer's time and finances. For the seller, the use of a chatbot will facilitate communication between the client and the supplier, solve the problem of shipment and delivery of goods, and reduce the time and money spent on processing the client. The costs associated with the implementation and use of a chatbot include the initial investment in the creation of the platform and the ongoing costs of maintaining the messaging service. But these costs will be covered by the benefit received in the process of using the chatbot. Conclusions and directions for further research. The advantages of using a chatbot by trading companies: optimization of sales processes, increased decision-making efficiency, assessment and analysis of the target audience, which will lead to cost savings, increased customers, increased conversion, turnover and margins. For further research into the prospects of using chatbots in business, it is necessary to more actively introduce this digital tool into e-commerce of various fields of activity, collect information about the advantages and disadvantages of using chatbots, and improve the theoretical and methodological base of digital marketing and e-commerce.</jats:p>",6,4,7,16,Chatbot; Computer science; The Internet; Service (business); World Wide Web; Interface (matter); Business; Marketing; Bubble; Maximum bubble pressure method; Parallel computing,chatbot; artificial intelligence; digital marketing; digital tool; e-commerce; building materials,,,,https://mdt-opu.com.ua/index.php/mdt/article/download/272/171 https://doi.org/10.15276/mdt.6.4.2022.1,http://dx.doi.org/10.15276/mdt.6.4.2022.1,,10.15276/mdt.6.4.2022.1,,,0,,1,true,,gold
190-556-528-003-167,Large Language Models for Automated Financial Code Generation and Documentation in Data Pipelines,2023-03-30,2023,journal article,International Journal of Emerging Trends in Computer Science and Information Technology,30509246,ScienceTech Xplore,,,,4,1,,,Documentation; Computer science; Pipeline transport; Programming language; Code (set theory); Software engineering; Data science; Database; Engineering; Set (abstract data type); Environmental engineering,,,,,,http://dx.doi.org/10.63282/3050-9246.ijetcsit-v4i1p113,,10.63282/3050-9246.ijetcsit-v4i1p113,,,0,,0,false,,
190-749-240-473-70X,Can Large Language Models Mine Interpretable Financial Factors More Effectively? A Neural-Symbolic Factor Mining Agent Model,,2024,conference proceedings article,Findings of the Association for Computational Linguistics ACL 2024,,Association for Computational Linguistics,,Zhiwei Li; Ran Song; Caihong Sun; Wei Xu; Zhengtao Yu; Ji-Rong Wen,,,,3891,3902,Computer science; Factor (programming language); Artificial intelligence; Artificial neural network; Data mining; Machine learning; Programming language,,,,,,http://dx.doi.org/10.18653/v1/2024.findings-acl.233,,10.18653/v1/2024.findings-acl.233,,,0,,3,false,,
190-833-495-242-219,FLAG-TRADER: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: ACL 2025,,Association for Computational Linguistics,,Guojun Xiong; Zhiyang Deng; Keyi Wang; Yupeng Cao; Haohang Li; Yangyang Yu; Xueqing Peng; Mingquan Lin; Kaleb E Smith; Xiao-Yang Liu; Jimin Huang; Sophia Ananiadou; Qianqian Xie,,,,13921,13934,Reinforcement learning; Flag (linear algebra); Q-learning; Computer science; Artificial intelligence; Mathematics; Pure mathematics; Algebra over a field,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-acl.716,,10.18653/v1/2025.findings-acl.716,,,0,,0,false,,
190-879-196-845-734,"The Development of Chatbots for the Financial and Procurement Office of the College of Arts, Media and Technology, Chiang Mai University",2025-06-04,2025,conference proceedings article,ACEID Official Conference Proceedings,2189101x,The International Academic Forum(IAFOR),,Jonglak Somrang; Janejira Cheenchart,,,,871,881,Chiang mai; The arts; Procurement; Media arts; Media studies; Management; Multimedia; Sociology; Computer science; Art; Visual arts; Economics; Socioeconomics,,,,,,http://dx.doi.org/10.22492/issn.2189-101x.2025.69,,10.22492/issn.2189-101x.2025.69,,,0,,0,false,,
191-384-641-806-007,Leveraging LLM-based sentiment analysis for portfolio optimization with proximal policy optimization,,2025,conference proceedings article,Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025),,Association for Computational Linguistics,,Kemal Kirtac; Guido Germano,,,,160,169,Computer science; Portfolio optimization; Portfolio; Sentiment analysis; Artificial intelligence; Business; Finance,,,,,,http://dx.doi.org/10.18653/v1/2025.realm-1.12,,10.18653/v1/2025.realm-1.12,,,0,,1,false,,
191-420-643-158-483,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,,2023,conference proceedings article,Proceedings of the Sixth Workshop on Financial Technology and Natural Language Processing,,Association for Computational Linguistics,,Pawan Rajpoot; Ankur Parikh,"Relation extraction (RE) is a crucial task in natural language processing (NLP) that aims to identify and classify relationships between entities mentioned in text. In the financial domain, relation extraction plays a vital role in extracting valuable information from financial documents, such as news articles, earnings reports, and company filings. This paper describes our solution to relation extraction on one such dataset REFinD. The dataset was released along with shared task as a part of the Fourth Workshop on Knowledge Discovery from Unstructured Data in Financial Services, co-located with SIGIR 2023. In this paper, we employed OpenAI models under the framework of in-context learning (ICL). We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example. The first retrieval mechanism, we employed, is a learning-free dense retriever and the other system is a learning-based retriever. We were able to achieve 3rd rank overall. Our best F1-score is 0.718.",,,42,45,Relationship extraction; Computer science; Relation (database); Context (archaeology); Task (project management); Named-entity recognition; Information extraction; Artificial intelligence; Natural language processing; Domain (mathematical analysis); Rank (graph theory); Machine learning; Information retrieval; Data mining; Paleontology; Mathematical analysis; Mathematics; Management; Combinatorics; Biology; Economics,,,,,https://aclanthology.org/2023.finnlp-2.5.pdf https://doi.org/10.18653/v1/2023.finnlp-2.5 http://arxiv.org/pdf/2306.17519 http://arxiv.org/abs/2306.17519,http://dx.doi.org/10.18653/v1/2023.finnlp-2.5,,10.18653/v1/2023.finnlp-2.5,,,0,,9,true,cc-by,hybrid
191-497-417-003-876,"Integrating BERT, GPT, Prophet Algorithm, and Finance Investment Strategies for Enhanced Predictive Modeling and Trend Analysis in Blockchain Technology",2024-12-12,2024,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,null Igba Emmanuel; null Moral Kuve Ihimoyan; null Babatunde Awotiwon; null Akinkunmi Rasheed Apampa,"<jats:p>This paper explores the integration of advanced machine learning models, including BERT, GPT, and the Prophet algorithm, with finance investment strategies to enhance predictive modeling and trend analysis in blockchain technology. The rapid evolution of blockchain has transformed financial ecosystems, offering decentralized platforms for secure and transparent transactions. However, predicting market trends and investment opportunities within this domain remains a complex challenge due to high volatility and the multifaceted nature of financial data. By leveraging the natural language processing capabilities of BERT and GPT for sentiment analysis and market behavior prediction, combined with the time-series forecasting strength of the Prophet algorithm, this study aims to provide a robust framework for analyzing blockchain-driven financial markets. Furthermore, the integration of finance investment strategies ensures practical applicability by aligning machine learning insights with real-world investment decision-making processes. The proposed approach demonstrates potential for optimizing portfolio management, enhancing risk mitigation, and improving strategic investment in blockchain ecosystems. This work bridges the gap between cutting-edge machine learning technologies and financial innovation, offering valuable insights for researchers and practitioners in both domains.</jats:p>",10,6,1620,1645,Blockchain; Computational finance; Financial market; Portfolio; Computer science; Financial services; Volatility (finance); Investment banking; Investment (military); Artificial intelligence; Finance; Machine learning; Economics; Computer security; Politics; Political science; Law,,,,,,http://dx.doi.org/10.32628/cseit241061214,,10.32628/cseit241061214,,,0,020-069-551-115-56X; 026-043-088-033-900; 027-546-223-416-455; 038-076-781-098-442; 043-104-736-690-961; 052-213-499-852-056; 059-044-890-514-527; 063-523-990-474-932; 068-792-704-802-657; 072-318-074-749-148; 074-850-025-916-96X; 078-756-667-410-359; 082-405-855-942-213; 084-243-133-488-956; 086-376-189-982-381; 087-256-717-330-927; 089-539-975-347-193; 102-427-818-295-716; 111-111-116-161-516; 115-929-393-439-789; 120-884-081-630-52X; 124-531-793-212-838; 124-868-958-099-655; 125-455-548-973-304; 131-027-393-061-971; 132-602-460-568-567; 141-949-674-699-22X; 142-386-029-793-141; 145-728-370-624-313; 163-800-733-627-342; 177-135-445-648-270; 179-530-042-076-901; 185-884-443-423-607; 196-065-426-892-487,21,true,,gold
191-668-842-870-819,"AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM
  Agents",2024-09-13,2024,preprint,arXiv (Cornell University),,,,Zhe Su; Xuhui Zhou; Sanketh Rangreji; Anubha Kabra; Julia Mendelsohn; Faeze Brahman; Maarten Sap,"To be safely and successfully deployed, LLMs must simultaneously satisfy truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI agent assisting a used car salesman selling a car with flaws), partly due to ambiguous or misleading user instructions. We propose AI-LieDar, a framework to study how LLM-based agents navigate scenarios with utility-truthfulness conflicts in a multi-turn interactive setting. We design a set of realistic scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models follow malicious instructions to deceive, and even truth-steered models can still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.",,,,,Economics; Psychology; Law and economics,,,,,https://arxiv.org/abs/2409.09013,http://dx.doi.org/10.48550/arxiv.2409.09013,,10.48550/arxiv.2409.09013,,,0,,0,true,,green
191-848-784-860-097,Integrating LLMs into Financial Systems,2025-10-01,2025,book chapter,Large Language Models Ops for Finance,,Apress,,Brindha Priyadarshini Jeyaraman,,,,167,200,Business; Finance,,,,,,http://dx.doi.org/10.1007/979-8-8688-1700-7_6,,10.1007/979-8-8688-1700-7_6,,,0,,0,false,,
191-928-890-262-12X,Bias Detection and Fairness in Large Language Models for Financial Services,2025-03-16,2025,journal article,"International Journal of Scientific Research in Computer Science, Engineering and Information Technology",24563307,Technoscience Academy,,null Rahul Vats; null Shekhar Agrawal; null Srinivasa Sunil Chippada,"<jats:p>This article addresses the critical issue of algorithmic bias and fairness in Large Language Models (LLMs) deployed across financial services. As these powerful AI systems increasingly influence decision-making in credit scoring, loan approvals, fraud detection, and risk assessments, they risk perpetuating or amplifying existing societal biases. The article introduces the Bias Detection and Fairness Evaluation (BDFE) Framework, a comprehensive methodology integrating adversarial testing, fairness-aware model training, and explainable AI to identify and mitigate biases in financial applications. Through real-world case studies involving credit underwriting during a major merger, international fraud detection systems, and insurance claim processing, it demonstrates how the framework significantly reduces bias while maintaining model accuracy and regulatory compliance. The article reveals that fairness-enhanced models deliver substantial business benefits including expanded market reach, reduced regulatory risk, and improved customer trust. It provides practical guidance for financial institutions navigating the complex intersection of AI innovation, ethical considerations, and regulatory requirements in an increasingly AI-driven industry.</jats:p>",11,2,1329,1345,Business,,,,,https://ijsrcseit.com/index.php/home/article/download/CSEIT25112461/CSEIT25112461 https://doi.org/10.32628/cseit25112461,http://dx.doi.org/10.32628/cseit25112461,,10.32628/cseit25112461,,,0,,1,true,,bronze
191-951-845-562-401,<p><span>Distant Investments: </span><span>Decoding Mutual Fund Skill with Large Language Models</span></p>,,2025,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Xiyuan Ma; Matthew I. Spiegel; Hong Zhang; Yijun Zhou,,,,,,Span (engineering); Life span; Psychology; Medicine; Structural engineering; Gerontology; Engineering,,,,,,http://dx.doi.org/10.2139/ssrn.5512859,,10.2139/ssrn.5512859,,,0,,0,false,,
192-259-472-170-397,"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant",,2024,conference proceedings article,Proceedings of the Thirty-ThirdInternational Joint Conference on Artificial Intelligence,,International Joint Conferences on Artificial Intelligence Organization,,Jemin Lee; Sihyeong Park; Jinse Kwon; Jihun Oh; Yongin Kwon,"<jats:p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. ; ; In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. ; ; Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; ; ; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance;; ; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model’s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.</jats:p>",,,8113,8121,,,,,,,http://dx.doi.org/10.24963/ijcai.2024/902,,10.24963/ijcai.2024/902,,,0,,0,false,,
192-381-964-899-327,FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering,2025-10-07,2025,preprint,arXiv (Cornell University),,,,Yitao Long; Tiansheng Hu; Yilun Zhao; Arman Cohan; Chen Zhao,"Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.",,,,,Question answering; Business; Psychology; Computer science; Information retrieval,,,,,https://arxiv.org/abs/2510.06426,http://dx.doi.org/10.48550/arxiv.2510.06426,,10.48550/arxiv.2510.06426,,,0,,0,true,,green
192-434-128-020-002,Modeling Contexts in Trade-off Total Cost and Customer Satisfaction VRP via Large Language Models,,2025,journal article,IEEE Transactions on Fuzzy Systems,10636706; 19410034,Institute of Electrical and Electronics Engineers (IEEE),United States,Hong-Wei Ding; Zhen-Song Chen; Yi Yang; Weiping Ding,,,,1,11,Customer satisfaction; Computer science; Vehicle routing problem; Routing (electronic design automation); Marketing; Business; Computer network,,,,,,http://dx.doi.org/10.1109/tfuzz.2025.3621215,,10.1109/tfuzz.2025.3621215,,,0,,0,false,,
192-806-908-794-847,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,2023-11-25,2023,conference proceedings article,4th ACM International Conference on AI in Finance,,ACM,,Boyu Zhang; Hongyang Yang; Tianyu Zhou; Muhammad Ali Babar; Xiao-Yang Liu,"Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score.",,,349,356,Computer science; Context (archaeology); Artificial intelligence; Sentiment analysis; Finance; Natural language processing; Machine learning; Business; Paleontology; Biology,,,,,https://arxiv.org/pdf/2310.04027 https://arxiv.org/abs/2310.04027,http://dx.doi.org/10.1145/3604237.3626866,,10.1145/3604237.3626866,,,8,020-563-084-760-554; 035-109-675-952-739; 057-982-454-558-150; 155-673-966-612-054; 163-979-407-953-145,112,true,,green
193-136-826-957-223,Large Language Models for Financial and Investment Management: Applications and Benchmarks,2024-11-08,2024,journal article,The Journal of Portfolio Management,00954918; 21688656,With Intelligence LLC,United States,Yaxuan Kong; Yuqi Nie; Xiaowen Dong; John M. Mulvey; H. Vincent Poor; Qingsong Wen; Stefan Zohren,,51,2,162,210,Investment (military); Business; Investment management; Finance; Economics; Political science; Politics; Market liquidity; Law,,,,,,http://dx.doi.org/10.3905/jpm.2024.1.645,,10.3905/jpm.2024.1.645,,,0,,11,false,,
193-165-020-342-222,Homo Silicus is Hyper-Rational: Why LLM Agents Fail to Replicate Attention-Driven Trading,,2025,preprint,,,Elsevier BV,,John Garcia,,,,,,,,,,,,http://dx.doi.org/10.2139/ssrn.5901742,,10.2139/ssrn.5901742,,,0,,0,false,,
193-326-972-102-175,FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering,,2025,conference proceedings article,Findings of the Association for Computational Linguistics: EMNLP 2025,,Association for Computational Linguistics,,Yitao Long; Tiansheng Hu; Yilun Zhao; Arman Cohan; Chen Zhao,,,,16730,16750,,,,,,,http://dx.doi.org/10.18653/v1/2025.findings-emnlp.908,,10.18653/v1/2025.findings-emnlp.908,,,0,,0,false,,
193-497-194-135-785,Black-Box Testing of Financial Virtual Assistants,2020-12-11,2020,,Social Science Research Network,,,,Iosif Itkin; Elena Treshcheva; Luba Konnova; Pavel Braslavski; Rostislav Yavorsky,"We propose a hybrid technique of black-box testing of virtual assistants (VAs) in the financial sector. The specifics of the highly regulated industry imposes numerous limitations on the testing process: GDPR and other data protection requirements, the absence of interaction logs with real users, restricted access to internal data, etc. These limitations also decrease the applicability of a few VA testing methods that are widely described in the research literature. The approach suggested in this paper consists of semi-controlled interaction logging from the trained testers and subsequent augmenting the collected data for automated testing.",,,,,Logging; White-box testing; Process management; Regulated Industry; Financial sector; Restricted access; Research literature; Computer science; Process (engineering); Data Protection Act 1998,,,,,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3783172,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3783172,,,3147975354,,0,,0,false,,
194-074-530-719-408,BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks,2024-01-01,2024,preprint,arXiv (Cornell University),,,,Valentina Aparicio; Daniel Gordon; Sebastian G. Huayamares; Yuhuai Luo,"Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks.",,,,,Computer science; Sentiment analysis; Stock (firearms); Topic model; Artificial intelligence; Engineering; Mechanical engineering,,,,,https://arxiv.org/abs/2401.11011,http://dx.doi.org/10.48550/arxiv.2401.11011,,10.48550/arxiv.2401.11011,,,0,,0,true,cc-by,green
194-147-208-449-627,Investigating Large Language Models for Financial Causality Detection in Multilingual Setup,2023-12-15,2023,conference proceedings article,2023 IEEE International Conference on Big Data (BigData),,IEEE,,Neelesh K Shukla; Raghu Katikeri; Msp Raja; Gowtham Sivam; Shlok Yadav; Amit Vaid; Shreenivas Prabhakararao,"This paper presents our contribution to the Financial Document Causality Detection (FinCausal) task, a component of the FNP-2023 workshop. The FinCausal challenge centers on the extraction of cause-and-effect relationships from financial texts written in both English and Spanish. Recent advancements in Generative AI and Large Language Models (LLMs) have instigated investigations into their reasoning abilities, propelling our exploration of LLMs' potential for causal reasoning within the financial domain. This study also ventures into the domain of non-English languages, aiming to uncover the capacity of LLMs on this front as well. Our investigation revealed that LLMs exhibit a remarkable ability to identify causal relationships, particularly when provided with few task-specific relevant examples. Additionally, our research demonstrates the effectiveness of LLMs in processing non-English languages when given the same English prompts along with language comprehension instructions. We conducted a comparative analysis between OpenAI GPT3.5 and 4, concluding that GPT-4 model is better-suited for this purpose. Our study unveils that LLMs yield semantically similar cause and effects. This discovery highlights LLMs don't rely solely on content for the predictions and so the necessity of adopting an evaluation approach for this task, one that emphasizes also on semantic similarity metrics.",,,2866,2871,Causality (physics); Computer science; Natural language processing; Artificial intelligence; Quantum mechanics; Physics,,,,,,http://dx.doi.org/10.1109/bigdata59044.2023.10386558,,10.1109/bigdata59044.2023.10386558,,,0,055-297-193-552-800,4,false,,
194-347-983-609-632,Financial Statement Analysis with Large Language Models,2024-01-01,2024,preprint,,,Elsevier BV,,Alex G. Kim; Maximilian Muhn; Valeri V. Nikolaev,"We investigate whether an LLM can successfully perform financial statement analysis in a way similar to a professional human analyst. We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of future earnings. Even without any narrative or industry-specific information, the LLM outperforms financial analysts in its ability to predict earnings changes. The LLM exhibits a relative advantage over human analysts in situations when the analysts tend to struggle. Furthermore, we find that the prediction accuracy of the LLM is on par with the performance of a narrowly trained state-of-the-art ML model. LLM prediction does not stem from its training memory. Instead, we find that the LLM generates useful narrative insights about a company's future performance. Lastly, our trading strategies based on GPT's predictions yield a higher Sharpe ratio and alphas than strategies based on other models. Taken together, our results suggest that LLMs may take a central role in decision-making.",,,,,Statement (logic); Financial statement analysis; Financial statement; Business; Finance; Financial analysis; Accounting; Linguistics; Philosophy; Audit,,,,,http://arxiv.org/pdf/2407.17866 http://arxiv.org/abs/2407.17866,http://dx.doi.org/10.2139/ssrn.4835311,,10.2139/ssrn.4835311,,,0,002-203-354-874-040; 005-143-574-893-961; 010-915-876-550-495; 015-187-209-232-839; 015-647-802-479-570; 017-339-047-150-170; 018-977-179-780-791; 027-633-223-258-050; 032-247-680-003-649; 035-014-823-490-790; 038-819-446-504-655; 040-128-220-480-725; 041-130-744-413-581; 044-159-946-118-837; 051-163-526-665-354; 070-292-847-601-968; 072-772-269-191-199; 075-845-380-121-512; 083-793-904-750-167; 084-076-525-954-623; 090-931-669-105-227; 097-483-708-469-305; 103-368-192-033-018; 103-747-304-824-982; 108-659-204-010-764; 122-452-758-681-032; 125-910-914-668-53X; 127-642-035-076-321; 132-290-965-934-761; 135-031-523-126-911; 142-012-996-108-529; 154-692-460-912-663; 161-805-999-562-937; 170-791-744-509-933; 172-911-011-812-878,32,true,,green
194-448-451-991-095,The recent history of large language model in investment and portfolio management: is it a revolution in finance?,2025-11-06,2025,journal article,Journal of Management History,17511348; 17587751,Emerald,United Kingdom,Rodrigo F. Malaquias; Yujong Hwang,"<jats:sec>;                     <jats:title>Purpose</jats:title>;                     <jats:p>ChatGPT gained significant attention around the world, reinforcing the interaction between humans and sophisticated computer programs. The effects of this technology on people’s quality of life are multifaceted, and a key area of debate is how it affects the natural human need for creativity. Recent advancements in large language models (LLMs) have motivated numerous studies to evaluate the use of ChatGPT in the context of investment strategies and portfolio management, particularly taking into account the complex and dynamic nature of financial market data. Considering this overview, the purpose of this research is to conduct a literature review to explore the recent history of artificial intelligence (AI) in investment, asset pricing and portfolio management.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Design/methodology/approach</jats:title>;                     <jats:p>To develop this literature review, the papers were selected from Scopus and Web of Science databases.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Findings</jats:title>;                     <jats:p>LLMs, in general, and ChatGPT, in particular, are transforming the way researchers can conduct studies in the field of finance, as well as the way individuals, firms and investment teams can analyze large amounts and diverse types of financial data to support investment strategies and portfolio management. The results suggest that while LLMs mark a relevant milestone in the history of finance, it still has some limitations.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Practical implications</jats:title>;                     <jats:p>Investors and financial institutions can use the results of this paper to understand how AI can support financial data analysis.</jats:p>;                   </jats:sec>;                   <jats:sec>;                     <jats:title>Originality/value</jats:title>;                     <jats:p>This research summarizes key contributions of studies addressing LLMs to the literature, with a particular focus on its applications in investments and portfolio management.</jats:p>;                   </jats:sec>",,,1,17,,,,,National Council for Scientific and Technological Development; Minas Gerais State Research Support Foundation,,http://dx.doi.org/10.1108/jmh-01-2025-0008,,10.1108/jmh-01-2025-0008,,,0,005-553-153-856-945; 009-096-390-741-572; 010-182-446-537-420; 012-634-631-533-381; 027-366-553-958-670; 027-705-607-730-326; 027-834-026-409-711; 030-511-987-534-062; 030-965-991-617-57X; 034-766-037-503-123; 035-014-823-490-790; 035-325-259-427-659; 038-420-353-448-995; 044-758-674-793-904; 045-484-225-394-116; 050-785-292-020-819; 052-562-208-955-529; 053-740-266-554-112; 053-946-201-015-461; 058-267-029-308-058; 065-010-405-236-060; 067-336-618-397-610; 070-285-668-639-869; 072-750-965-244-728; 073-054-189-602-509; 074-601-462-746-690; 077-904-429-840-586; 081-697-470-656-444; 086-260-434-815-196; 086-665-182-419-394; 087-107-196-554-693; 091-045-550-749-586; 100-446-896-445-856; 100-535-669-734-070; 102-705-597-705-968; 104-277-691-103-925; 104-926-507-961-442; 105-262-624-771-696; 105-789-171-897-536; 108-760-705-522-553; 110-133-405-502-904; 110-989-760-950-013; 113-165-126-603-679; 114-247-597-982-201; 121-269-504-973-455; 121-420-944-532-812; 123-725-054-263-091; 123-906-633-448-288; 127-606-420-638-425; 128-037-224-234-65X; 129-194-097-023-213; 143-875-130-007-110; 147-315-528-414-322; 147-951-066-981-37X; 154-042-138-562-576; 156-226-721-432-567; 157-852-466-158-269; 162-483-858-141-888; 167-471-471-643-799; 167-727-392-269-352; 168-036-345-097-619; 168-196-673-540-74X; 178-551-929-566-887; 183-764-443-755-602; 190-888-164-044-777; 191-041-689-806-87X; 191-121-254-949-724; 191-424-188-274-666; 193-050-538-820-292; 193-136-826-957-223; 199-327-163-535-344,0,false,,
194-687-489-001-649,"Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade
  Offs in Large Language Model Training",2024-04-01,2024,preprint,arXiv (Cornell University),,,,Vivian Liu; Yiqiao Yin,"Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We argue for the training of LLMs in a way that is responsible and sustainable by suggesting measures for reducing carbon emissions. Furthermore, we discuss how the choice of hardware affects CO2 emissions by contrasting the CO2 emissions during model training for two widely used GPUs. Based on our results, we present the benefits and drawbacks of our proposed solutions and make the argument for the possibility of training more environmentally safe AI models without sacrificing their robustness and performance.",,,,,Training (meteorology); Carbon fibers; Computer science; Environmental science; Geography; Meteorology; Algorithm; Composite number,,,,,https://arxiv.org/abs/2404.01157,http://dx.doi.org/10.48550/arxiv.2404.01157,,10.48550/arxiv.2404.01157,,,0,,0,true,,green
194-704-666-068-820,LLM-Enhanced Trading Decision Framework with Multi-Scale Memory for Electricity Markets,2025-09-29,2025,conference proceedings article,"2025 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)",,IEEE,,Ruixiang Tian; Minghui Zhang; Yilei Liu; Hao Wang; Yanru Zhang,,,,1,6,Electricity; Scale (ratio); Computer science; Electrical engineering; Engineering; Physics; Quantum mechanics,,,,Research and Development; Australian Research Council,,http://dx.doi.org/10.1109/smartgridcomm65349.2025.11204628,,10.1109/smartgridcomm65349.2025.11204628,,,0,016-275-884-543-565; 021-499-376-760-436; 034-111-475-208-944; 038-074-429-172-624; 049-469-968-063-22X; 080-434-101-637-511; 088-200-518-695-441; 100-398-098-235-682; 120-935-980-195-354; 123-671-400-388-233; 124-631-028-021-91X; 138-806-616-215-286; 149-444-522-983-894; 184-359-942-138-876,0,false,,
194-899-063-628-983,Harnessing LLMs for Financial Forecasting: A Systematic Review of Advances in Stock Market Prediction and Portfolio Optimization,2024-11-30,2024,journal article,International Journal for Research in Applied Science and Engineering Technology,23219653,International Journal for Research in Applied Science and Engineering Technology (IJRASET),,Prof. Maheshwari Divate; Parikshit Jadhav; Ashutosh Jha; Sachin Joshi; Krushna Darak,"<jats:p>Abstract: This review paper examines the rapidly evolving landscape of Large Language Models (LLMs) in financial analysis, synthesizing recent advances and applications in this transformative field. We trace the progression from traditional natural language processing (NLP) methods to contemporary language models in financial applications, showing how these technologies are reshaping market analysis, risk assessment, fraud detection, and investment decision-making across various financial sectors. The paper offers an in-depth overview of LLM architectures and approaches used in finance, covering both general-purpose models adapted for financial tasks and specialized models tailored for industry needs. Through systematic analysis of the latest research and empirical studies, we assess the capabilities of LLMs in processing diverse financial data sources, including real-time market data, news articles, social media sentiment, earnings calls, and regulatory filings, enhancing insights and predictive accuracy. We identify key challenges in LLM implementation, such as the need for real-time data processing, high accuracy, and interpretability, crucial for trust and adoption in high-stakes contexts. Additionally, we explore emerging trends and future research directions, highlighting both the transformative potential and limitations of LLMs as they redefine analytical frameworks and decision support in finance. This review underscores the need for ongoing research to bridge gaps and fully realize LLMs’ potential in reshaping financial analysis and practices</jats:p>",12,11,1101,1105,Portfolio; Portfolio optimization; Stock market; Financial economics; Financial market; Economics; Stock (firearms); Actuarial science; Business; Econometrics; Finance; Engineering; Geography; Mechanical engineering; Context (archaeology); Archaeology,,,,,,http://dx.doi.org/10.22214/ijraset.2024.65283,,10.22214/ijraset.2024.65283,,,0,,3,true,,bronze
195-130-335-892-133,"CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models
  using Data Fusion in Financial Applications",2024-07-02,2024,preprint,arXiv (Cornell University),,,,Yupeng Cao; Zhiyuan Yao; Zhi Chen; Zhiyang Deng,"The integration of Large Language Models (LLMs) into financial analysis has garnered significant attention in the NLP community. This paper presents our solution to IJCAI-2024 FinLLM challenge, investigating the capabilities of LLMs within three critical areas of financial tasks: financial classification, financial text summarization, and single stock trading. We adopted Llama3-8B and Mistral-7B as base models, fine-tuning them through Parameter Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) approaches. To enhance model performance, we combine datasets from task 1 and task 2 for data fusion. Our approach aims to tackle these diverse tasks in a comprehensive and integrated manner, showcasing LLMs' capacity to address diverse and complex financial tasks with improved accuracy and decision-making capabilities.",,,,,Task (project management); Computer science; Fusion; Finance; Artificial intelligence; Business; Economics; Linguistics; Management; Philosophy,,,,,https://arxiv.org/abs/2407.01953,http://dx.doi.org/10.48550/arxiv.2407.01953,,10.48550/arxiv.2407.01953,,,0,,1,true,,green
195-170-862-484-234,Sentiment Spin: Attacking Financial Sentiment with Gpt-3,2023-01-01,2023,preprint,,,Elsevier BV,,Markus Leippold,"In this study, we explore the susceptibility of financial sentiment analysis to adversarial attacks that manipulate financial texts. With the rise of AI readership in the financial sector, companies are adapting their language and disclosures to fit AI processing better, leading to concerns about the potential for manipulation. In the finance literature, keyword-based methods, such as dictionaries, are still widely used for financial sentiment analysis due to their perceived transparency. However, our research demonstrates the vulnerability of keyword-based approaches by successfully generating adversarial attacks using the sophisticated transformer model, GPT-3.",,,,,Sentiment analysis; Business; Finance; Computer science; Artificial intelligence,,,,,https://www.zora.uzh.ch/id/eprint/234515/1/1_s2.0_S154461232300329X_main.pdf https://www.zora.uzh.ch/id/eprint/236069/1/Sentiment_Spin.pdf,http://dx.doi.org/10.2139/ssrn.4384956,,10.2139/ssrn.4384956,,,0,,5,true,,green
195-781-612-719-443,"Safeguarding Large Language Models in Real-time with Tunable
  Safety-Performance Trade-offs",2025-01-02,2025,preprint,arXiv (Cornell University),,,,Joao Fonseca; Andrew Bell; Julia Stoyanovich,"Large Language Models (LLMs) have been shown to be susceptible to jailbreak attacks, or adversarial attacks used to illicit high risk behavior from a model. Jailbreaks have been exploited by cybercriminals and blackhat actors to cause significant harm, highlighting the critical need to safeguard widely-deployed models. Safeguarding approaches, which include fine-tuning models or having LLMs ""self-reflect"", may lengthen the inference time of a model, incur a computational penalty, reduce the semantic fluency of an output, and restrict ``normal'' model behavior. Importantly, these Safety-Performance Trade-offs (SPTs) remain an understudied area. In this work, we introduce a novel safeguard, called SafeNudge, that combines Controlled Text Generation with ""nudging"", or using text interventions to change the behavior of a model. SafeNudge triggers during text-generation while a jailbreak attack is being executed, and can reduce successful jailbreak attempts by 30% by guiding the LLM towards a safe responses. It adds minimal latency to inference and has a negligible impact on the semantic fluency of outputs. Further, we allow for tunable SPTs. SafeNudge is open-source and available through https://pypi.org/, and is compatible with models loaded with the Hugging Face ""transformers"" library.",,,,,Safeguarding; Computer science; Business; Computer security; Medicine; Nursing,,,,,https://arxiv.org/abs/2501.02018,http://dx.doi.org/10.48550/arxiv.2501.02018,,10.48550/arxiv.2501.02018,,,0,,0,true,,green
195-796-372-082-616,Portfolio Construction with News Sentiment using Large Language Model,,2023,journal article,SSRN Electronic Journal,15565068,Elsevier BV,,Qi Zhang; Jianxin Wang; Wei Liu,,,,,,Sentiment analysis; Portfolio; Computer science; Actuarial science; Natural language processing; Business; Finance,,,,,,http://dx.doi.org/10.2139/ssrn.4454949,,10.2139/ssrn.4454949,,,0,000-454-346-263-577; 000-618-066-096-979; 003-308-586-533-154; 006-399-176-500-152; 007-745-005-548-640; 013-544-219-879-206; 013-770-623-528-921; 014-435-609-630-063; 014-584-126-186-85X; 015-296-101-569-213; 018-193-483-515-989; 018-738-780-501-003; 020-506-148-516-294; 020-965-431-531-011; 022-486-017-682-206; 022-823-390-114-498; 024-608-245-302-901; 025-867-000-800-243; 026-451-375-335-947; 028-089-236-226-656; 040-672-185-519-039; 042-152-947-297-801; 044-020-624-114-140; 046-566-166-111-15X; 046-833-188-916-868; 047-871-035-706-003; 051-389-300-065-390; 052-170-807-894-470; 053-015-995-110-341; 054-233-047-106-221; 054-309-998-155-297; 054-514-676-319-693; 055-318-598-916-211; 057-040-097-643-496; 057-982-454-558-150; 060-040-167-116-658; 060-459-945-830-165; 063-627-948-247-679; 064-082-736-992-676; 064-540-856-003-276; 067-755-440-346-74X; 068-023-248-247-605; 069-476-068-106-168; 071-549-719-560-193; 078-981-577-025-706; 079-413-308-537-026; 080-600-379-598-503; 087-195-434-141-855; 088-420-275-638-868; 089-236-153-235-514; 089-751-234-475-049; 089-844-745-151-655; 092-196-990-956-962; 094-892-140-158-288; 096-124-262-017-361; 099-313-865-267-572; 103-292-054-804-39X; 104-972-636-546-082; 110-047-638-667-418; 113-256-775-452-798; 115-061-352-792-603; 117-586-378-674-466; 126-103-153-468-474; 128-950-131-708-238; 129-631-634-304-647; 131-897-893-142-646; 134-927-262-006-588; 137-408-709-935-186; 137-923-832-529-809; 137-925-447-012-933; 143-224-880-625-779; 147-427-616-482-743; 154-042-138-562-576; 160-838-562-631-770; 168-485-640-067-096; 172-084-336-031-66X; 189-197-165-376-265,0,false,,
195-872-378-393-695,"Research Guides: LLM Writing Group: Private International Law, Law & Trade (2020/21): Getting Help",2020-09-22,2020,libguide,,,,,Michelle Pearse,,,,,,Political science; Law; Conflict of laws; Group (mathematics),,,,,https://guides.library.harvard.edu/c.php?g=1082940&p=7905980,https://guides.library.harvard.edu/c.php?g=1082940&p=7905980,,,3088651044,,0,,0,false,,
196-068-497-575-765,"FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial
  Workflows with LLMs",2025-05-18,2025,preprint,arXiv (Cornell University),,,,Junzhe Jiang; Chang Yang; Aixin Cui; Sihan Jin; Ruiyu Wang; Bo Li; Xiao Huang; Dongning Sun; Xinrun Wang,"Financial tasks are pivotal to global economic stability; however, their execution faces challenges including labor intensive processes, low error tolerance, data fragmentation, and tool limitations. Although large language models (LLMs) have succeeded in various natural language processing tasks and have shown potential in automating workflows through reasoning and contextual understanding, current benchmarks for evaluating LLMs in finance lack sufficient domain-specific data, have simplistic task design, and incomplete evaluation frameworks. To address these gaps, this article presents FinMaster, a comprehensive financial benchmark designed to systematically assess the capabilities of LLM in financial literacy, accounting, auditing, and consulting. Specifically, FinMaster comprises three main modules: i) FinSim, which builds simulators that generate synthetic, privacy-compliant financial data for companies to replicate market dynamics; ii) FinSuite, which provides tasks in core financial domains, spanning 183 tasks of various types and difficulty levels; and iii) FinEval, which develops a unified interface for evaluation. Extensive experiments over state-of-the-art LLMs reveal critical capability gaps in financial reasoning, with accuracy dropping from over 90% on basic tasks to merely 40% on complex scenarios requiring multi-step reasoning. This degradation exhibits the propagation of computational errors, where single-metric calculations initially demonstrating 58% accuracy decreased to 37% in multimetric scenarios. To the best of our knowledge, FinMaster is the first benchmark that covers full-pipeline financial workflows with challenging tasks. We hope that FinMaster can bridge the gap between research and industry practitioners, driving the adoption of LLMs in real-world financial practices to enhance efficiency and accuracy.",,,,,Pipeline (software); Workflow; Benchmark (surveying); Business; Finance; Computer science; Geography; Operating system; Database; Cartography,,,,,https://arxiv.org/abs/2505.13533,http://dx.doi.org/10.48550/arxiv.2505.13533,,10.48550/arxiv.2505.13533,,,0,,0,true,,green
196-872-577-044-718,CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models,,2025,conference proceedings article,Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),,Association for Computational Linguistics,,Ying Nie; Binwei Yan; Tianyu Guo; Hao Liu; Haoyu Wang; Wei He; Binfan Zheng; Weihao Wang; Qiang Li; Weijian Sun; Yunhe Wang; Dacheng Tao,,,,876,891,Benchmark (surveying); Computer science; Geology; Geodesy,,,,,,http://dx.doi.org/10.18653/v1/2025.naacl-long.40,,10.18653/v1/2025.naacl-long.40,,,0,,0,false,,
197-210-699-395-528,Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English,2024-08-24,2024,conference proceedings article,Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,ACM,,Xiao Zhang; Ruoyu Xiang; Chenhan Yuan; Duanyu Feng; Weiguang Han; Alejandro Lopez-Lira; Xiao-Yang Liu; Meikang Qiu; Sophia Ananiadou; Min Peng; Jimin Huang; Qianqian Xie,"Despite Spanish's pivotal role in the global finance industry, a pronounced gap exists in Spanish financial natural language processing (NLP) and application studies compared to English, especially in the era of large language models (LLMs). To bridge this gap, we unveil Toisón de Oro, the first bilingual framework that establishes instruction datasets, finetuned LLMs, and evaluation benchmark for financial LLMs in Spanish joint with English. We construct a rigorously curated bilingual instruction dataset including over 144K Spanish and English samples from 15 datasets covering 7 tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual financial applications. We evaluate our model and existing LLMs using FLARE-ES, the first comprehensive bilingual evaluation benchmark with 21 datasets covering 9 tasks. The FLARE-ES benchmark results reveal a significant multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic instruction tuning and leveraging data from diverse linguistic resources, highlighting the positive impact of cross-linguistic transfer. All our datasets, models, and benchmarks have been released.",,,6236,6246,Business; Finance; Computer science,,,,National Science and Technology Major Project; General Program of Natural Science Foundation of China; the NextGenerationEU; Key R&D Project of Hubei Province; New Energy and Industrial Technology Development Organization; National Natural Science Foundation of China,https://arxiv.org/pdf/2402.07405 https://arxiv.org/abs/2402.07405,http://dx.doi.org/10.1145/3637528.3671554,,10.1145/3637528.3671554,,,0,004-853-485-857-424; 016-887-102-988-613; 018-866-913-622-957; 025-375-743-897-030; 045-269-723-137-348; 056-542-756-996-88X; 057-982-454-558-150; 070-049-608-205-466; 080-878-370-554-949; 092-770-866-733-36X; 099-500-667-461-132; 133-419-572-269-204,7,true,,green
197-223-811-619-679,The Development of Large Language Models in the Financial Field,2025-04-28,2025,journal article,Proceedings of Business and Economic Studies,2209265x; 22092641,"Bio-Byword Scientific Publishing, Pty. Ltd.",,Yanling Liu; Yun Li,"<jats:p>With the rapid development of natural language processing (NLP) and machine learning technology, applying large language models (LLMs) in the financial field shows a significant growth trend. This paper systematically reviews the development status, main applications, challenges, and future development direction of LLMs in the financial field. Financial Language models (FinLLMs) have been successfully applied to many scenarios, such as sentiment analysis, automated trading, risk assessment, etc., through deep learning architectures such as BERT, Llama, and domain data fine-tuning. However, issues such as data privacy, model interpretability, and ethical governance still pose constraints to their widespread application. Future research should focus on improving model performance, addressing bias issues, strengthening privacy protection, and establishing a sound regulatory framework to ensure the healthy development of LLMs in the financial sector.</jats:p>",8,2,49,54,Field (mathematics); Business; Mathematics; Pure mathematics,,,,,,http://dx.doi.org/10.26689/pbes.v8i2.10267,,10.26689/pbes.v8i2.10267,,,0,,0,true,,bronze
197-279-385-479-281,"QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large
  Language Model",2024-02-06,2024,preprint,arXiv (Cornell University),,,,Saizhuo Wang; Hang Yuan; Lionel M. Ni; Jian Guo,"Autonomous agents based on Large Language Models (LLMs) that devise plans and tackle real-world challenges have gained prominence.However, tailoring these agents for specialized domains like quantitative investment remains a formidable task. The core challenge involves efficiently building and integrating a domain-specific knowledge base for the agent's learning process. This paper introduces a principled framework to address this challenge, comprising a two-layer loop.In the inner loop, the agent refines its responses by drawing from its knowledge base, while in the outer loop, these responses are tested in real-world scenarios to automatically enhance the knowledge base with new insights.We demonstrate that our approach enables the agent to progressively approximate optimal behavior with provable efficiency.Furthermore, we instantiate this framework through an autonomous agent for mining trading signals named QuantAgent. Empirical results showcase QuantAgent's capability in uncovering viable financial signals and enhancing the accuracy of financial forecasts.",,,,,Holy Grail; Computer science; World Wide Web,,,,,https://arxiv.org/abs/2402.03755,http://dx.doi.org/10.48550/arxiv.2402.03755,,10.48550/arxiv.2402.03755,,,0,,2,true,,green
197-302-310-261-045,FinMem: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design,,2025,journal article,IEEE Transactions on Big Data,23327790; 23722096,Institute of Electrical and Electronics Engineers (IEEE),,Yangyang Yu; Haohang Li; Zhi Chen; Yuechen Jiang; Yang Li; Jordan W. Suchow; Denghui Zhang; Khaldoun Khashanah,,11,6,3443,3459,,,,,,,http://dx.doi.org/10.1109/tbdata.2025.3593370,,10.1109/tbdata.2025.3593370,,,0,000-177-105-616-607; 012-194-321-529-671; 018-721-557-134-596; 018-866-913-622-957; 020-142-555-000-805; 020-649-956-170-47X; 023-828-224-273-425; 030-233-624-504-860; 038-241-735-129-443; 040-336-141-036-374; 043-871-173-599-124; 043-899-490-415-899; 055-807-088-113-125; 056-544-315-402-264; 060-339-517-968-401; 069-342-392-300-129; 069-909-532-822-274; 072-250-552-207-557; 075-299-226-215-531; 076-403-621-361-233; 077-168-490-480-353; 088-536-752-660-721; 091-576-618-454-390; 094-198-278-160-874; 094-971-511-071-911; 110-117-313-236-899; 114-356-780-706-953; 115-675-627-245-542; 118-603-455-848-422; 122-619-267-680-643; 122-675-055-275-028; 122-795-259-391-609; 124-831-727-114-417; 137-898-855-175-685; 139-239-027-110-484; 140-247-441-843-347; 142-267-721-696-397; 151-923-143-063-350; 154-201-884-155-679; 157-283-175-441-200; 164-630-963-895-841; 173-384-213-892-313; 180-062-583-239-276; 180-650-157-858-526; 186-827-515-649-986; 191-368-768-023-034; 198-011-392-712-589,0,false,,
197-564-054-078-515,AI-Driven Chatbots in Financial Institutions: Strengthening Fraud Prevention and Risk Management,2024-12-20,2024,conference proceedings article,2024 International Conference on Artificial Intelligence and Quantum Computation-Based Sensor Application (ICAIQSA),,IEEE,,Chetan Trivedi; Sunil Kumar,,,,1,6,Risk management; Financial management; Business; Finance; Computer science,,,,Chandigarh University,,http://dx.doi.org/10.1109/icaiqsa64000.2024.10882416,,10.1109/icaiqsa64000.2024.10882416,,,0,,2,false,,
198-348-246-455-530,BERT for Stock Trading: Can Large Language Models Predict Long-Term Stock Price Movements?,2025-12-30,2025,journal article,The Journal of Financial Data Science,26403943,With Intelligence LLC,,Stefan Pasch; Daniel Ehnes,,,,jfds.2025.1.213,,,,,,,,http://dx.doi.org/10.3905/jfds.2025.1.213,,10.3905/jfds.2025.1.213,,,0,,0,false,,
198-570-572-548-510,"The Future of Financial Assistance: Leveraging LLMs for Personalized, Human-Like Interactions",2025-01-28,2025,journal article,International journal of Web & Semantic Technology,09762280; 09759026,Academy and Industry Research Collaboration Center (AIRCC),,Hamza Landolsi; Ines Abdeljaoued-Tej,"<jats:p>Generative Artificial Intelligence (GenAI) is transforming the business landscape by enhancing accessibility, efficiency, cost-effectiveness, and innovation. This paper investigates the application of Large Language Models (LLMs) and GenAI in the financial sector, proposing a novel framework to reimagine robo-advisory systems. The framework shifts from traditional, rigid platforms to a more humanized approach that actively engages investors in a personalized asset selection process while leveraging LLMs to better understand their goals and profiles. We present an end-to-end solution designed to address key limitations of conventional roboadvisors, such as inflexibility, restricted asset type offerings (typically limited to equities), and challenges in accessing high-quality, real-time data. The proposed architecture incorporates dynamic client profiling, risk aversion estimation, and portfolio optimization. A tailored asset selector agent, supported by robust data pipelines, ensures the curation of up-to-date market information. Through iterative development, we utilized prompt engineering and multi-agent workflows to refine user interactions and deliver actionable insights. By implementing an innovative chatbot platform, we demonstrate the potential of LLMs to revolutionize customer service, enhance investor engagement, and provide strategic financial guidance. This study highlights the transformative impact of GenAI in creating more adaptive, personalized, and effective financial advisory solutions.</jats:p>",16,1,15,34,Business; Finance; Computer science; Accounting,,,,,,http://dx.doi.org/10.5121/ijwest.2025.16102,,10.5121/ijwest.2025.16102,,,0,,0,true,,bronze
198-656-305-597-446,Large Language Models for Corporate Financial Distress Prediction: Overview and Exploration,2025-06-30,2025,journal article,"Highlights in Business, Economics and Management",2957952x,Darcy & Roy Press Co. Ltd.,,Weiya Fu,"<jats:p>With the increasing complexity of the business environment and the evolution of information disclosure tools, financial distress prediction (FDP) is gradually transforming from structured data-driven to semantic information fusion. Traditional models rely on financial ratios and statistical indicators, which make it difficult to capture risk propensity in “soft signals” such as management tone and textual metaphors. And the existing large language models (LLMs) provide a new perspective for the text-driven FDP system by the excellent semantic modeling and inference generation capabilities. This paper systematically sorts out the application path of LLMs in FDP by focusing on variable construction and model construction. Three types of representative text features, namely, emotional tone, semantic embedding, and generative variables, are summarized. The modeling mechanism analyzes LLMs as categorical predictive models and their fusion patterns in multimodal integrated systems. In addition, this work points out that there are still challenges such as scarce data labels, non-interpretable models, high cost of system deployment and lack of compliance mechanisms in existing studies, which urgently requires the evolution towards an intelligent early warning system with high credibility, transparency and adaptability under the synergistic promotion of multidisciplinary efforts. This work will provide a cutting-edge reference for constructing intelligent risk control systems and developing financial regulatory technology.</jats:p>",58,,319,324,Financial distress; Business; Financial system,,,,,,http://dx.doi.org/10.54097/2gaabd72,,10.54097/2gaabd72,,,0,013-308-543-369-844; 023-891-410-598-17X; 028-050-640-100-440; 030-721-862-358-355; 031-699-361-568-870; 046-345-290-864-459; 048-699-882-498-246; 051-592-746-379-54X; 054-224-269-910-915; 072-222-626-626-722; 091-215-759-150-725; 094-611-827-820-65X; 095-761-242-666-669; 113-332-218-839-924; 117-047-065-769-760; 117-517-155-827-876; 119-969-508-565-091; 125-812-396-894-254; 128-096-165-167-740; 143-220-226-632-888; 151-037-020-138-360; 154-042-138-562-576; 159-585-251-610-202; 171-008-154-421-439; 171-166-942-189-558; 190-209-715-673-59X; 191-041-689-806-87X; 192-806-908-794-847; 194-304-815-250-165; 195-380-773-935-297,0,false,,
199-164-314-858-99X,TraderTalk: An LLM Behavioural ABM applied to Simulating Human Bilateral Trading Interactions,2024-12-04,2024,conference proceedings article,2024 IEEE International Conference on Agents (ICA),,IEEE,,Alicia Vidler; Toby Walsh,,,,164,167,Computer science; Human–computer interaction,,,,,,http://dx.doi.org/10.1109/ica63002.2024.00042,,10.1109/ica63002.2024.00042,,,0,006-608-497-742-345; 021-584-528-623-251; 030-757-941-886-943; 032-387-381-449-530; 033-560-233-522-209; 046-105-697-660-033; 056-465-555-696-550; 072-659-678-468-799; 075-299-226-215-531; 075-338-707-825-559; 098-107-690-367-85X; 129-892-094-684-444; 176-094-257-521-067,0,false,,
199-327-163-535-344,Generative artificial intelligence chatbots in investment decision-making: a phantom menace or a new hope?,2025-03-10,2025,journal article,foresight,14636689; 14659832,Emerald,United Kingdom,Kumbirai Mabwe; Nasir Aminu; Stanislav Hristov Ivanov; Diyan Dimov,"<jats:sec><jats:title content-type=""abstract-subheading"">Purpose</jats:title>; <jats:p>This study aims to investigate the relevance, accuracy, specificity and justification of investment recommendations of generative artificial intelligence (GenAI) chatbots for different investment capitals and countries (UK and Bulgaria).</jats:p>; </jats:sec>; <jats:sec><jats:title content-type=""abstract-subheading"">Design/methodology/approach</jats:title>; <jats:p>A two-stage mixed methods approach was used. Prompts were queried into OpenAI’s ChatGPT, Microsoft Bing and Google Bard (now Gemini). Finance and investment practitioners and finance and investment lecturers assessed the chatbots’ recommendations through an online questionnaire using a five-point Likert scale. The Chi-squared test, Wilcoxon-signed ranks test, Mann–Whitney <jats:italic>U</jats:italic> test and Friedman test were used for data analysis to compare GenAIs’ recommendations for the UK and Bulgaria across different amounts of investment capital and to assess the consistency of the chatbots.</jats:p>; </jats:sec>; <jats:sec><jats:title content-type=""abstract-subheading"">Findings</jats:title>; <jats:p>GenAI chatbots’ responses were found to perform medium-to-high in terms of relevance, accuracy, specificity and justification. For the UK sample, the amount of investment had a marginal effect but prompt timing had an interesting impact. Unlike the British sample, the GenAI application, prompt timing and investment amount did not significantly influence the Bulgarian respondents’ evaluations. While the mean responses of the British sample were slightly higher, these differences were not statistically significant, indicating that ChatGPT, Bing and Bard performed similarly in both the UK and Bulgaria.</jats:p>; </jats:sec>; <jats:sec><jats:title content-type=""abstract-subheading"">Originality/value</jats:title>; <jats:p>The study assesses the relevance, accuracy, specificity and justification of GenAI chatbots’ investment recommendations for two different periods, investment amounts and countries.</jats:p>; </jats:sec>",27,4,820,863,Generative grammar; Investment (military); Artificial intelligence; Computer science; Psychology; Cognitive science; Political science; Law; Politics,,,,,,http://dx.doi.org/10.1108/fs-06-2024-0122,,10.1108/fs-06-2024-0122,,,0,000-824-696-220-011; 012-121-456-001-578; 015-880-921-822-280; 016-261-541-516-330; 018-068-292-560-215; 023-318-150-053-234; 026-758-567-804-396; 026-983-179-499-108; 029-633-297-149-300; 031-677-844-859-333; 035-612-385-226-657; 038-788-689-124-99X; 040-353-042-523-112; 041-882-695-246-220; 049-539-988-689-490; 051-762-643-430-110; 052-407-175-838-608; 058-408-946-230-514; 059-640-149-464-572; 069-507-976-179-943; 075-129-793-350-340; 078-882-601-731-229; 082-661-002-316-563; 083-989-154-270-555; 091-464-696-578-261; 097-580-995-451-371; 112-657-470-256-607; 119-146-042-099-078; 121-215-593-767-368; 126-416-704-188-100; 130-454-132-850-921; 134-136-252-227-167; 134-245-919-952-07X; 137-911-103-851-138; 139-239-027-110-484; 142-556-459-252-950; 148-794-870-392-357; 154-513-179-142-971; 154-762-066-597-395; 161-206-420-147-642; 163-643-688-727-398; 164-632-309-285-204; 164-723-276-052-474; 168-111-059-976-396; 168-944-989-890-456; 185-007-495-739-641; 187-382-938-962-20X; 191-041-689-806-87X; 191-424-188-274-666; 192-813-949-331-350; 194-794-850-269-106; 196-395-945-746-897; 198-605-588-413-677,5,false,,
199-392-235-944-050,Empowering financial futures: Large language models in the modern financial landscape,2024-07-25,2024,journal article,EAI Endorsed Transactions on AI and Robotics,27907511,European Alliance for Innovation n.o.,,Xinwei Cao; Shuai Li; Vasilios Katsikis; Ameer Tamoor Khan; Hailing He; Zhengping Liu; Lieping Zhang; Chen Peng,"<jats:p>In this paper, we delve into the transformative influence of Large Language Models (LLMs) in the financial sector. Through meticulous exploration, we uncover the multifaceted applications of LLMs, ranging from elevating customer support and fortifying fraud detection to reshaping market analysis and prediction. LLMs, with their unparalleled ability to process extensive textual data, bring forth innovative solutions and insights. However, we also address critical challenges such as user trust and ethical considerations, emphasizing the need for responsible integration. Collaborative efforts between industry stakeholders and researchers are essential prerequisites for making a pivotal stride towards a future where LLMs redefine financial practices, with efficiency, accuracy, and ethical precision shaping the industry’s evolution.</jats:p>",3,,,,Transformative learning; Futures contract; Process (computing); Financial market; Business; Finance; Computer science; Sociology; Pedagogy; Operating system,,,,,https://publications.eai.eu/index.php/airo/article/download/6117/3305 https://doi.org/10.4108/airo.6117,http://dx.doi.org/10.4108/airo.6117,,10.4108/airo.6117,,,0,,12,true,cc-by-nc-sa,hybrid
199-422-702-757-84X,Domain-specialized LLM: Financial fine-tuning and utilization method using Mistral 7B,2024-03-31,2024,journal article,Journal of Intelligence and Information Systems,22884866; 22884882,Korea Intelligent Information System Society,,Cheonsu Jeong,"최근 사전학습된 범용적인 LLM(Large Language Model) 출시가 활발해지고 있지만, 도메인 특화 파인튜닝된 LLM 연구와 생성 방법을 제시하는 것은 부족한 실정이다. 본 연구는 도메인에 특화된 LLM의 파인튜닝과 활용에 대한 방안을 탐구하고 LLM의 최신 동향, 파운데이션 모델 및 LLM의 사전학습, 그리고 도메인 특화 LLM 파인튜닝에 대한 방법을 제시한다. 특히, 금융 분야에서의 언어 모델 활용이 중요하기 때문에 금융 특화 데이터셋의 선정과 전처리 방법, 모델 선정 및 파인튜닝 절차, 그리고 금융 특화 LLM 파인튜닝 시 고려해야 할 사항들에 대해 구체적으로 제시한다. 금융 데이터 특성을 고려한 도메인 특화 어휘의 구축과 보안 및 규정 준수에 대한 고려사항을 다룬다. LLM 파인튜닝의 적용과 활용 연구에서는 SLM인 Mistral 7B을 활용한 실제 보험 금융 도메인 LLM을 생성하는 방법 및 구현 절차와 다양한 금융 분야에 대한 사례를 제시한다. 이를 통해 본 연구는 LLM을 금융 도메인 분야에 적용하는 가능성을 알아보고 한계점과 개선 방향을 제안함으로써 향후 연구 방향성을 제시한다. 따라서 본 연구는 업무 도메인 분야에서 자연어 처리 기술의 적용과 발전에 기여함과 동시에 다양한 산업 분야에서의 LLM 활용 방향성을 제공함으로써 기업 내 금융 서비스 및 다양한 산업군에 LLM을 적극적으로 활용할 수 있도록 하는데 의미와 가치가 있다.",30,1,93,120,Domain (mathematical analysis); Computer science; Finance; Business; Mathematics; Mathematical analysis,,,,,,http://dx.doi.org/10.13088/jiis.2024.30.1.093,,10.13088/jiis.2024.30.1.093,,,0,,33,true,,bronze
199-580-547-765-295,"FinRipple: Aligning Large Language Models with Financial Market for
  Event Ripple Effect Awareness",2025-05-28,2025,preprint,arXiv (Cornell University),,,,Yuanjian Xu; Jianing Hao; Kunsheng Tang; Jingnan Chen; Anxian Liu; Peng Liu; Guang Zhang,"Financial markets exhibit complex dynamics where localized events trigger ripple effects across entities. Previous event studies, constrained by static single-company analyses and simplistic assumptions, fail to capture these ripple effects. While large language models (LLMs) offer emergent reasoning capabilities, their direct application falters due to structural market unawareness and limited capacity to analyze ripple effects. We propose FinRipple, an elegant framework that empowers LLMs with the ability to analyze ripple effects through financial theory-guided large-scale reinforcement learning. We begin by relaxing the assumptions of previous methods, incorporating a time-varying knowledge graph to accurately represent market structure. By seamlessly integrating classical asset pricing theory, we align the LLM with the market, enabling it to predict ripple effects. To the best of our knowledge, we are the first to provide a standardized definition of ripple effect prediction, a task that is extremely important yet unexplored in the financial domain. Extensive experiments demonstrate that FinRipple provides a promising solution to this task.",,,,,Ripple; Event (particle physics); Business; Financial market; Finance; Financial system; Engineering; Physics; Electrical engineering; Quantum mechanics; Voltage,,,,,https://arxiv.org/abs/2505.23826,http://dx.doi.org/10.48550/arxiv.2505.23826,,10.48550/arxiv.2505.23826,,,0,,0,true,,green
199-663-733-693-33X,Leveraging Large Language Models and Prompt Settings for Context-Aware Financial Sentiment Analysis,2024-02-19,2024,conference proceedings article,2024 5th International Conference on Advancements in Computational Sciences (ICACS),,IEEE,,Rabbia Ahmed; Sadaf Abdul Rauf; Seemab Latif,"Carefully crafted prompts can significantly enhance the accuracy and effectiveness of sentiment classification models. This paper explores the use of prompt engineering and large language models for financial sentiment analysis on financial reports of companies. Zero-shot and few-shot with prompts are designed to extract sentiment and contextual information. AI-generated synthetic examples were created for few-shot settings. Human-evaluated results are compared with four LLMs. Results show varying performance and output quality among LLMs, influenced by prompt design, report content, and task complexity. The LLMs' responses varied in length, detail, and style, affecting their readability and usefulness. The paper discusses the implications and limitations of these findings, suggesting future research directions.",,,1,9,Computer science; Context (archaeology); Sentiment analysis; Language model; Natural language processing; Data science; Artificial intelligence; Paleontology; Biology,,,,,,http://dx.doi.org/10.1109/icacs60934.2024.10473283,,10.1109/icacs60934.2024.10473283,,,0,006-399-176-500-152; 008-302-382-641-449; 018-866-913-622-957; 026-829-003-233-984; 028-397-979-531-015; 034-270-957-372-34X; 037-853-357-392-375; 039-110-388-312-10X; 043-015-109-339-094; 046-752-152-625-765; 047-729-043-873-226; 056-226-337-623-814; 056-990-384-509-129; 083-314-867-143-630; 083-449-940-348-930; 084-225-727-447-001; 085-260-413-523-411; 089-312-271-916-486; 116-354-018-099-585; 124-886-932-205-623; 133-218-004-325-22X; 142-971-697-178-379; 159-315-225-396-281,9,false,,
